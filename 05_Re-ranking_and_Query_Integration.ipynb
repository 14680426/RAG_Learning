{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0ecd3b-95da-4e65-a8d6-c65c57743d1f",
   "metadata": {},
   "source": [
    "# RAGé‡æ’åºä¸æŸ¥è¯¢é›†æˆï¼šæå‡æ£€ç´¢ç²¾åº¦çš„å…³é”®æŠ€æœ¯\n",
    "\n",
    "åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•ä¼˜åŒ–æ–‡æ¡£ç´¢å¼•å’Œæ£€ç´¢ç­–ç•¥ã€‚ä½†æ˜¯ï¼Œåˆæ¬¡æ£€ç´¢çš„ç»“æœå¾€å¾€ä¸å¤Ÿç²¾ç¡®ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨å¦‚ä½•é€šè¿‡é‡æ’åºå’ŒæŸ¥è¯¢é›†æˆæŠ€æœ¯è¿›ä¸€æ­¥æå‡æ£€ç´¢è´¨é‡ã€‚\n",
    "\n",
    "## ä¸ºä»€ä¹ˆè¦é‡æ’åºï¼Ÿ\n",
    "\n",
    "### å‘é‡æ£€ç´¢çš„å±€é™æ€§\n",
    "\n",
    "```python\n",
    "# é—®é¢˜ï¼šå•ä¸€å‘é‡ç›¸ä¼¼åº¦ä¸å¤Ÿç²¾ç¡®\n",
    "\n",
    "åœºæ™¯ï¼š\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\"\n",
    "\n",
    "å‘é‡æ£€ç´¢ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰:\n",
    "1. æ–‡æ¡£A: \"Pythonå¼‚å¸¸å¤„ç†æœºåˆ¶...try-except...\" (ç›¸ä¼¼åº¦: 0.85)\n",
    "2. æ–‡æ¡£B: \"Pythonä¸­çš„é”™è¯¯ç±»å‹...Exceptionç±»...\" (ç›¸ä¼¼åº¦: 0.83)\n",
    "3. æ–‡æ¡£C: \"Pythonç¼–ç¨‹åŸºç¡€...å˜é‡ã€å‡½æ•°...\" (ç›¸ä¼¼åº¦: 0.82)\n",
    "\n",
    "# é—®é¢˜åˆ†æ:\n",
    "â†’ æ–‡æ¡£Aæœ€ç›¸å…³ï¼Œæ’åºæ­£ç¡® âœ…\n",
    "â†’ ä½†ç›¸ä¼¼åº¦å·®å¼‚å¾ˆå°ï¼ˆ0.85 vs 0.83 vs 0.82ï¼‰\n",
    "â†’ å‘é‡ç›¸ä¼¼åº¦ä¸èƒ½å®Œå…¨åæ˜ çœŸå®ç›¸å…³æ€§\n",
    "â†’ æ–‡æ¡£Cä¸å¤ªç›¸å…³ä½†ç›¸ä¼¼åº¦ä¹Ÿä¸ä½\n",
    "\n",
    "# è§£å†³æ–¹æ¡ˆï¼šé‡æ’åº ğŸ¯\n",
    "â†’ ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹é‡æ–°è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§\n",
    "â†’ è€ƒè™‘æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç²¾ç¡®åŒ¹é…åº¦\n",
    "â†’ è°ƒæ•´æ’åºï¼Œç¡®ä¿æœ€ç›¸å…³çš„æ–‡æ¡£æ’åœ¨å‰é¢\n",
    "```\n",
    "\n",
    "## æœ¬ç« æŠ€æœ¯æ¦‚è§ˆ\n",
    "\n",
    "| æŠ€æœ¯ | æ ¸å¿ƒåŠŸèƒ½ | ä¼˜åŠ¿ | å¤æ‚åº¦ | æ¨èæŒ‡æ•° |\n",
    "|------|----------|------|--------|----------|\n",
    "| äº¤å‰ç¼–ç å™¨é‡æ’åº | ç²¾ç¡®ç›¸å…³æ€§è¯„ä¼° | å‡†ç¡®åº¦æœ€é«˜ | â­â­â­ | â­â­â­â­â­ |\n",
    "| å€’æ•°æ’åºèåˆ(RRF) | å¤šç»“æœèåˆ | é²æ£’æ€§å¼º | â­â­ | â­â­â­â­ |\n",
    "| å¤šæŸ¥è¯¢æ£€ç´¢ | å¤šè§’åº¦æŸ¥è¯¢ | å¬å›ç‡é«˜ | â­â­ | â­â­â­â­ |\n",
    "| æŸ¥è¯¢æ‰©å±• | è¯­ä¹‰æ‰©å…… | è¦†ç›–é¢å¹¿ | â­â­ | â­â­â­ |\n",
    "| æ··åˆæ£€ç´¢ | å‘é‡+å…³é”®è¯ | å…¨é¢æ€§å¥½ | â­â­â­ | â­â­â­â­â­ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2183999-52c7-4c15-9744-5f2561837d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/shared-nvme/conda-envs/py310/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 21:53:38 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 11-11 21:53:38 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', speculative_config=None, tokenizer='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8)\n",
      "WARNING 11-11 21:53:38 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-11 21:53:38 utils.py:660] Found nccl from library /usr/lib/x86_64-linux-gnu/libnccl.so.2\n",
      "INFO 11-11 21:53:38 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 11-11 21:53:38 selector.py:32] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:53:40,691 - modelscope - INFO - PyTorch version 2.3.0+cu118 Found.\n",
      "2025-11-11 21:53:40,692 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-11-11 21:53:40,742 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 298ceecce207285dd10b135af16e71cc and a total number of 964 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 21:53:47 model_runner.py:175] Loading model weights took 8.4983 GB\n",
      "INFO 11-11 21:53:49 gpu_executor.py:114] # GPU blocks: 1404, # CPU blocks: 512\n",
      "INFO 11-11 21:53:53 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-11 21:53:53 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-11 21:54:02 model_runner.py:1017] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹å‡†å¤‡å·¥ä½œ\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 1. åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹\n",
    "local_model_path = \"./Models/maidalun/bce-embedding-base_v1\" \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=local_model_path,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 2. åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“\n",
    "vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# 3. åŠ è½½æœ¬åœ°å¤§æ¨¡å‹\n",
    "model_dir=\"../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8\"\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "llm = LLM(\n",
    "    model=model_dir,\n",
    "    tokenizer=model_dir,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773bbd7f-cfe7-43e0-98e6-fc2df520a479",
   "metadata": {},
   "source": [
    "## Part 1: äº¤å‰ç¼–ç å™¨é‡æ’åº - Cross-Encoder Reranking\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "\n",
    "åŒç¼–ç å™¨(Bi-Encoder) vs äº¤å‰ç¼–ç å™¨(Cross-Encoder):\n",
    "\n",
    "```python\n",
    "# åŒç¼–ç å™¨ï¼ˆç”¨äºåˆå§‹æ£€ç´¢ï¼‰\n",
    "æŸ¥è¯¢ â†’ ç¼–ç å™¨ â†’ æŸ¥è¯¢å‘é‡ â”€â”€â”\n",
    "                            â”œâ†’ ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "æ–‡æ¡£ â†’ ç¼–ç å™¨ â†’ æ–‡æ¡£å‘é‡ â”€â”€â”˜\n",
    "\n",
    "ä¼˜ç‚¹ï¼šå¿«é€Ÿï¼Œå¯é¢„å…ˆè®¡ç®—æ–‡æ¡£å‘é‡\n",
    "ç¼ºç‚¹ï¼šæŸ¥è¯¢å’Œæ–‡æ¡£ç‹¬ç«‹ç¼–ç ï¼Œæ— æ³•æ•æ‰ç»†ç²’åº¦äº¤äº’\n",
    "\n",
    "# äº¤å‰ç¼–ç å™¨ï¼ˆç”¨äºé‡æ’åºï¼‰\n",
    "æŸ¥è¯¢ + æ–‡æ¡£ â†’ ç¼–ç å™¨ â†’ ç›¸å…³æ€§åˆ†æ•°\n",
    "\n",
    "ä¼˜ç‚¹ï¼šæŸ¥è¯¢å’Œæ–‡æ¡£è”åˆç¼–ç ï¼Œæ›´ç²¾ç¡®\n",
    "ç¼ºç‚¹ï¼šæ…¢ï¼Œæ— æ³•é¢„å…ˆè®¡ç®—\n",
    "```\n",
    "\n",
    "å·¥ä½œæµç¨‹ï¼š\n",
    "```python\n",
    "# RAG + é‡æ’åºæµç¨‹\n",
    "\n",
    "1. åˆå§‹æ£€ç´¢ï¼ˆåŒç¼–ç å™¨ï¼‰\n",
    "   â†’ ä»å¤§é‡æ–‡æ¡£ä¸­å¿«é€Ÿæ£€ç´¢top-kä¸ªå€™é€‰\n",
    "   â†’ ä¾‹å¦‚ï¼šä»10000ä¸ªæ–‡æ¡£ä¸­æ£€ç´¢top-50\n",
    "\n",
    "2. é‡æ’åºï¼ˆäº¤å‰ç¼–ç å™¨ï¼‰\n",
    "   â†’ å¯¹top-kä¸ªå€™é€‰é‡æ–°è¯„åˆ†\n",
    "   â†’ æ›´ç²¾ç¡®åœ°æ’åº\n",
    "\n",
    "3. è¿”å›æœ€ç»ˆç»“æœ\n",
    "   â†’ è¿”å›é‡æ’åºåçš„top-nä¸ªæ–‡æ¡£\n",
    "   â†’ ä¾‹å¦‚ï¼šè¿”å›æœ€ç›¸å…³çš„top-5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b82bd6-3055-4ff2-a648-8d42715d0d1f",
   "metadata": {},
   "source": [
    "### å®ç°æœ¬åœ°äº¤å‰ç¼–ç å™¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1f25693-ac42-4a04-9210-508a9dd9a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "class LocalCrossEncoderReranker:\n",
    "    \"\"\"ä½¿ç”¨æœ¬åœ°äº¤å‰ç¼–ç å™¨æ¨¡å‹é‡æ’åº\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, model_name: str = \"./Models/ms-marco-MiniLM-L-6-v2/cross-encoder/ms-marco-MiniLM-L6-v2\"):\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        # åŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹\n",
    "        print(f\"ğŸ“¥ åŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹: {model_name}\")\n",
    "        self.cross_encoder = CrossEncoder(model_name)\n",
    "        self.persist_directory: str = \"./chroma_db\"\n",
    "\n",
    "        client = chromadb.PersistentClient(path=self.persist_directory)  \n",
    "        try:\n",
    "            client.delete_collection(\"local_rerank\")\n",
    "            print(\"ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„ Chroma é›†åˆ 'local_rerank'\")\n",
    "        except ValueError:\n",
    "            print(\"ğŸ†• æœªå‘ç°æ—§é›†åˆï¼Œå°†åˆ›å»ºæ–°çš„ 'local_rerank' é›†åˆ\")\n",
    "        \n",
    "        # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"local_rerank\",\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=self.persist_directory \n",
    "        )\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£\"\"\"\n",
    "        self.vectorstore.add_documents(documents)\n",
    "    \n",
    "    def retrieve_and_rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        initial_k: int = 20,\n",
    "        final_k: int = 5\n",
    "    ):\n",
    "        \"\"\"æ£€ç´¢å¹¶é‡æ’åº\"\"\"\n",
    "        # 1. åˆå§‹æ£€ç´¢\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=initial_k)\n",
    "        \n",
    "        # 2. å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹\n",
    "        query_doc_pairs = [\n",
    "            [query, doc.page_content] for doc in initial_docs\n",
    "        ]\n",
    "        \n",
    "        # 3. ä½¿ç”¨äº¤å‰ç¼–ç å™¨è®¡ç®—ç›¸å…³æ€§åˆ†æ•°\n",
    "        print(f\"ğŸ¯ ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ–°è¯„åˆ†...\")\n",
    "        scores = self.cross_encoder.predict(query_doc_pairs)\n",
    "        \n",
    "        # 4. æ ¹æ®åˆ†æ•°æ’åº\n",
    "        scored_docs = [\n",
    "            {'document': doc, 'score': score}\n",
    "            for doc, score in zip(initial_docs, scores)\n",
    "        ]\n",
    "        \n",
    "        # æŒ‰åˆ†æ•°é™åºæ’åº\n",
    "        scored_docs.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # 5. è¿”å›top-k\n",
    "        reranked_docs = scored_docs[:final_k]\n",
    "        \n",
    "        print(\"\\né‡æ’åºç»“æœ:\")\n",
    "        for i, item in enumerate(reranked_docs):\n",
    "            print(f\"{i+1}. [å¾—åˆ†: {item['score']:.4f}] {item['document'].page_content[:100]}...\")\n",
    "        \n",
    "        return reranked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63057cec-2c77-4091-b80d-3cf6ccd55226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ åŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹: ./Models/ms-marco-MiniLM-L-6-v2/cross-encoder/ms-marco-MiniLM-L6-v2\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„ Chroma é›†åˆ 'local_rerank'\n"
     ]
    }
   ],
   "source": [
    "# å‡†å¤‡æµ‹è¯•æ–‡æ¡£\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
    "        \n",
    "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
    "        try:\n",
    "            risky_operation()\n",
    "        except Exception as e:\n",
    "            print(f\"å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        \n",
    "        å¯ä»¥æ•è·ç‰¹å®šå¼‚å¸¸ç±»å‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨finallyå­å¥ã€‚\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"python_exceptions.md\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Pythoné”™è¯¯å’Œå¼‚å¸¸ç±»å‹\n",
    "        \n",
    "        Pythonæœ‰å¤šç§å†…ç½®å¼‚å¸¸ç±»å‹ï¼š\n",
    "        - ValueError: å€¼é”™è¯¯\n",
    "        - TypeError: ç±»å‹é”™è¯¯\n",
    "        - KeyError: é”®é”™è¯¯\n",
    "        - IndexError: ç´¢å¼•é”™è¯¯\n",
    "        \n",
    "        æ‰€æœ‰å¼‚å¸¸éƒ½ç»§æ‰¿è‡ªExceptionç±»ã€‚\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"python_error_types.md\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Pythonç¼–ç¨‹åŸºç¡€æ•™ç¨‹\n",
    "        \n",
    "        æœ¬æ•™ç¨‹æ¶µç›–ï¼š\n",
    "        - å˜é‡å’Œæ•°æ®ç±»å‹\n",
    "        - æ§åˆ¶æµè¯­å¥\n",
    "        - å‡½æ•°å®šä¹‰\n",
    "        - æ¨¡å—å¯¼å…¥\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"python_basics.md\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        å¦‚ä½•åœ¨Pythonä¸­ä¼˜é›…åœ°å¤„ç†é”™è¯¯\n",
    "        \n",
    "        æœ€ä½³å®è·µï¼š\n",
    "        1. åªæ•è·ä½ èƒ½å¤„ç†çš„å¼‚å¸¸\n",
    "        2. ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯Exception\n",
    "        3. æä¾›æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯\n",
    "        4. é€‚å½“æ—¶ä½¿ç”¨finallyæ¸…ç†èµ„æº\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"python_error_best_practices.md\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "local_reranker = LocalCrossEncoderReranker(embeddings)\n",
    "local_reranker.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bdbaa27-daf1-4d9c-86b3-5c54dfea4d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ–°è¯„åˆ†...\n",
      "\n",
      "é‡æ’åºç»“æœ:\n",
      "1. [å¾—åˆ†: 8.3246] \n",
      "        å¦‚ä½•åœ¨Pythonä¸­ä¼˜é›…åœ°å¤„ç†é”™è¯¯\n",
      "        \n",
      "        æœ€ä½³å®è·µï¼š\n",
      "        1. åªæ•è·ä½ èƒ½å¤„ç†çš„å¼‚å¸¸\n",
      "        2. ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯Excep...\n",
      "2. [å¾—åˆ†: 7.6097] \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation(...\n",
      "3. [å¾—åˆ†: 7.5047] \n",
      "        Pythoné”™è¯¯å’Œå¼‚å¸¸ç±»å‹\n",
      "        \n",
      "        Pythonæœ‰å¤šç§å†…ç½®å¼‚å¸¸ç±»å‹ï¼š\n",
      "        - ValueError: å€¼é”™è¯¯\n",
      "        - TypeErr...\n"
     ]
    }
   ],
   "source": [
    "query = \"Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\"\n",
    "results = local_reranker.retrieve_and_rerank(query, initial_k=10, final_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9e6c4-3443-43f9-9c61-e5f6fdf6cb47",
   "metadata": {},
   "source": [
    "### å¸¸ç”¨äº¤å‰ç¼–ç å™¨æ¨¡å‹\n",
    "\n",
    "\n",
    "```python\n",
    "RERANKER_MODELS = {\n",
    "    \"small_fast\": {\n",
    "        \"name\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\",\n",
    "        \"params\": \"~4M\",\n",
    "        \"speed\": \"very fast\",\n",
    "        \"quality\": \"good\"\n",
    "    },\n",
    "    \"balanced\": {\n",
    "        \"name\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        \"params\": \"~22M\",\n",
    "        \"speed\": \"fast\",\n",
    "        \"quality\": \"very good\"\n",
    "    },\n",
    "    \"high_quality\": {\n",
    "        \"name\": \"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
    "        \"params\": \"~33M\",\n",
    "        \"speed\": \"medium\",\n",
    "        \"quality\": \"excellent\"\n",
    "    },\n",
    "    \"multilingual\": {\n",
    "        \"name\": \"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\",\n",
    "        \"params\": \"~118M\",\n",
    "        \"speed\": \"medium\",\n",
    "        \"quality\": \"excellent (å¤šè¯­è¨€)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def choose_reranker(priority: str = \"balanced\"):\n",
    "    \"\"\"é€‰æ‹©åˆé€‚çš„é‡æ’åºæ¨¡å‹\"\"\"\n",
    "    model_info = RERANKER_MODELS.get(priority, RERANKER_MODELS[\"balanced\"])\n",
    "    \n",
    "    print(f\"ğŸ“Š é€‰æ‹©æ¨¡å‹: {model_info['name']}\")\n",
    "    print(f\"   å‚æ•°é‡: {model_info['params']}\")\n",
    "    print(f\"   é€Ÿåº¦: {model_info['speed']}\")\n",
    "    print(f\"   è´¨é‡: {model_info['quality']}\")\n",
    "    \n",
    "    return CrossEncoder(model_info['name'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e93391-fb27-4523-89bf-af153c94eba3",
   "metadata": {},
   "source": [
    "## Part 2: å€’æ•°æ’åºèåˆ - Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šå€’æ•°æ’åºèåˆ(RRF)æ˜¯ä¸€ç§èåˆå¤šä¸ªæ’åºåˆ—è¡¨çš„ç®—æ³•ï¼Œå®ƒä¸éœ€è¦çŸ¥é“å…·ä½“çš„åˆ†æ•°ï¼Œåªéœ€è¦æ’åã€‚\n",
    "\n",
    "**RRFç®—æ³•åŸç†**\n",
    "\n",
    "```python\n",
    "# RRFå…¬å¼\n",
    "\n",
    "å¯¹äºæ–‡æ¡£dï¼Œå…¶RRFåˆ†æ•°ä¸ºï¼š\n",
    "RRF(d) = Î£ (1 / (k + rank_i(d)))\n",
    "\n",
    "å…¶ä¸­:\n",
    "- rank_i(d): æ–‡æ¡£dåœ¨ç¬¬iä¸ªæ’åºåˆ—è¡¨ä¸­çš„æ’å\n",
    "- k: å¸¸æ•°ï¼Œé€šå¸¸å–60\n",
    "- Î£: å¯¹æ‰€æœ‰æ’åºåˆ—è¡¨æ±‚å’Œ\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "å‡è®¾æœ‰2ä¸ªæ’åºåˆ—è¡¨ï¼Œk=60:\n",
    "\n",
    "åˆ—è¡¨1: [DocA, DocB, DocC]  (DocAæ’å1, DocBæ’å2, DocCæ’å3)\n",
    "åˆ—è¡¨2: [DocC, DocA, DocD]  (DocCæ’å1, DocAæ’å2, DocDæ’å3)\n",
    "\n",
    "RRFåˆ†æ•°:\n",
    "DocA: 1/(60+1) + 1/(60+2) = 0.0164 + 0.0161 = 0.0325\n",
    "DocB: 1/(60+2) + 0        = 0.0161\n",
    "DocC: 1/(60+3) + 1/(60+1) = 0.0159 + 0.0164 = 0.0323\n",
    "DocD: 0        + 1/(60+3) = 0.0159\n",
    "\n",
    "æœ€ç»ˆæ’åº: DocA > DocC > DocB > DocD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a7195-1bda-4e75-95a5-49611219c2f2",
   "metadata": {},
   "source": [
    "### å®ç°RRFèåˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2683c7d4-e5c3-4244-8df6-2b11653f8bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRFèåˆç»“æœ:\n",
      "1. [RRFåˆ†æ•°: 0.0164] æ–‡æ¡£A: Pythonå¼‚å¸¸å¤„ç†......\n",
      "2. [RRFåˆ†æ•°: 0.0164] æ–‡æ¡£C: ç¼–ç¨‹åŸºç¡€......\n",
      "3. [RRFåˆ†æ•°: 0.0161] æ–‡æ¡£B: é”™è¯¯ç±»å‹......\n",
      "4. [RRFåˆ†æ•°: 0.0161] æ–‡æ¡£A: Pythonå¼‚å¸¸å¤„ç†......\n",
      "5. [RRFåˆ†æ•°: 0.0159] æ–‡æ¡£C: ç¼–ç¨‹åŸºç¡€......\n",
      "6. [RRFåˆ†æ•°: 0.0159] æ–‡æ¡£D: æœ€ä½³å®è·µ......\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "class ReciprocalRankFusion:\n",
    "    \"\"\"å€’æ•°æ’åºèåˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 60):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k: RRFå¸¸æ•°ï¼Œé€šå¸¸å–60\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "    \n",
    "    def fuse(self, ranked_lists: List[List[Document]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        èåˆå¤šä¸ªæ’åºåˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            ranked_lists: å¤šä¸ªæ–‡æ¡£æ’åºåˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            èåˆåçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆåŒ…å«RRFåˆ†æ•°ï¼‰\n",
    "        \"\"\"\n",
    "        # å­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„RRFåˆ†æ•°\n",
    "        doc_scores = defaultdict(float)\n",
    "        doc_map = {}  # æ–‡æ¡£IDåˆ°æ–‡æ¡£å¯¹è±¡çš„æ˜ å°„\n",
    "        \n",
    "        # éå†æ¯ä¸ªæ’åºåˆ—è¡¨\n",
    "        for ranked_list in ranked_lists:\n",
    "            for rank, doc in enumerate(ranked_list, start=1):\n",
    "                # ä½¿ç”¨page_contentä½œä¸ºæ–‡æ¡£å”¯ä¸€æ ‡è¯†\n",
    "                doc_id = id(doc)\n",
    "                \n",
    "                # è®¡ç®—RRFåˆ†æ•°\n",
    "                rrf_score = 1.0 / (self.k + rank)\n",
    "                doc_scores[doc_id] += rrf_score\n",
    "                \n",
    "                # ä¿å­˜æ–‡æ¡£å¯¹è±¡\n",
    "                if doc_id not in doc_map:\n",
    "                    doc_map[doc_id] = doc\n",
    "        \n",
    "        # æŒ‰RRFåˆ†æ•°æ’åº\n",
    "        sorted_docs = sorted(\n",
    "            doc_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # æ„å»ºç»“æœ\n",
    "        fused_results = [\n",
    "            {\n",
    "                'document': doc_map[doc_id],\n",
    "                'rrf_score': score\n",
    "            }\n",
    "            for doc_id, score in sorted_docs\n",
    "        ]\n",
    "        \n",
    "        return fused_results\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "rrf = ReciprocalRankFusion(k=60)\n",
    "\n",
    "# æ¨¡æ‹Ÿå¤šä¸ªæ£€ç´¢å™¨çš„ç»“æœ\n",
    "# æ£€ç´¢å™¨1: å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢\n",
    "retriever1_results = [\n",
    "    Document(page_content=\"æ–‡æ¡£A: Pythonå¼‚å¸¸å¤„ç†...\"),\n",
    "    Document(page_content=\"æ–‡æ¡£B: é”™è¯¯ç±»å‹...\"),\n",
    "    Document(page_content=\"æ–‡æ¡£C: ç¼–ç¨‹åŸºç¡€...\")\n",
    "]\n",
    "\n",
    "# æ£€ç´¢å™¨2: BM25å…³é”®è¯æ£€ç´¢\n",
    "retriever2_results = [\n",
    "    Document(page_content=\"æ–‡æ¡£C: ç¼–ç¨‹åŸºç¡€...\"),\n",
    "    Document(page_content=\"æ–‡æ¡£A: Pythonå¼‚å¸¸å¤„ç†...\"),\n",
    "    Document(page_content=\"æ–‡æ¡£D: æœ€ä½³å®è·µ...\")\n",
    "]\n",
    "\n",
    "# èåˆç»“æœ\n",
    "fused = rrf.fuse([retriever1_results, retriever2_results])\n",
    "\n",
    "print(\"RRFèåˆç»“æœ:\")\n",
    "for i, item in enumerate(fused):\n",
    "    print(f\"{i+1}. [RRFåˆ†æ•°: {item['rrf_score']:.4f}] {item['document'].page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d355550-b647-4791-8693-4feed5de937c",
   "metadata": {},
   "source": [
    "### å®ç°å®Œæ•´çš„RRFæ£€ç´¢å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d980a778-fffa-474d-b46a-f39fec4621e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from typing import List, Dict\n",
    "\n",
    "class RRFEnsembleRetriever:\n",
    "    \"\"\"ä½¿ç”¨RRFçš„é›†æˆæ£€ç´¢å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, documents: List[Document]):\n",
    "        self.embeddings = embeddings\n",
    "        self.persist_directory: str = \"./chroma_db\"\n",
    "        # 1. å‘é‡æ£€ç´¢å™¨\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"rrf_ensemble\",\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=self.persist_directory \n",
    "        )\n",
    "        self.vectorstore.add_documents(documents)\n",
    "        self.vector_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        \n",
    "        # 2. BM25å…³é”®è¯æ£€ç´¢å™¨\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "        self.bm25_retriever.k = 10\n",
    "        \n",
    "        # 3. RRFèåˆå™¨\n",
    "        self.rrf = ReciprocalRankFusion(k=60)\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"ä½¿ç”¨RRFèåˆå¤šä¸ªæ£€ç´¢å™¨çš„ç»“æœ\"\"\"\n",
    "        print(f\"ğŸ” æŸ¥è¯¢: {query}\\n\")\n",
    "        \n",
    "        # 1. å‘é‡æ£€ç´¢\n",
    "        print(\"ğŸ“Š å‘é‡æ£€ç´¢...\")\n",
    "        vector_results = self.vector_retriever.get_relevant_documents(query)\n",
    "        print(f\"  â†’ æ£€ç´¢åˆ° {len(vector_results)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        # 2. BM25æ£€ç´¢\n",
    "        print(\"ğŸ”¤ BM25å…³é”®è¯æ£€ç´¢...\")\n",
    "        bm25_results = self.bm25_retriever.get_relevant_documents(query)\n",
    "        print(f\"  â†’ æ£€ç´¢åˆ° {len(bm25_results)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        # 3. RRFèåˆ\n",
    "        print(\"\\nğŸ”€ RRFèåˆ...\")\n",
    "        fused_results = self.rrf.fuse([vector_results, bm25_results])\n",
    "        \n",
    "        # è¿”å›top-k\n",
    "        return fused_results[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62f107fc-5c16-481c-a405-b95d88027aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æŸ¥è¯¢: Pythonå¼‚å¸¸å¤„ç†\n",
      "\n",
      "ğŸ“Š å‘é‡æ£€ç´¢...\n",
      "  â†’ æ£€ç´¢åˆ° 10 ä¸ªæ–‡æ¡£\n",
      "ğŸ”¤ BM25å…³é”®è¯æ£€ç´¢...\n",
      "  â†’ æ£€ç´¢åˆ° 4 ä¸ªæ–‡æ¡£\n",
      "\n",
      "ğŸ”€ RRFèåˆ...\n",
      "\n",
      "æœ€ç»ˆç»“æœ:\n",
      "\n",
      "1. [RRFåˆ†æ•°: 0.0164]\n",
      "   \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation()\n",
      "        except Exception as e:\n",
      "            print...\n",
      "\n",
      "2. [RRFåˆ†æ•°: 0.0164]\n",
      "   \n",
      "        å¦‚ä½•åœ¨Pythonä¸­ä¼˜é›…åœ°å¤„ç†é”™è¯¯\n",
      "        \n",
      "        æœ€ä½³å®è·µï¼š\n",
      "        1. åªæ•è·ä½ èƒ½å¤„ç†çš„å¼‚å¸¸\n",
      "        2. ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯Exception\n",
      "        3. æä¾›æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯\n",
      "        4. é€‚å½“æ—¶ä½¿ç”¨finallyæ¸…...\n",
      "\n",
      "3. [RRFåˆ†æ•°: 0.0161]\n",
      "   \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation()\n",
      "        except Exception as e:\n",
      "            print...\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "rrf_retriever = RRFEnsembleRetriever(embeddings, documents)\n",
    "\n",
    "results = rrf_retriever.retrieve(\"Pythonå¼‚å¸¸å¤„ç†\", k=3)\n",
    "\n",
    "print(\"\\næœ€ç»ˆç»“æœ:\")\n",
    "for i, item in enumerate(results):\n",
    "    print(f\"\\n{i+1}. [RRFåˆ†æ•°: {item['rrf_score']:.4f}]\")\n",
    "    print(f\"   {item['document'].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e29fc-da1a-4508-9fa8-b21e19b70643",
   "metadata": {},
   "source": [
    "## Part 3: å¤šæŸ¥è¯¢æ£€ç´¢ - Multi-Query Retrieval\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "å¤šæŸ¥è¯¢æ£€ç´¢çš„æ€è·¯æ˜¯ï¼š\n",
    "\n",
    "1. ä»å•ä¸ªç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆå¤šä¸ªç›¸ä¼¼ä½†ä¸åŒçš„æŸ¥è¯¢\n",
    "2. å¯¹æ¯ä¸ªæŸ¥è¯¢åˆ†åˆ«æ£€ç´¢\n",
    "3. èåˆæ‰€æœ‰æ£€ç´¢ç»“æœ\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦å¤šæŸ¥è¯¢ï¼Ÿ\n",
    "\n",
    "\n",
    "```python\n",
    "# é—®é¢˜ï¼šå•ä¸€æŸ¥è¯¢å¯èƒ½ä¸å¤Ÿå…¨é¢\n",
    "\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"Pythonå¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ\"\n",
    "\n",
    "# å¯èƒ½çš„ç›¸å…³æ–‡æ¡£:\n",
    "- \"Pythonæ–‡ä»¶è¯»å–open()å‡½æ•°\"  âœ… åŒ¹é…\n",
    "- \"è¯»å†™æ–‡ä»¶çš„æœ€ä½³å®è·µ\"        âŒ å¯èƒ½ä¸åŒ¹é…ï¼ˆæ²¡æœ‰\"Python\"ï¼‰\n",
    "- \"ä½¿ç”¨pathlibå¤„ç†æ–‡ä»¶è·¯å¾„\"  âŒ å¯èƒ½ä¸åŒ¹é…ï¼ˆæ²¡æœ‰\"è¯»å–\"ï¼‰\n",
    "\n",
    "# è§£å†³æ–¹æ¡ˆï¼šç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“ âœ…\n",
    "åŸå§‹æŸ¥è¯¢: \"Pythonå¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ\"\n",
    "\n",
    "ç”Ÿæˆçš„æŸ¥è¯¢å˜ä½“:\n",
    "1. \"åœ¨Pythonä¸­æ‰“å¼€å’Œè¯»å–æ–‡ä»¶\"\n",
    "2. \"Python file I/Oæ“ä½œ\"\n",
    "3. \"ä½¿ç”¨open()å‡½æ•°è¯»å–æ–‡ä»¶å†…å®¹\"\n",
    "4. \"Pythonæ–‡ä»¶å¤„ç†æ–¹æ³•\"\n",
    "\n",
    "â†’ ä¸åŒå˜ä½“å¯èƒ½åŒ¹é…ä¸åŒçš„ç›¸å…³æ–‡æ¡£\n",
    "â†’ èåˆç»“æœï¼Œæé«˜å¬å›ç‡\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18575c-cc30-42a0-9997-c953c96ac149",
   "metadata": {},
   "source": [
    "### å®ç°ä½¿ç”¨LLMç”ŸæˆæŸ¥è¯¢å˜ä½“\n",
    "\n",
    "è¿™æ˜¯åœ¨02ç« èŠ‚å®ç°è¿‡çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d495577f-6785-4eae-8f8b-4cd9f16507d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict\n",
    "from vllm import SamplingParams\n",
    "\n",
    "class MultiQueryRetriever:\n",
    "    \"\"\"å¤šæŸ¥è¯¢æ£€ç´¢å™¨ï¼ˆé€‚é…æœ¬åœ°æ¨¡å‹ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_queries(self, question: str) -> List[str]:\n",
    "        \"\"\"ç”ŸæˆæŸ¥è¯¢å˜ä½“\"\"\"\n",
    "        # ä½¿ç”¨ChatMLæ ¼å¼\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ç”ŸæˆåŠ©æ‰‹ã€‚è¯·ä¸ºç”¨æˆ·çš„æŸ¥è¯¢ç”Ÿæˆ3ä¸ªä¸åŒçš„å˜ä½“ï¼Œè¿™äº›å˜ä½“è¡¨è¾¾ç›¸åŒçš„æ„å›¾ä½†ç”¨è¯ä¸åŒã€‚\n",
    "            \n",
    "            è¯·æ¯è¡Œè¾“å‡ºä¸€ä¸ªæŸ¥è¯¢ï¼Œä¸è¦ç¼–å·ã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            ç”¨æˆ·æŸ¥è¯¢ï¼š{question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_tokens=200,\n",
    "            stop=[\"<|im_end|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        response = outputs[0].outputs[0].text.strip() if outputs and outputs[0].outputs else \"\"\n",
    "        \n",
    "        # è§£æç”Ÿæˆçš„æŸ¥è¯¢\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        \n",
    "        # åŠ å…¥åŸå§‹æŸ¥è¯¢\n",
    "        all_queries = [question] + queries\n",
    "        \n",
    "        print(f\"ğŸ“ ç”Ÿæˆäº† {len(all_queries)} ä¸ªæŸ¥è¯¢å˜ä½“\")\n",
    "        return all_queries\n",
    "    \n",
    "    def simple_rrf_fusion(self, all_results: List[List[Document]], k: int = 5) -> List[Document]:\n",
    "        \"\"\"ç®€å•çš„RRFèåˆ\"\"\"\n",
    "        # ç®€å•çš„æ–‡æ¡£å»é‡å’Œæ’åº\n",
    "        seen_content = set()\n",
    "        unique_docs = []\n",
    "        \n",
    "        for results in all_results:\n",
    "            for doc in results:\n",
    "                if doc.page_content not in seen_content:\n",
    "                    seen_content.add(doc.page_content)\n",
    "                    unique_docs.append(doc)\n",
    "        \n",
    "        return unique_docs[:k]\n",
    "    \n",
    "    def retrieve(self, question: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"å¤šæŸ¥è¯¢æ£€ç´¢\"\"\"\n",
    "        # 1. ç”ŸæˆæŸ¥è¯¢å˜ä½“\n",
    "        queries = self.generate_queries(question)\n",
    "        \n",
    "        # 2. å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢\n",
    "        all_results = []\n",
    "        for query in queries:\n",
    "            results = self.vectorstore.similarity_search(query, k=k)\n",
    "            all_results.append(results)\n",
    "\n",
    "        # 3. ç®€å•çš„ç»“æœèåˆ\n",
    "        fused_results = self.simple_rrf_fusion(all_results, k)\n",
    "        \n",
    "        print(f\"âœ… æœ€ç»ˆæ£€ç´¢åˆ° {len(fused_results)} ä¸ªæ–‡æ¡£\")\n",
    "        # print(f\"fused_results: {fused_results}\")\n",
    "        return fused_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "537ae74e-1b34-409c-b6a4-c349fba11382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. åˆ›å»ºå‘é‡æ•°æ®åº“\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_query\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "vectorstore.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "367cb2b4-6b92-4263-8091-7b308016cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. åˆ›å»ºå¤šæŸ¥è¯¢æ£€ç´¢å™¨\n",
    "multi_query_retriever = MultiQueryRetriever(vectorstore, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6e5b75d-188e-4a76-b073-f7ad49c984d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆäº† 4 ä¸ªæŸ¥è¯¢å˜ä½“\n",
      "âœ… æœ€ç»ˆæ£€ç´¢åˆ° 3 ä¸ªæ–‡æ¡£\n",
      "\n",
      "æœ€ç»ˆæ£€ç´¢ç»“æœ:\n",
      "\n",
      "1. \n",
      "        Pythonç¼–ç¨‹åŸºç¡€æ•™ç¨‹\n",
      "        \n",
      "        æœ¬æ•™ç¨‹æ¶µç›–ï¼š\n",
      "        - å˜é‡å’Œæ•°æ®ç±»å‹\n",
      "        - æ§åˆ¶æµè¯­å¥\n",
      "        - å‡½æ•°å®šä¹‰\n",
      "     ...\n",
      "\n",
      "2. \n",
      "        å¦‚ä½•åœ¨Pythonä¸­ä¼˜é›…åœ°å¤„ç†é”™è¯¯\n",
      "        \n",
      "        æœ€ä½³å®è·µï¼š\n",
      "        1. åªæ•è·ä½ èƒ½å¤„ç†çš„å¼‚å¸¸\n",
      "        2. ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯Excep...\n",
      "\n",
      "3. \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation(...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. æ£€ç´¢æµ‹è¯•\n",
    "question = \"Pythonå¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ\"\n",
    "results = multi_query_retriever.retrieve(question, k=3)\n",
    "\n",
    "print(\"\\næœ€ç»ˆæ£€ç´¢ç»“æœ:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n{i+1}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b3b40-201f-425e-ac8b-875b90ccf2a3",
   "metadata": {},
   "source": [
    "## Part 4: æŸ¥è¯¢æ‰©å±• - Query Expansion\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ:æŸ¥è¯¢æ‰©å±•é€šè¿‡æ·»åŠ ç›¸å…³æœ¯è¯­æ¥å¢å¼ºåŸå§‹æŸ¥è¯¢ï¼Œæé«˜æ£€ç´¢çš„è¦†ç›–é¢ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813066b-7de9-4c03-ba16-3314e3b28616",
   "metadata": {},
   "source": [
    "### å®ç°åŸºäºLLMçš„æŸ¥è¯¢æ‰©å±•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e59983b-c399-4f96-a799-7594d728ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "class QueryExpander:\n",
    "    \"\"\"æŸ¥è¯¢æ‰©å±•å™¨ï¼ˆé€‚é…æœ¬åœ°æ¨¡å‹ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def expand(self, query: str) -> str:\n",
    "        \"\"\"æ‰©å±•æŸ¥è¯¢\"\"\"\n",
    "        # ä½¿ç”¨ChatMLæ ¼å¼\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ‰©å±•ä¸“å®¶ã€‚è¯·ä¸ºç”¨æˆ·æŸ¥è¯¢æ·»åŠ åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­ã€‚\n",
    "            \n",
    "            è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š\n",
    "            åŸå§‹æœ¯è¯­ OR åŒä¹‰è¯1 OR åŒä¹‰è¯2<|im_end|>\n",
    "            <|im_start|>user\n",
    "            æŸ¥è¯¢ï¼š{query}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            æ‰©å±•æŸ¥è¯¢ï¼š\"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            max_tokens=512,\n",
    "            stop=[\"<|im_end|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        expanded = outputs[0].outputs[0].text.strip() if outputs and outputs[0].outputs else query\n",
    "        \n",
    "        print(f\"åŸå§‹æŸ¥è¯¢: {query}\")\n",
    "        print(f\"æ‰©å±•æŸ¥è¯¢: {expanded}\")\n",
    "        \n",
    "        return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86b0aa46-ab1b-404b-8c87-1c16b3886e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæŸ¥è¯¢æ‰©å±•å™¨\n",
    "expander = QueryExpander(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd381508-0f68-4cc2-a7c3-d5cbc5b00177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æŸ¥è¯¢: Pythonæœºå™¨å­¦ä¹ \n",
      "æ‰©å±•æŸ¥è¯¢: Pythonæ•°æ®åˆ†æ OR Pythonç»Ÿè®¡åˆ†æ OR Pythonæ•°æ®æŒ–æ˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•æ‰©å±•\n",
    "original_query = \"Pythonæœºå™¨å­¦ä¹ \"\n",
    "expanded_query = expander.expand(original_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db211243-a137-42ec-aa2c-d127734845c7",
   "metadata": {},
   "source": [
    "### å®ç°ä¼ªç›¸å…³åé¦ˆ(Pseudo Relevance Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98f25f3b-9ae7-4ece-9762-0d49c0bbd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from vllm import SamplingParams\n",
    "\n",
    "class PseudoRelevanceFeedback:\n",
    "    \"\"\"ä¼ªç›¸å…³åé¦ˆæŸ¥è¯¢æ‰©å±•\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "    \n",
    "    def expand_query(self, query: str, top_k: int = 3) -> str:\n",
    "        \"\"\"ä½¿ç”¨ä¼ªç›¸å…³åé¦ˆæ‰©å±•æŸ¥è¯¢\"\"\"\n",
    "        # 1. åˆå§‹æ£€ç´¢\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=top_k)\n",
    "        \n",
    "        # 2. ä»topæ–‡æ¡£æå–å…³é”®è¯\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in initial_docs])\n",
    "        \n",
    "        # 3. ä½¿ç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ‰©å±•åŠ©æ‰‹ã€‚åŸºäºç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œä¸ºåŸå§‹æŸ¥è¯¢æ·»åŠ é‡è¦çš„ç›¸å…³æœ¯è¯­ã€‚\n",
    "            \n",
    "            è¯·ä¿æŒç®€æ´ï¼Œåªæ·»åŠ æœ€é‡è¦çš„æœ¯è¯­ã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            åŸå§‹æŸ¥è¯¢ï¼š{query}\n",
    "            \n",
    "            ç›¸å…³æ–‡æ¡£ï¼š\n",
    "            {context}\n",
    "            \n",
    "            è¯·ç”Ÿæˆæ‰©å±•æŸ¥è¯¢ï¼š<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            æ‰©å±•æŸ¥è¯¢ï¼š\"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            max_tokens=512,\n",
    "            stop=[\"<|im_end|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        expanded = outputs[0].outputs[0].text.strip() if outputs and outputs[0].outputs else query\n",
    "        \n",
    "        print(f\"ğŸ“ åŸå§‹æŸ¥è¯¢: {query}\")\n",
    "        print(f\"âœ¨ æ‰©å±•æŸ¥è¯¢: {expanded}\")\n",
    "        \n",
    "        return expanded\n",
    "    \n",
    "    def retrieve_with_expansion(self, query: str, k: int = 5):\n",
    "        \"\"\"ä½¿ç”¨æ‰©å±•æŸ¥è¯¢æ£€ç´¢\"\"\"\n",
    "        # 1. æ‰©å±•æŸ¥è¯¢\n",
    "        expanded_query = self.expand_query(query)\n",
    "        \n",
    "        # 2. ä½¿ç”¨æ‰©å±•æŸ¥è¯¢æ£€ç´¢\n",
    "        results = self.vectorstore.similarity_search(expanded_query, k=k)\n",
    "        \n",
    "        print(f\"âœ… æ£€ç´¢åˆ° {len(results)} ä¸ªæ–‡æ¡£\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0099857-29ee-46c0-bc4e-3dc6b378ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. åˆ›å»ºä¼ªç›¸å…³åé¦ˆæ£€ç´¢å™¨\n",
    "prf = PseudoRelevanceFeedback(vectorstore, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1633305-dffa-496d-9497-8aa5051264c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ åŸå§‹æŸ¥è¯¢: Pythonå¼‚å¸¸\n",
      "âœ¨ æ‰©å±•æŸ¥è¯¢: Pythonå¼‚å¸¸å¤„ç†\n",
      "\n",
      "  1. Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—ï¼šhttps://realpython.com/python-exceptions/\n",
      "  2. ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼šhttps://realpython.com/python-try-except/\n",
      "  3. å¯ä»¥æ•è·ç‰¹å®šå¼‚å¸¸ç±»å‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨finallyå­å¥ï¼šhttps://realpython.com/python-finally/\n",
      "  4. Pythoné”™è¯¯å’Œå¼‚å¸¸ç±»å‹ï¼šhttps://docs.python.org/3/library/exceptions.html\n",
      "  5. å¦‚ä½•åœ¨Pythonä¸­ä¼˜é›…åœ°å¤„ç†é”™è¯¯ï¼šhttps://realpython.com/python-error-handling/\n",
      "\n",
      "æ³¨æ„ï¼šåœ¨å¤„ç†å¼‚å¸¸æ—¶ï¼Œåº”åªæ•è·ä½ èƒ½å¤„ç†çš„å¼‚å¸¸ï¼Œä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯Exceptionï¼Œæä¾›æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯ï¼Œé€‚å½“æ—¶ä½¿ç”¨finallyæ¸…ç†èµ„æºã€‚\n",
      "âœ… æ£€ç´¢åˆ° 3 ä¸ªæ–‡æ¡£\n",
      "\n",
      "æœ€ç»ˆæ£€ç´¢ç»“æœ:\n",
      "1. \n",
      "        å¦‚ä½•åœ¨Pythonä¸­ä¼˜é›…åœ°å¤„ç†é”™è¯¯\n",
      "        \n",
      "        æœ€ä½³å®è·µï¼š\n",
      "        1. åªæ•è·ä½ èƒ½å¤„ç†çš„å¼‚å¸¸\n",
      "        2. ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯Excep...\n",
      "2. \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation(...\n",
      "3. \n",
      "        Pythoné”™è¯¯å’Œå¼‚å¸¸ç±»å‹\n",
      "        \n",
      "        Pythonæœ‰å¤šç§å†…ç½®å¼‚å¸¸ç±»å‹ï¼š\n",
      "        - ValueError: å€¼é”™è¯¯\n",
      "        - TypeErr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. æ£€ç´¢æµ‹è¯•\n",
    "results = prf.retrieve_with_expansion(\"Pythonå¼‚å¸¸\", k=3)\n",
    "\n",
    "print(\"\\næœ€ç»ˆæ£€ç´¢ç»“æœ:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38d2c4-2b21-4f29-85de-46c44e06bb22",
   "metadata": {},
   "source": [
    "## Part 5: æ··åˆæ£€ç´¢ - Hybrid Search\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šæ··åˆæ£€ç´¢ç»“åˆå‘é‡æ£€ç´¢(è¯­ä¹‰æœç´¢)å’Œå…³é”®è¯æ£€ç´¢(å¦‚BM25)ï¼Œåˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ã€‚\n",
    "\n",
    "**å‘é‡æ£€ç´¢ vs å…³é”®è¯æ£€ç´¢**\n",
    "\n",
    "```python\n",
    "# å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰æœç´¢ï¼‰\n",
    "ä¼˜ç‚¹:\n",
    "âœ… ç†è§£è¯­ä¹‰ï¼Œèƒ½åŒ¹é…åŒä¹‰è¯\n",
    "âœ… èƒ½å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢\n",
    "âœ… è·¨è¯­è¨€èƒ½åŠ›ï¼ˆå¤šè¯­è¨€æ¨¡å‹ï¼‰\n",
    "\n",
    "ç¼ºç‚¹:\n",
    "âŒ å¯¹ç²¾ç¡®æœ¯è¯­åŒ¹é…ä¸æ•æ„Ÿ\n",
    "âŒ å¯¹ç½•è§è¯æˆ–ä¸“æœ‰åè¯æ•ˆæœå·®\n",
    "âŒ è®¡ç®—æˆæœ¬é«˜\n",
    "\n",
    "# å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰\n",
    "ä¼˜ç‚¹:\n",
    "âœ… ç²¾ç¡®åŒ¹é…å…³é”®è¯\n",
    "âœ… å¯¹ä¸“æœ‰åè¯ã€ä»£ç ç­‰æ•ˆæœå¥½\n",
    "âœ… è®¡ç®—å¿«é€Ÿ\n",
    "\n",
    "ç¼ºç‚¹:\n",
    "âŒ ä¸ç†è§£è¯­ä¹‰\n",
    "âŒ æ— æ³•åŒ¹é…åŒä¹‰è¯\n",
    "âŒ å¯¹æŸ¥è¯¢æªè¾æ•æ„Ÿ\n",
    "\n",
    "# æ··åˆæ£€ç´¢ = å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢ ğŸ¯\n",
    "â†’ ç»“åˆä¸¤è€…ä¼˜åŠ¿\n",
    "â†’ é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73d49a-12c5-4504-9736-24626ee4470c",
   "metadata": {},
   "source": [
    "### å®ç°å‘é‡+BM25æ··åˆæ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb107739-8d2e-41b8-b110-e1499885b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import jieba  # ä¸­æ–‡åˆ†è¯\n",
    "import numpy as np\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡ + BM25ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, documents: List[Document], weights: tuple = (0.5, 0.5)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights: (å‘é‡æƒé‡, BM25æƒé‡)ï¼Œä¸¤è€…ä¹‹å’Œåº”ä¸º1.0\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.documents = documents\n",
    "        self.vector_weight, self.bm25_weight = weights\n",
    "        \n",
    "        # 1. å‘é‡å­˜å‚¨\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"multi_query\",\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=\"./chroma_db\",\n",
    "        )\n",
    "        # self.vectorstore.add_documents(documents)\n",
    "        \n",
    "        # 2. BM25ç´¢å¼•\n",
    "        self._build_bm25_index()\n",
    "    \n",
    "    def _build_bm25_index(self):\n",
    "        \"\"\"æ„å»ºBM25ç´¢å¼•\"\"\"\n",
    "        # åˆ†è¯ï¼ˆä¸­æ–‡ä½¿ç”¨jiebaï¼Œè‹±æ–‡å¯ä»¥ä½¿ç”¨splitï¼‰\n",
    "        tokenized_docs = [\n",
    "            list(jieba.cut(doc.page_content)) for doc in self.documents\n",
    "        ]\n",
    "        \n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "        print(f\"âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆ\")\n",
    "    \n",
    "    def _vector_search(self, query: str, k: int) -> List[tuple]:\n",
    "        \"\"\"å‘é‡æ£€ç´¢ï¼Œè¿”å› (doc, score)\"\"\"\n",
    "        results = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        # å½’ä¸€åŒ–åˆ†æ•°åˆ°[0, 1]\n",
    "        if results:\n",
    "            max_score = max(score for _, score in results)\n",
    "            min_score = min(score for _, score in results)\n",
    "            score_range = max_score - min_score if max_score != min_score else 1\n",
    "            \n",
    "            normalized = [\n",
    "                (doc, 1 - (score - min_score) / score_range)  # è·ç¦»è½¬ç›¸ä¼¼åº¦\n",
    "                for doc, score in results\n",
    "            ]\n",
    "        else:\n",
    "            normalized = []\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def _bm25_search(self, query: str, k: int) -> List[tuple]:\n",
    "        \"\"\"BM25æ£€ç´¢ï¼Œè¿”å› (doc, score)\"\"\"\n",
    "        # æŸ¥è¯¢åˆ†è¯\n",
    "        tokenized_query = list(jieba.cut(query))\n",
    "        \n",
    "        # BM25è¯„åˆ†\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # è·å–top-k\n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "        \n",
    "        # å½’ä¸€åŒ–åˆ†æ•°\n",
    "        max_score = scores[top_indices[0]] if len(top_indices) > 0 else 1\n",
    "        \n",
    "        results = [\n",
    "            (self.documents[i], scores[i] / max_score if max_score > 0 else 0)\n",
    "            for i in top_indices\n",
    "        ]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def hybrid_search(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"æ··åˆæ£€ç´¢\"\"\"\n",
    "        print(f\"ğŸ” æ··åˆæ£€ç´¢: {query}\")\n",
    "        print(f\"   æƒé‡: å‘é‡={self.vector_weight}, BM25={self.bm25_weight}\\n\")\n",
    "        \n",
    "        # 1. å‘é‡æ£€ç´¢\n",
    "        print(\"ğŸ“Š å‘é‡æ£€ç´¢...\")\n",
    "        vector_results = self._vector_search(query, k=k*2)\n",
    "        \n",
    "        # 2. BM25æ£€ç´¢\n",
    "        print(\"ğŸ”¤ BM25æ£€ç´¢...\")\n",
    "        bm25_results = self._bm25_search(query, k=k*2)\n",
    "        \n",
    "        # 3. åˆå¹¶åˆ†æ•°\n",
    "        print(\"\\nğŸ”€ åˆå¹¶ç»“æœ...\")\n",
    "        combined_scores = defaultdict(float)\n",
    "        doc_map = {}\n",
    "        \n",
    "        for doc, score in vector_results:\n",
    "            doc_id = id(doc)\n",
    "            combined_scores[doc_id] += self.vector_weight * score\n",
    "            doc_map[doc_id] = doc\n",
    "        \n",
    "        for doc, score in bm25_results:\n",
    "            doc_id = id(doc)\n",
    "            combined_scores[doc_id] += self.bm25_weight * score\n",
    "            doc_map[doc_id] = doc\n",
    "        \n",
    "        # 4. æ’åº\n",
    "        sorted_results = sorted(\n",
    "            combined_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "        \n",
    "        # 5. æ„å»ºç»“æœ\n",
    "        final_results = [\n",
    "            {\n",
    "                'document': doc_map[doc_id],\n",
    "                'hybrid_score': score\n",
    "            }\n",
    "            for doc_id, score in sorted_results\n",
    "        ]\n",
    "        \n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56a11dfd-e690-4061-8269-c1fcd5ec4e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    embeddings=embeddings,\n",
    "    documents=documents,\n",
    "    weights=(0.6, 0.4)  # 60%å‘é‡ï¼Œ40% BM25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee9a66bd-7c25-4f8e-88e3-fb051c96c9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ··åˆæ£€ç´¢: Pythonå¼‚å¸¸å¤„ç†\n",
      "   æƒé‡: å‘é‡=0.6, BM25=0.4\n",
      "\n",
      "ğŸ“Š å‘é‡æ£€ç´¢...\n",
      "ğŸ”¤ BM25æ£€ç´¢...\n",
      "\n",
      "ğŸ”€ åˆå¹¶ç»“æœ...\n",
      "æ··åˆæ£€ç´¢ç»“æœ:\n",
      "\n",
      "1. [æ··åˆåˆ†æ•°: 0.6000]\n",
      "   \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation()\n",
      "        except Exception as e:\n",
      "            print...\n",
      "\n",
      "2. [æ··åˆåˆ†æ•°: 0.6000]\n",
      "   \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation()\n",
      "        except Exception as e:\n",
      "            print...\n",
      "\n",
      "3. [æ··åˆåˆ†æ•°: 0.6000]\n",
      "   \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation()\n",
      "        except Exception as e:\n",
      "            print...\n"
     ]
    }
   ],
   "source": [
    "results = hybrid_retriever.hybrid_search(\"Pythonå¼‚å¸¸å¤„ç†\", k=3)\n",
    "\n",
    "print(\"æ··åˆæ£€ç´¢ç»“æœ:\")\n",
    "for i, item in enumerate(results):\n",
    "    print(f\"\\n{i+1}. [æ··åˆåˆ†æ•°: {item['hybrid_score']:.4f}]\")\n",
    "    print(f\"   {item['document'].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e53d7a-f500-4218-b808-ca92b456d53b",
   "metadata": {},
   "source": [
    "## ç»¼åˆå®æˆ˜ï¼šå®Œæ•´çš„é«˜çº§æ£€ç´¢ç³»ç»Ÿ\n",
    "\n",
    "å°†æ‰€æœ‰æŠ€æœ¯æ•´åˆæˆä¸€ä¸ªç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "158ccce1-5334-4cac-a2a5-2fe14dfa6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRetrievalSystem:\n",
    "    \"\"\"é«˜çº§æ£€ç´¢ç³»ç»Ÿï¼ˆé›†æˆæ‰€æœ‰æŠ€æœ¯ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, llm, documents: List[Document]):\n",
    "        self.embeddings = embeddings\n",
    "        self.llm = llm\n",
    "        self.documents = documents\n",
    "        \n",
    "        # å‘é‡å­˜å‚¨\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"multi_query\",\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=\"./chroma_db\",\n",
    "        )\n",
    "        # self.vectorstore.add_documents(documents)\n",
    "        \n",
    "        # ç»„ä»¶\n",
    "        self.multi_query = MultiQueryRetriever(self.vectorstore, llm)\n",
    "        self.reranker = LocalCrossEncoderReranker(embeddings)\n",
    "        self.reranker.add_documents(documents)\n",
    "        self.hybrid = HybridRetriever(embeddings, documents, weights=(0.6, 0.4))\n",
    "        self.rrf = ReciprocalRankFusion(k=60)\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        mode: str = \"hybrid_multiquery_rerank\",\n",
    "        k: int = 5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        é«˜çº§æ£€ç´¢\n",
    "        \n",
    "        Args:\n",
    "            mode: æ£€ç´¢æ¨¡å¼\n",
    "                - \"simple\": ç®€å•å‘é‡æ£€ç´¢\n",
    "                - \"hybrid\": æ··åˆæ£€ç´¢\n",
    "                - \"multiquery\": å¤šæŸ¥è¯¢æ£€ç´¢\n",
    "                - \"hybrid_multiquery\": æ··åˆ+å¤šæŸ¥è¯¢\n",
    "                - \"hybrid_multiquery_rerank\": æ··åˆ+å¤šæŸ¥è¯¢+é‡æ’åºï¼ˆæœ€å¼ºï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ¯ æ£€ç´¢æ¨¡å¼: {mode}\")\n",
    "        print(f\"â“ æŸ¥è¯¢: {query}\\n\")\n",
    "        \n",
    "        if mode == \"simple\":\n",
    "            # ç®€å•å‘é‡æ£€ç´¢\n",
    "            results = self.vectorstore.similarity_search(query, k=k)\n",
    "            results = [{'document': doc, 'score': 0} for doc in results]\n",
    "        \n",
    "        elif mode == \"hybrid\":\n",
    "            # æ··åˆæ£€ç´¢\n",
    "            results = self.hybrid.hybrid_search(query, k=k)\n",
    "        \n",
    "        elif mode == \"multiquery\":\n",
    "            # å¤šæŸ¥è¯¢æ£€ç´¢\n",
    "            results = self.multi_query.retrieve(query, k=k)\n",
    "        \n",
    "        elif mode == \"hybrid_multiquery\":\n",
    "            # æ··åˆ + å¤šæŸ¥è¯¢\n",
    "            # 1. ç”ŸæˆæŸ¥è¯¢å˜ä½“\n",
    "            queries = self.multi_query.generate_queries(query)\n",
    "            \n",
    "            # 2. å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ··åˆæ£€ç´¢\n",
    "            all_results = []\n",
    "            for q in queries:\n",
    "                hybrid_results = self.hybrid.hybrid_search(q, k=10)\n",
    "                all_results.append([item['document'] for item in hybrid_results])\n",
    "            \n",
    "            # 3. RRFèåˆ\n",
    "            results = self.rrf.fuse(all_results)[:k]\n",
    "        \n",
    "        elif mode == \"hybrid_multiquery_rerank\":\n",
    "            # æ··åˆ + å¤šæŸ¥è¯¢ + é‡æ’åºï¼ˆæœ€å¼ºæ¨¡å¼ï¼‰\n",
    "            # 1. ç”ŸæˆæŸ¥è¯¢å˜ä½“\n",
    "            queries = self.multi_query.generate_queries(query)\n",
    "            \n",
    "            # 2. æ··åˆæ£€ç´¢\n",
    "            all_results = []\n",
    "            for q in queries:\n",
    "                hybrid_results = self.hybrid.hybrid_search(q, k=10)\n",
    "                all_results.append([item['document'] for item in hybrid_results])\n",
    "            \n",
    "            # 3. RRFèåˆ\n",
    "            fused = self.rrf.fuse(all_results)\n",
    "            candidate_docs = [item['document'] for item in fused[:20]]\n",
    "            \n",
    "            # 4. äº¤å‰ç¼–ç å™¨é‡æ’åº\n",
    "            print(\"\\nğŸ¯ é‡æ’åº...\")\n",
    "            results = self.reranker.retrieve_and_rerank(\n",
    "                query=query,\n",
    "                initial_k=len(candidate_docs),\n",
    "                final_k=k\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "\n",
    "    def query(self, question: str, mode: str = \"hybrid_multiquery_rerank\"):\n",
    "        \"\"\"æ‰§è¡Œå®Œæ•´çš„RAGæŸ¥è¯¢ï¼ˆä¿®å¤é‡å¤ä»£ç é—®é¢˜ï¼‰\"\"\"\n",
    "        # 1. æ£€ç´¢\n",
    "        results = self.retrieve(question, mode=mode, k=3)\n",
    "    \n",
    "        # 2. æå–æ–‡æ¡£ï¼šå…¼å®¹ä¸¤ç§æ ¼å¼\n",
    "        if results and isinstance(results[0], dict) and 'document' in results[0]:\n",
    "            # æ ¼å¼: [{\"document\": Document, \"score\": ...}, ...]\n",
    "            docs = [item['document'] for item in results]\n",
    "        else:\n",
    "            # æ ¼å¼: [Document, Document, ...] æˆ–å…¶ä»–æ ¼å¼\n",
    "            docs = results  # ç›´æ¥ä½¿ç”¨ç»“æœ\n",
    "        \n",
    "        # 3. ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ChatMLæ ¼å¼ï¼‰\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # æ„å»ºChatMLæ ¼å¼çš„æç¤º\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚\n",
    "            \n",
    "            æ–‡æ¡£å†…å®¹ï¼š\n",
    "            {context}<|im_end|>\n",
    "            <|im_start|>user\n",
    "            é—®é¢˜ï¼š{question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ\n",
    "        from vllm import SamplingParams\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=512,\n",
    "            stop=[\"<|im_end|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        answer = outputs[0].outputs[0].text.strip() if outputs and outputs[0].outputs else \"æœªèƒ½ç”Ÿæˆç­”æ¡ˆ\"\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"documents\": docs,\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc305e45-5a4a-4f0c-8a90-7b66cd17e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ åŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹: ./Models/ms-marco-MiniLM-L-6-v2/cross-encoder/ms-marco-MiniLM-L6-v2\n",
      "ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„ Chroma é›†åˆ 'local_rerank'\n",
      "âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨é«˜çº§æ£€ç´¢ç³»ç»Ÿ\n",
    "advanced_system = AdvancedRetrievalSystem(embeddings, llm, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2589c398-8c3f-4e62-a1ce-db99d31429ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "æµ‹è¯•æ¨¡å¼: simple\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ æ£€ç´¢æ¨¡å¼: simple\n",
      "â“ æŸ¥è¯¢: Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ ç­”æ¡ˆ:\n",
      "åœ¨Pythonä¸­ï¼Œå¯ä»¥ä½¿ç”¨try-exceptè¯­å¥æ¥æ•è·å’Œå¤„ç†å¼‚å¸¸ã€‚tryè¯­å¥å—ä¸­åŒ…å«å¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸çš„ä»£ç ï¼Œå¦‚æœtryè¯­å¥å—ä¸­çš„ä»£ç æŠ›å‡ºäº†å¼‚å¸¸ï¼Œé‚£ä¹ˆç¨‹åºä¼šç«‹å³è·³è½¬åˆ°ä¸è¯¥å¼‚å¸¸åŒ¹é…çš„exceptè¯­å¥å—ä¸­ã€‚exceptè¯­å¥å—ä¸­çš„ä»£ç ä¼šåœ¨å¼‚å¸¸å‘ç”Ÿæ—¶è¢«æ‰§è¡Œï¼Œå¯ä»¥ç”¨æ¥å¤„ç†å¼‚å¸¸ï¼Œä¾‹å¦‚æ‰“å°é”™è¯¯ä¿¡æ¯ï¼Œæˆ–è€…æ‰§è¡Œå…¶ä»–æ¢å¤æ“ä½œã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨finallyè¯­å¥å—æ¥ç¡®ä¿å³ä½¿åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶ï¼ŒæŸäº›ä»£ç ä¹Ÿä¼šè¢«æ‰§è¡Œã€‚\n",
      "\n",
      "\n",
      "============================================================\n",
      "æµ‹è¯•æ¨¡å¼: hybrid\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ æ£€ç´¢æ¨¡å¼: hybrid\n",
      "â“ æŸ¥è¯¢: Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\n",
      "\n",
      "ğŸ” æ··åˆæ£€ç´¢: Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\n",
      "   æƒé‡: å‘é‡=0.6, BM25=0.4\n",
      "\n",
      "ğŸ“Š å‘é‡æ£€ç´¢...\n",
      "ğŸ”¤ BM25æ£€ç´¢...\n",
      "\n",
      "ğŸ”€ åˆå¹¶ç»“æœ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ ç­”æ¡ˆ:\n",
      "åœ¨Pythonä¸­ï¼Œå¯ä»¥ä½¿ç”¨try-exceptè¯­å¥æ¥æ•è·å’Œå¤„ç†å¼‚å¸¸ã€‚tryå—ä¸­åŒ…å«å¯èƒ½ä¼šå¼•å‘å¼‚å¸¸çš„ä»£ç ï¼Œå¦‚æœtryå—ä¸­çš„ä»£ç å¼•å‘å¼‚å¸¸ï¼Œç¨‹åºä¼šç«‹å³è·³è½¬åˆ°ä¸è¯¥å¼‚å¸¸åŒ¹é…çš„exceptå—ä¸­ã€‚exceptå—ä¸­çš„ä»£ç ä¼šåœ¨å‘ç”Ÿå¼‚å¸¸æ—¶è¢«æ‰§è¡Œï¼Œå¯ä»¥ç”¨æ¥å¤„ç†å¼‚å¸¸ï¼Œä¾‹å¦‚æ‰“å°é”™è¯¯ä¿¡æ¯ã€è®°å½•æ—¥å¿—ã€æ¢å¤ç¨‹åºçŠ¶æ€ç­‰ã€‚å¦‚æœexceptå—ä¸­æ²¡æœ‰åŒ¹é…çš„å¼‚å¸¸ï¼Œç¨‹åºä¼šç»§ç»­æ‰§è¡Œã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨finallyå—æ¥ç¡®ä¿åœ¨try-exceptè¯­å¥å—ä¸­çš„ä»£ç æ— è®ºå¦‚ä½•éƒ½ä¼šè¢«æ‰§è¡Œï¼Œæ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸ã€‚\n",
      "\n",
      "\n",
      "============================================================\n",
      "æµ‹è¯•æ¨¡å¼: hybrid_multiquery_rerank\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ æ£€ç´¢æ¨¡å¼: hybrid_multiquery_rerank\n",
      "â“ æŸ¥è¯¢: Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆäº† 4 ä¸ªæŸ¥è¯¢å˜ä½“\n",
      "ğŸ” æ··åˆæ£€ç´¢: Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\n",
      "   æƒé‡: å‘é‡=0.6, BM25=0.4\n",
      "\n",
      "ğŸ“Š å‘é‡æ£€ç´¢...\n",
      "ğŸ”¤ BM25æ£€ç´¢...\n",
      "\n",
      "ğŸ”€ åˆå¹¶ç»“æœ...\n",
      "ğŸ” æ··åˆæ£€ç´¢: 1. å¦‚ä½•åœ¨Pythonä¸­å¤„ç†å¼‚å¸¸ï¼Ÿ\n",
      "   æƒé‡: å‘é‡=0.6, BM25=0.4\n",
      "\n",
      "ğŸ“Š å‘é‡æ£€ç´¢...\n",
      "ğŸ”¤ BM25æ£€ç´¢...\n",
      "\n",
      "ğŸ”€ åˆå¹¶ç»“æœ...\n",
      "ğŸ” æ··åˆæ£€ç´¢: 2. å¦‚ä½•åº”å¯¹Pythonä¸­çš„å¼‚å¸¸æƒ…å†µï¼Ÿ\n",
      "   æƒé‡: å‘é‡=0.6, BM25=0.4\n",
      "\n",
      "ğŸ“Š å‘é‡æ£€ç´¢...\n",
      "ğŸ”¤ BM25æ£€ç´¢...\n",
      "\n",
      "ğŸ”€ åˆå¹¶ç»“æœ...\n",
      "ğŸ” æ··åˆæ£€ç´¢: 3. å¦‚ä½•å¤„ç†Pythonç¨‹åºä¸­çš„é”™è¯¯ï¼Ÿ\n",
      "   æƒé‡: å‘é‡=0.6, BM25=0.4\n",
      "\n",
      "ğŸ“Š å‘é‡æ£€ç´¢...\n",
      "ğŸ”¤ BM25æ£€ç´¢...\n",
      "\n",
      "ğŸ”€ åˆå¹¶ç»“æœ...\n",
      "\n",
      "ğŸ¯ é‡æ’åº...\n",
      "ğŸ¯ ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ–°è¯„åˆ†...\n",
      "\n",
      "é‡æ’åºç»“æœ:\n",
      "1. [å¾—åˆ†: 8.3246] \n",
      "        å¦‚ä½•åœ¨Pythonä¸­ä¼˜é›…åœ°å¤„ç†é”™è¯¯\n",
      "        \n",
      "        æœ€ä½³å®è·µï¼š\n",
      "        1. åªæ•è·ä½ èƒ½å¤„ç†çš„å¼‚å¸¸\n",
      "        2. ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯Excep...\n",
      "2. [å¾—åˆ†: 7.6097] \n",
      "        Pythonå¼‚å¸¸å¤„ç†å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ä½¿ç”¨try-exceptæ•è·å¼‚å¸¸ï¼š\n",
      "        try:\n",
      "            risky_operation(...\n",
      "3. [å¾—åˆ†: 7.5047] \n",
      "        Pythoné”™è¯¯å’Œå¼‚å¸¸ç±»å‹\n",
      "        \n",
      "        Pythonæœ‰å¤šç§å†…ç½®å¼‚å¸¸ç±»å‹ï¼š\n",
      "        - ValueError: å€¼é”™è¯¯\n",
      "        - TypeErr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ ç­”æ¡ˆ:\n",
      "åœ¨Pythonä¸­ï¼Œå¯ä»¥ä½¿ç”¨try-exceptè¯­å¥æ¥ä¼˜é›…åœ°å¤„ç†å¼‚å¸¸ã€‚tryå—ä¸­åŒ…å«å¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸çš„ä»£ç ï¼Œå¦‚æœtryå—ä¸­çš„ä»£ç æŠ›å‡ºå¼‚å¸¸ï¼Œç¨‹åºä¼šç«‹å³è·³è½¬åˆ°ä¸ä¹‹åŒ¹é…çš„exceptå—ä¸­ã€‚exceptå—ä¸­çš„ä»£ç ä¼šåœ¨å¼‚å¸¸å‘ç”Ÿæ—¶è¢«æ‰§è¡Œï¼Œå¯ä»¥ç”¨æ¥å¤„ç†å¼‚å¸¸æˆ–æ•è·ç‰¹å®šç±»å‹çš„å¼‚å¸¸ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨finallyå­å¥æ¥æ¸…ç†èµ„æºï¼Œæ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸ã€‚\n",
      "\n",
      "\n",
      "============================================================\n",
      "æµ‹è¯•æ¨¡å¼: multiquery\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ æ£€ç´¢æ¨¡å¼: multiquery\n",
      "â“ æŸ¥è¯¢: Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆäº† 4 ä¸ªæŸ¥è¯¢å˜ä½“\n",
      "âœ… æœ€ç»ˆæ£€ç´¢åˆ° 2 ä¸ªæ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ ç­”æ¡ˆ:\n",
      "åœ¨Pythonä¸­ï¼Œå¯ä»¥ä½¿ç”¨try-exceptè¯­å¥æ¥æ•è·å’Œå¤„ç†å¼‚å¸¸ã€‚tryå—ä¸­åŒ…å«å¯èƒ½ä¼šå¼•å‘å¼‚å¸¸çš„ä»£ç ï¼Œå¦‚æœtryå—ä¸­çš„ä»£ç å¼•å‘äº†å¼‚å¸¸ï¼Œç¨‹åºä¼šç«‹å³è·³è½¬åˆ°ä¸ä¹‹åŒ¹é…çš„exceptå—ä¸­ã€‚exceptå—ä¸­çš„ä»£ç ä¼šåœ¨å¼‚å¸¸å‘ç”Ÿæ—¶è¢«æ‰§è¡Œï¼Œå¯ä»¥ç”¨æ¥å¤„ç†å¼‚å¸¸æˆ–æä¾›æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨finallyå­å¥æ¥ç¡®ä¿åœ¨try-exceptè¯­å¥å—ä¸­çš„ä»£ç æ— è®ºå¦‚ä½•éƒ½ä¼šè¢«æ‰§è¡Œï¼Œå³ä½¿æ²¡æœ‰å¼•å‘å¼‚å¸¸ã€‚\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ä¸åŒæ¨¡å¼\n",
    "modes = [\"simple\", \"hybrid\", \"hybrid_multiquery_rerank\", \"multiquery\"]\n",
    "\n",
    "question = \"Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\"\n",
    "\n",
    "for mode in modes:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æµ‹è¯•æ¨¡å¼: {mode}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    result = advanced_system.query(question, mode=mode)\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ç­”æ¡ˆ:\\n{result['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb0429-d928-46c0-99f0-b7c0c9026110",
   "metadata": {},
   "source": [
    "## å®éªŒæ€»ç»“\n",
    "\n",
    "1. é‡æ’åº: ä½¿ç”¨äº¤å‰ç¼–ç å™¨æå‡ç²¾åº¦\n",
    "2. RRFèåˆ: ç®€å•æœ‰æ•ˆçš„ç»“æœèåˆæ–¹æ³•\n",
    "3. å¤šæŸ¥è¯¢: æé«˜å¬å›ç‡\n",
    "4. æ··åˆæ£€ç´¢: ç»“åˆå‘é‡å’Œå…³é”®è¯æ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86cd78ce-fbb8-4dca-b0c8-771e313dc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŠ€æœ¯é€‰æ‹©æŒ‡å—\n",
    "\n",
    "RETRIEVAL_STRATEGIES = {\n",
    "    \"å¿«é€ŸåŸå‹\": {\n",
    "        \"ç­–ç•¥\": \"simple\",\n",
    "        \"è¯´æ˜\": \"ç®€å•å‘é‡æ£€ç´¢\",\n",
    "        \"é€‚ç”¨\": \"å¿«é€ŸéªŒè¯æƒ³æ³•ï¼Œæ•°æ®é‡å°\"\n",
    "    },\n",
    "    \n",
    "    \"ç”Ÿäº§ç¯å¢ƒåŸºç¡€\": {\n",
    "        \"ç­–ç•¥\": \"hybrid\",\n",
    "        \"è¯´æ˜\": \"æ··åˆæ£€ç´¢ï¼ˆå‘é‡+BM25ï¼‰\",\n",
    "        \"é€‚ç”¨\": \"å¤§å¤šæ•°ç”Ÿäº§åœºæ™¯ï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡\"\n",
    "    },\n",
    "    \n",
    "    \"é«˜å¬å›ç‡\": {\n",
    "        \"ç­–ç•¥\": \"multiquery\",\n",
    "        \"è¯´æ˜\": \"å¤šæŸ¥è¯¢æ£€ç´¢\",\n",
    "        \"é€‚ç”¨\": \"éœ€è¦å…¨é¢è¦†ç›–ï¼Œä¸è¦é—æ¼ç›¸å…³æ–‡æ¡£\"\n",
    "    },\n",
    "    \n",
    "    \"é«˜ç²¾åº¦\": {\n",
    "        \"ç­–ç•¥\": \"hybrid_multiquery_rerank\",\n",
    "        \"è¯´æ˜\": \"æ··åˆ+å¤šæŸ¥è¯¢+é‡æ’åº\",\n",
    "        \"é€‚ç”¨\": \"å¯¹è´¨é‡è¦æ±‚æé«˜ï¼Œå¯ä»¥ç‰ºç‰²é€Ÿåº¦\"\n",
    "    },\n",
    "    \n",
    "    \"å®æ—¶åº”ç”¨\": {\n",
    "        \"ç­–ç•¥\": \"hybrid + ç¼“å­˜\",\n",
    "        \"è¯´æ˜\": \"æ··åˆæ£€ç´¢+ç»“æœç¼“å­˜\",\n",
    "        \"é€‚ç”¨\": \"éœ€è¦å¿«é€Ÿå“åº”çš„åº”ç”¨\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def choose_strategy(priority: str):\n",
    "    \"\"\"æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©ç­–ç•¥\"\"\"\n",
    "    strategy = RETRIEVAL_STRATEGIES.get(priority)\n",
    "    \n",
    "    if strategy:\n",
    "        print(f\"æ¨èç­–ç•¥: {strategy['ç­–ç•¥']}\")\n",
    "        print(f\"è¯´æ˜: {strategy['è¯´æ˜']}\")\n",
    "        print(f\"é€‚ç”¨åœºæ™¯: {strategy['é€‚ç”¨']}\")\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821be323-9cbc-4491-acc3-419ffe8cff61",
   "metadata": {},
   "source": [
    "### æŠ€æœ¯ç»„åˆå»ºè®®\n",
    "\n",
    "```python\n",
    "æ£€ç´¢è´¨é‡å±‚çº§:\n",
    "Level 1: å‘é‡æ£€ç´¢ (åŸºç¡€)\n",
    "  â†“\n",
    "Level 2: æ··åˆæ£€ç´¢ (å‘é‡ + BM25)\n",
    "  â†“\n",
    "Level 3: æ··åˆæ£€ç´¢ + RRFèåˆ\n",
    "  â†“\n",
    "Level 4: å¤šæŸ¥è¯¢ + æ··åˆæ£€ç´¢ + RRF\n",
    "  â†“\n",
    "Level 5: å¤šæŸ¥è¯¢ + æ··åˆæ£€ç´¢ + RRF + é‡æ’åº (æœ€å¼º)\n",
    "\n",
    "æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„çº§åˆ«:\n",
    "- å¿«é€ŸåŸå‹: Level 1\n",
    "- ç”Ÿäº§åŸºç¡€: Level 2-3\n",
    "- é«˜è´¨é‡åº”ç”¨: Level 4-5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fdf39-63bb-4272-92cd-7e3eb12da8c6",
   "metadata": {},
   "source": [
    "**è‡³æ­¤ï¼Œæ­å–œï¼ä½ å·²ç»æŒæ¡äº†RAGç³»ç»Ÿä¸­æœ€é‡è¦çš„æ£€ç´¢ä¼˜åŒ–æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯å°†æ˜¾è‘—æå‡ä½ çš„RAGåº”ç”¨è´¨é‡ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
