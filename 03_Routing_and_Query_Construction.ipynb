{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f56ff3f-46df-4374-a06b-b06402771eb4",
   "metadata": {},
   "source": [
    "# RAGè·¯ç”±ä¸æŸ¥è¯¢æ„å»ºï¼šæ™ºèƒ½æ£€ç´¢çš„æ ¸å¿ƒæŠ€æœ¯\n",
    "\n",
    "åœ¨å‰ä¸¤ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†RAGç³»ç»Ÿçš„åŸºç¡€å’ŒæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯ã€‚ä½†æ˜¯ï¼Œå½“é¢å¯¹å¤šä¸ªæ•°æ®æºæˆ–éœ€è¦ç»“æ„åŒ–æŸ¥è¯¢æ—¶ï¼Œå¦‚ä½•æ™ºèƒ½åœ°é€‰æ‹©æ­£ç¡®çš„æ•°æ®æºå’Œæ„å»ºåˆé€‚çš„æŸ¥è¯¢å‘¢ï¼Ÿæœ¬ç« å°†ä»‹ç»è·¯ç”±æœºåˆ¶å’ŒæŸ¥è¯¢æ„å»ºæŠ€æœ¯ã€‚\n",
    "\n",
    "## ä¸ºä»€ä¹ˆéœ€è¦è·¯ç”±å’ŒæŸ¥è¯¢æ„å»ºï¼Ÿ\n",
    "\n",
    "### å®é™…åœºæ™¯ä¸­çš„æŒ‘æˆ˜\n",
    "\n",
    "```python\n",
    "# åœºæ™¯1: å¤šä¸ªæ•°æ®æº\n",
    "\n",
    "æ•°æ®æº1: æŠ€æœ¯æ–‡æ¡£æ•°æ®åº“\n",
    "æ•°æ®æº2: ç”¨æˆ·æ‰‹å†Œæ•°æ®åº“  \n",
    "æ•°æ®æº3: FAQçŸ¥è¯†åº“\n",
    "æ•°æ®æº4: APIå‚è€ƒæ–‡æ¡£\n",
    "\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"å¦‚ä½•ä½¿ç”¨Python SDKè¿æ¥æ•°æ®åº“ï¼Ÿ\"\n",
    "\n",
    "# åº”è¯¥æŸ¥è¯¢å“ªä¸ªæ•°æ®æºï¼Ÿ\n",
    "â†’ å•ä¸€æ•°æ®æºå¯èƒ½ä¸å¤Ÿ\n",
    "â†’ æŸ¥è¯¢æ‰€æœ‰æ•°æ®æºæ•ˆç‡ä½\n",
    "â†’ éœ€è¦æ™ºèƒ½è·¯ç”±æœºåˆ¶\n",
    "```\n",
    "\n",
    "```python\n",
    "# åœºæ™¯2: å¤æ‚æŸ¥è¯¢æ¡ä»¶\n",
    "\n",
    "å‘é‡æ•°æ®åº“åŒ…å«:\n",
    "- æ–‡æ¡£å†…å®¹ (embedding)\n",
    "- å…ƒæ•°æ®: \n",
    "  - ä½œè€…\n",
    "  - å‘å¸ƒæ—¥æœŸ\n",
    "  - æ–‡æ¡£ç±»å‹\n",
    "  - æ ‡ç­¾\n",
    "\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"æ‰¾å‡º2023å¹´å‘å¸ƒçš„å…³äºæœºå™¨å­¦ä¹ çš„æ–‡ç« \"\n",
    "\n",
    "# éœ€è¦åŒæ—¶è€ƒè™‘:\n",
    "â†’ è¯­ä¹‰ç›¸ä¼¼åº¦ (æœºå™¨å­¦ä¹ )\n",
    "â†’ ç»“æ„åŒ–æ¡ä»¶ (æ—¥æœŸ >= 2023-01-01)\n",
    "â†’ éœ€è¦æŸ¥è¯¢æ„å»ºæŠ€æœ¯\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390b0f9-82d6-4c5f-a8ba-88190b759be7",
   "metadata": {},
   "source": [
    "## æœ¬ç« å†…å®¹æ¦‚è§ˆ\n",
    "\n",
    "| æŠ€æœ¯ | æ ¸å¿ƒåŠŸèƒ½ | é€‚ç”¨åœºæ™¯ | å¤æ‚åº¦ |\n",
    "|:-----|:--------:|:--------:|:------:|\n",
    "| é€»è¾‘è·¯ç”± | åŸºäºè§„åˆ™çš„è·¯ç”± | ç¡®å®šæ€§è·¯ç”± | â­ |\n",
    "| è¯­ä¹‰è·¯ç”± | åŸºäºLLMçš„è·¯ç”± | çµæ´»è·¯ç”± | â­â­ |\n",
    "| ç»“æ„åŒ–æŸ¥è¯¢ | æ„å»ºfilteræ¡ä»¶ | å¸¦å…ƒæ•°æ®æŸ¥è¯¢ | â­â­ |\n",
    "| è‡ªæŸ¥è¯¢æ£€ç´¢å™¨ | è‡ªåŠ¨åˆ†ç¦»æŸ¥è¯¢æ„å›¾ | å¤æ‚æŸ¥è¯¢ | â­â­â­ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9080eb-a565-4158-9344-6daa83b4c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/shared-nvme/conda-envs/py310/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 20:07:21 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 11-11 20:07:21 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', speculative_config=None, tokenizer='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8)\n",
      "WARNING 11-11 20:07:22 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-11 20:07:22 utils.py:660] Found nccl from library /usr/lib/x86_64-linux-gnu/libnccl.so.2\n",
      "INFO 11-11 20:07:22 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 11-11 20:07:22 selector.py:32] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 20:07:23,909 - modelscope - INFO - PyTorch version 2.3.0+cu118 Found.\n",
      "2025-11-11 20:07:23,911 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-11-11 20:07:23,963 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 298ceecce207285dd10b135af16e71cc and a total number of 964 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 20:07:31 model_runner.py:175] Loading model weights took 8.4983 GB\n",
      "INFO 11-11 20:07:33 gpu_executor.py:114] # GPU blocks: 1404, # CPU blocks: 512\n",
      "INFO 11-11 20:07:36 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-11 20:07:36 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-11 20:07:45 model_runner.py:1017] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹å‡†å¤‡å·¥ä½œ\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 1. åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹\n",
    "local_model_path = \"./Models/maidalun/bce-embedding-base_v1\" \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=local_model_path,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 2. åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“\n",
    "vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# 3. åŠ è½½æœ¬åœ°å¤§æ¨¡å‹\n",
    "model_dir=\"../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8\"\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "llm = LLM(\n",
    "    model=model_dir,\n",
    "    tokenizer=model_dir,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938b958-16db-403b-aab4-12aa24506ba0",
   "metadata": {},
   "source": [
    "### å¾€å‘é‡æ•°æ®åº“ä¸­æ·»åŠ web_contenté›†åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8d278b-80a2-4edf-bc29-bd1692b92120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ›å»º web_content é›†åˆæˆåŠŸ\n",
      "ğŸš€ å¼€å§‹å¹¶è¡ŒåŠ è½½ 5 ä¸ªURL...\n",
      "âœ… æˆåŠŸåŠ è½½: https://www.hanspub.org/journal/PaperInformation?paperID=84081 -> 1 ä¸ªæ–‡æ¡£\n",
      "âœ… æˆåŠŸåŠ è½½: https://blog.csdn.net/WhiffeYF/article/details/110829105 -> 1 ä¸ªæ–‡æ¡£\n",
      "âœ… æˆåŠŸåŠ è½½: https://blog.csdn.net/Yyuan12345678/article/details/142108850?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-142108850.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187 -> 1 ä¸ªæ–‡æ¡£\n",
      "âœ… æˆåŠŸåŠ è½½: https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187 -> 1 ä¸ªæ–‡æ¡£\n",
      "âœ… æˆåŠŸåŠ è½½: https://blog.csdn.net/qq_40081208/article/details/111053208?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111053208.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187 -> 1 ä¸ªæ–‡æ¡£\n",
      "ğŸ“Š æ€»å…±åŠ è½½äº† 5 ä¸ªæ–‡æ¡£\n",
      "ğŸ’¾ æ­£åœ¨å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...\n",
      "âœ… æˆåŠŸæ·»åŠ  5 ä¸ªæ–‡æ¡£åˆ° web_content é›†åˆ\n",
      "\n",
      "ğŸ” éªŒè¯æ•°æ®åº“å†…å®¹:\n",
      "é›†åˆä¸­çš„æ–‡æ¡£æ•°é‡: 5\n",
      "\n",
      "ğŸ” æµ‹è¯•æ£€ç´¢ 'æŠ“å–æ£€æµ‹':\n",
      "   1. æ¥æº: https://blog.csdn.net/qq_40081208/article/details/111053208?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111053208.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\n",
      "      å†…å®¹é¢„è§ˆ:  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0_dexnet-CSDNåšå®¢\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "æŠ“å–æ£€æµ‹ä¹‹Dex-Ne...\n",
      "   2. æ¥æº: https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\n",
      "      å†…å®¹é¢„è§ˆ:  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåšå®¢\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# 2. åˆ›å»ºæ–°çš„é›†åˆ web_content\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"web_content\",  # æŒ‡å®šé›†åˆåç§°\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"âœ… åˆ›å»º web_content é›†åˆæˆåŠŸ\")\n",
    "\n",
    "def load_single_url(url: str):\n",
    "    \"\"\"åŠ è½½å•ä¸ªURL\"\"\"\n",
    "    try:\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½: {url} -> {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½å¤±è´¥: {url} -> é”™è¯¯: {e}\")\n",
    "        return []\n",
    "\n",
    "async def load_urls_parallel(urls: list, max_workers: int = 5):\n",
    "    \"\"\"å¹¶è¡ŒåŠ è½½å¤šä¸ªURL\"\"\"\n",
    "    print(f\"ğŸš€ å¼€å§‹å¹¶è¡ŒåŠ è½½ {len(urls)} ä¸ªURL...\")\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡\n",
    "        tasks = [\n",
    "            loop.run_in_executor(executor, load_single_url, url)\n",
    "            for url in urls\n",
    "        ]\n",
    "        \n",
    "        # å¹¶è¡Œæ‰§è¡Œ\n",
    "        all_docs = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰æ–‡æ¡£\n",
    "    flat_docs = []\n",
    "    for docs in all_docs:\n",
    "        flat_docs.extend(docs)\n",
    "    \n",
    "    print(f\"ğŸ“Š æ€»å…±åŠ è½½äº† {len(flat_docs)} ä¸ªæ–‡æ¡£\")\n",
    "    return flat_docs\n",
    "\n",
    "def add_docs_to_vectorstore(docs: list):\n",
    "    \"\"\"å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“\"\"\"\n",
    "    if not docs:\n",
    "        print(\"âš ï¸ æ²¡æœ‰æ–‡æ¡£å¯æ·»åŠ \")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ’¾ æ­£åœ¨å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...\")\n",
    "    \n",
    "    # æ·»åŠ æ–‡æ¡£åˆ°é›†åˆ\n",
    "    vectorstore.add_documents(docs)\n",
    "    \n",
    "    # æŒä¹…åŒ–ä¿å­˜\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    print(f\"âœ… æˆåŠŸæ·»åŠ  {len(docs)} ä¸ªæ–‡æ¡£åˆ° web_content é›†åˆ\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "async def main():\n",
    "    # æ‚¨çš„URLåˆ—è¡¨ï¼ˆè¯·åœ¨è¿™é‡Œå¡«å†™æ‚¨çš„é“¾æ¥ï¼‰\n",
    "    url_list = [\n",
    "        \"https://blog.csdn.net/qq_40081208/article/details/111053208?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111053208.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\",\n",
    "        \"https://blog.csdn.net/Yyuan12345678/article/details/142108850?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-142108850.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\",\n",
    "        \"https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\",\n",
    "        \"https://www.hanspub.org/journal/PaperInformation?paperID=84081\",\n",
    "        \"https://blog.csdn.net/WhiffeYF/article/details/110829105\"\n",
    "    ]\n",
    "    \n",
    "    if not url_list:\n",
    "        print(\"âš ï¸ è¯·å…ˆåœ¨ url_list ä¸­æ·»åŠ URLé“¾æ¥\")\n",
    "        return\n",
    "    \n",
    "    # 1. å¹¶è¡ŒåŠ è½½æ‰€æœ‰URL\n",
    "    all_docs = await load_urls_parallel(url_list)\n",
    "    \n",
    "    # 2. æ·»åŠ åˆ°å‘é‡æ•°æ®åº“\n",
    "    add_docs_to_vectorstore(all_docs)\n",
    "    \n",
    "    # 3. éªŒè¯æ·»åŠ ç»“æœ\n",
    "    print(\"\\nğŸ” éªŒè¯æ•°æ®åº“å†…å®¹:\")\n",
    "    collection_info = vectorstore._client.get_collection(\"web_content\")\n",
    "    print(f\"é›†åˆä¸­çš„æ–‡æ¡£æ•°é‡: {collection_info.count()}\")\n",
    "    \n",
    "    # 4. æµ‹è¯•æ£€ç´¢\n",
    "    test_query = \"æŠ“å–æ£€æµ‹\"\n",
    "    results = vectorstore.similarity_search(test_query, k=2)\n",
    "    print(f\"\\nğŸ” æµ‹è¯•æ£€ç´¢ '{test_query}':\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"   {i}. æ¥æº: {doc.metadata.get('source', 'N/A')}\")\n",
    "        print(f\"      å†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d45589-49d8-4464-a363-96bbf32f3cc9",
   "metadata": {},
   "source": [
    "## Part1: é€»è¾‘è·¯ç”± - Logical Routing\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ:é€»è¾‘è·¯ç”±ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•æ¥å†³å®šå°†æŸ¥è¯¢å‘é€åˆ°å“ªä¸ªæ•°æ®æºã€‚å®ƒé€šè¿‡LLMç†è§£æŸ¥è¯¢å†…å®¹ï¼Œç„¶åæ ¹æ®é¢„å®šä¹‰çš„è§„åˆ™é€‰æ‹©åˆé€‚çš„æ•°æ®æºã€‚\n",
    "\n",
    "å·¥ä½œåŸç†ï¼š\n",
    "\n",
    "```python\n",
    "ç”¨æˆ·æŸ¥è¯¢\n",
    "    â†“\n",
    "LLMåˆ†ææŸ¥è¯¢æ„å›¾\n",
    "    â†“\n",
    "åŒ¹é…é¢„å®šä¹‰çš„è·¯ç”±è§„åˆ™\n",
    "    â†“\n",
    "é€‰æ‹©ç›®æ ‡æ•°æ®æº\n",
    "    â†“\n",
    "æ‰§è¡Œæ£€ç´¢\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f2bed7-dd14-4eb0-b055-6d9bb789ceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: å¦‚ä½•åœ¨æ•°æ®åº“ä¸­æŸ¥è¯¢ï¼Ÿ\n",
      "Route: æ•°æ®åº“\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\n",
      "Route: web_search\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰è·¯ç”±æç¤ºè¯ï¼ˆChatMLæ ¼å¼ï¼‰\n",
    "route_prompt_template = \"\"\"<|im_start|>system\n",
    "ä½ æ˜¯ä¸€ä¸ªè·¯ç”±åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·æŸ¥è¯¢å‘é€åˆ°æ­£ç¡®çš„æ•°æ®æºã€‚\n",
    "\n",
    "å¯ç”¨çš„æ•°æ®æº:\n",
    "- python_docs: Pythonç¼–ç¨‹ç›¸å…³çš„æŠ€æœ¯æ–‡æ¡£\n",
    "- web_search: éœ€è¦å®æ—¶ä¿¡æ¯æˆ–ä¸€èˆ¬æ€§é—®é¢˜\n",
    "- database: æ•°æ®åº“ç›¸å…³çš„æŸ¥è¯¢\n",
    "\n",
    "è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›æœ€åˆé€‚çš„æ•°æ®æºåç§°(åªè¿”å›åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹)ã€‚<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# åˆ›å»ºè·¯ç”±é“¾\n",
    "def route_chain(question: str) -> str:\n",
    "    prompt = route_prompt_template.format(question=question)\n",
    "    \n",
    "    # ä½¿ç”¨ vLLM çš„æ­£ç¡®è°ƒç”¨æ–¹å¼\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=50,\n",
    "        stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "    )\n",
    "    \n",
    "    # ç”Ÿæˆå“åº”\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    \n",
    "    # æå–ç»“æœ\n",
    "    if outputs and len(outputs) > 0:\n",
    "        output = outputs[0]\n",
    "        if hasattr(output, 'outputs') and output.outputs:\n",
    "            return output.outputs[0].text.strip()\n",
    "    \n",
    "    return \"web_search\"  # é»˜è®¤è¿”å›\n",
    "\n",
    "# æµ‹è¯•è·¯ç”±\n",
    "question1 = \"å¦‚ä½•åœ¨æ•°æ®åº“ä¸­æŸ¥è¯¢ï¼Ÿ\"\n",
    "route1 = route_chain(question1)\n",
    "print(f\"Query: {question1}\")\n",
    "print(f\"Route: {route1}\\n\")\n",
    "\n",
    "question2 = \"ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"\n",
    "route2 = route_chain(question2)\n",
    "print(f\"Query: {question2}\")\n",
    "print(f\"Route: {route2}\\n\")\n",
    "\n",
    "# è¾“å‡ºç¤ºä¾‹:\n",
    "# Route: python_docs\n",
    "# Route: web_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea255fde-40e5-43be-87d7-3cecec12f321",
   "metadata": {},
   "source": [
    "### å®ç°å®Œæ•´è·¯ç”±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b30632-a299-4534-bd6a-ce1af9bc6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from vllm import SamplingParams\n",
    "\n",
    "class LogicalRouter:\n",
    "    \"\"\"é€»è¾‘è·¯ç”±å™¨ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, embeddings, persist_directory=\"./chroma_db\"):\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.persist_directory = persist_directory\n",
    "        self.routes = {}\n",
    "        self.route_descriptions = {}  # å•ç‹¬å­˜å‚¨æè¿°\n",
    "        \n",
    "        # åˆå§‹åŒ–æ‚¨çš„ä¸¤ä¸ªé›†åˆ\n",
    "        self._init_collections()\n",
    "    \n",
    "    def _init_collections(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ‚¨çš„ä¸¤ä¸ªå‘é‡æ•°æ®åº“é›†åˆ\"\"\"\n",
    "        # 1. langchain é›†åˆï¼ˆè®ºæ–‡ï¼‰\n",
    "        self.langchain_collection = Chroma(\n",
    "            collection_name=\"langchain\",\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # 2. web_content é›†åˆï¼ˆç½‘é¡µå†…å®¹ï¼‰\n",
    "        self.web_content_collection = Chroma(\n",
    "            collection_name=\"web_content\",\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # è‡ªåŠ¨æ·»åŠ è·¯ç”±\n",
    "        self.add_route(\"langchain\", self.langchain_collection.as_retriever(), \n",
    "                      \"å­¦æœ¯è®ºæ–‡å’ŒæŠ€æœ¯æ–‡æ¡£ï¼ŒåŒ…å«æŠ“å–æ£€æµ‹ã€æ»‘åŠ¨æ£€æµ‹ç›¸å…³çš„ç ”ç©¶è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£\")\n",
    "        self.add_route(\"web_content\", self.web_content_collection.as_retriever(), \n",
    "                      \"ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«CSDNåšå®¢ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ç­‰ç½‘é¡µæ–‡ç« \")\n",
    "    \n",
    "    def add_route(self, name: str, retriever, description: str = None):\n",
    "        \"\"\"æ·»åŠ è·¯ç”±\"\"\"\n",
    "        self.routes[name] = retriever\n",
    "        if description:\n",
    "            self.route_descriptions[name] = description  # å•ç‹¬å­˜å‚¨æè¿°\n",
    "    \n",
    "    def _call_llm(self, prompt: str, max_tokens: int = 100) -> str:\n",
    "        \"\"\"ç›´æ¥è°ƒç”¨vLLMæ¨¡å‹\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        \n",
    "        if outputs and outputs[0].outputs:\n",
    "            return outputs[0].outputs[0].text.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def route(self, question: str):\n",
    "        \"\"\"æ‰§è¡Œè·¯ç”±\"\"\"\n",
    "        # æ„å»ºè·¯ç”±æç¤ºè¯\n",
    "        route_descriptions = self._get_route_descriptions()\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªè·¯ç”±åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·æŸ¥è¯¢å‘é€åˆ°æ­£ç¡®çš„æ•°æ®æºã€‚\n",
    "            \n",
    "            å¯ç”¨çš„æ•°æ®æºï¼š\n",
    "            {route_descriptions}\n",
    "            \n",
    "            è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›æœ€åˆé€‚çš„æ•°æ®æºåç§°ï¼ˆåªè¿”å›åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            é—®é¢˜ï¼š{question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            æ•°æ®æºï¼š\"\"\"\n",
    "        \n",
    "        # è°ƒç”¨vLLMæ¨¡å‹\n",
    "        route_name = self._call_llm(prompt, max_tokens=50)\n",
    "        route_name = route_name.strip().lower()\n",
    "        \n",
    "        print(f\"ğŸ¤– LLMè·¯ç”±å†³ç­–: '{route_name}'\")\n",
    "        \n",
    "        # æ£€æŸ¥è·¯ç”±æ˜¯å¦å­˜åœ¨\n",
    "        if route_name not in self.routes:\n",
    "            # å°è¯•æ¨¡ç³ŠåŒ¹é…\n",
    "            for name in self.routes.keys():\n",
    "                if name in route_name or route_name in name:\n",
    "                    route_name = name\n",
    "                    print(f\"ğŸ”„ æ¨¡ç³ŠåŒ¹é…åˆ°: {route_name}\")\n",
    "                    break\n",
    "            else:\n",
    "                # é»˜è®¤å›é€€åˆ° web_content\n",
    "                print(f\"âš ï¸ æœªæ‰¾åˆ°è·¯ç”±: {route_name}ï¼Œä½¿ç”¨é»˜è®¤è·¯ç”±: web_content\")\n",
    "                route_name = \"web_content\"\n",
    "        \n",
    "        return route_name, self.routes[route_name]\n",
    "    \n",
    "    def _get_route_descriptions(self) -> str:\n",
    "        \"\"\"è·å–è·¯ç”±æè¿°\"\"\"\n",
    "        descriptions = []\n",
    "        for name, retriever in self.routes.items():\n",
    "            desc = self.route_descriptions.get(name, f'{name}æ•°æ®æº')\n",
    "            descriptions.append(f\"- {name}: {desc}\")\n",
    "        return \"\\n\".join(descriptions)\n",
    "    \n",
    "    def query(self, question: str, k: int = 4):\n",
    "        \"\"\"æ‰§è¡Œå®Œæ•´æŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¯ æŸ¥è¯¢: {question}\")\n",
    "        \n",
    "        # è·¯ç”±åˆ°æ­£ç¡®çš„æ•°æ®æº\n",
    "        route_name, retriever = self.route(question)\n",
    "        print(f\"ğŸ“ è·¯ç”±åˆ°: {route_name}\")\n",
    "        \n",
    "        # æ‰§è¡Œæ£€ç´¢\n",
    "        docs = retriever.get_relevant_documents(question, k=k)\n",
    "        print(f\"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        return {\n",
    "            \"route\": route_name,\n",
    "            \"documents\": docs,\n",
    "            \"question\": question,\n",
    "            \"collection_size\": self._get_collection_size(route_name)\n",
    "        }\n",
    "    \n",
    "    def _get_collection_size(self, collection_name: str) -> int:\n",
    "        \"\"\"è·å–é›†åˆä¸­çš„æ–‡æ¡£æ•°é‡\"\"\"\n",
    "        try:\n",
    "            if collection_name == \"langchain\":\n",
    "                collection = self.langchain_collection._client.get_collection(\"langchain\")\n",
    "            else:\n",
    "                collection = self.web_content_collection._client.get_collection(\"web_content\")\n",
    "            return collection.count()\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def query_both(self, question: str, k: int = 3):\n",
    "        \"\"\"åŒæ—¶æŸ¥è¯¢ä¸¤ä¸ªæ•°æ®æº\"\"\"\n",
    "        print(f\"ğŸ¯ æŸ¥è¯¢ä¸¤ä¸ªæ•°æ®æº: {question}\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for route_name, retriever in self.routes.items():\n",
    "            print(f\"ğŸ” æŸ¥è¯¢ {route_name}...\")\n",
    "            docs = retriever.get_relevant_documents(question, k=k)\n",
    "            results[route_name] = {\n",
    "                \"documents\": docs,\n",
    "                \"count\": len(docs)\n",
    "            }\n",
    "            print(f\"   ğŸ“Š {route_name}: æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    def get_collection_info(self):\n",
    "        \"\"\"è·å–é›†åˆä¿¡æ¯\"\"\"\n",
    "        info = {}\n",
    "        for name in [\"langchain\", \"web_content\"]:\n",
    "            try:\n",
    "                if name == \"langchain\":\n",
    "                    collection = self.langchain_collection._client.get_collection(\"langchain\")\n",
    "                else:\n",
    "                    collection = self.web_content_collection._client.get_collection(\"web_content\")\n",
    "                info[name] = {\n",
    "                    \"document_count\": collection.count(),\n",
    "                    \"description\": self.route_descriptions.get(name, 'N/A')\n",
    "                }\n",
    "            except Exception as e:\n",
    "                info[name] = {\"error\": str(e)}\n",
    "        \n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33438814-cc79-4f80-9ed7-ed8710e61c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š é›†åˆä¿¡æ¯:\n",
      "   langchain: 12721 ä¸ªæ–‡æ¡£ - å­¦æœ¯è®ºæ–‡å’ŒæŠ€æœ¯æ–‡æ¡£ï¼ŒåŒ…å«æŠ“å–æ£€æµ‹ã€æ»‘åŠ¨æ£€æµ‹ç›¸å…³çš„ç ”ç©¶è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£\n",
      "   web_content: 5 ä¸ªæ–‡æ¡£ - ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«CSDNåšå®¢ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ç­‰ç½‘é¡µæ–‡ç« \n"
     ]
    }
   ],
   "source": [
    "router = LogicalRouter(llm, embeddings, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# æŸ¥çœ‹é›†åˆä¿¡æ¯\n",
    "print(\"ğŸ“Š é›†åˆä¿¡æ¯:\")\n",
    "info = router.get_collection_info()\n",
    "for name, data in info.items():\n",
    "    if \"document_count\" in data:\n",
    "        print(f\"   {name}: {data['document_count']} ä¸ªæ–‡æ¡£ - {data['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f6c16bf-6412-4e36-ae61-b3098122ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ æŸ¥è¯¢: ç½‘é¡µæ–‡æ¡£ä¸­å…³äºæŠ“å–æ£€æµ‹çš„ä»‹ç»\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLMè·¯ç”±å†³ç­–: 'web_content'\n",
      "ğŸ“ è·¯ç”±åˆ°: web_content\n",
      "ğŸ“š æ£€ç´¢åˆ° 4 ä¸ªæ–‡æ¡£\n",
      "è·¯ç”±ç»“æœ: web_content\n",
      "æ–‡æ¡£æ•°é‡: 4\n",
      "  1. æ¥æº: https://blog.csdn.net/qq_40081208/article/details/111053208?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111053208.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\n",
      "  2. æ¥æº: https://blog.csdn.net/WhiffeYF/article/details/110829105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = router.query(\"ç½‘é¡µæ–‡æ¡£ä¸­å…³äºæŠ“å–æ£€æµ‹çš„ä»‹ç»\")\n",
    "print(f\"è·¯ç”±ç»“æœ: {result['route']}\")\n",
    "print(f\"æ–‡æ¡£æ•°é‡: {len(result['documents'])}\")\n",
    "\n",
    "# æ˜¾ç¤ºå‰ä¸¤ä¸ªæ–‡æ¡£çš„é¢„è§ˆ\n",
    "for i, doc in enumerate(result['documents'][:2], 1):\n",
    "    source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')\n",
    "    print(f\"  {i}. æ¥æº: {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1557fb67-4172-41b7-8609-212021b54e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ æŸ¥è¯¢: è®ºæ–‡ä¸­å…³äºæŠ“å–æ£€æµ‹çš„ä»‹ç»\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLMè·¯ç”±å†³ç­–: 'langchain'\n",
      "ğŸ“ è·¯ç”±åˆ°: langchain\n",
      "ğŸ“š æ£€ç´¢åˆ° 4 ä¸ªæ–‡æ¡£\n",
      "è·¯ç”±ç»“æœ: langchain\n",
      "æ–‡æ¡£æ•°é‡: 4\n",
      "  1. æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦è§‰èåˆçš„æœºå™¨äººç‰©ä½“è¯†åˆ«å’ŒæŠ“å–ç¨³å®šæ€§æ£€æµ‹çš„ç ”ç©¶ä¸åº”ç”¨_ä¸Šå®˜æ˜é›¨.pdf\n",
      "  2. æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = router.query(\"è®ºæ–‡ä¸­å…³äºæŠ“å–æ£€æµ‹çš„ä»‹ç»\")\n",
    "print(f\"è·¯ç”±ç»“æœ: {result['route']}\")\n",
    "print(f\"æ–‡æ¡£æ•°é‡: {len(result['documents'])}\")\n",
    "\n",
    "# æ˜¾ç¤ºå‰ä¸¤ä¸ªæ–‡æ¡£çš„é¢„è§ˆ\n",
    "for i, doc in enumerate(result['documents'][:2], 1):\n",
    "    source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')\n",
    "    print(f\"  {i}. æ¥æº: {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710114e-77a4-445c-8431-4ef2b6a7ef36",
   "metadata": {},
   "source": [
    "### ä¼˜åŒ–ï¼šå¸¦å›é€€æœºåˆ¶çš„è·¯ç”±\n",
    "\n",
    "æ‰§è¡Œæµç¨‹ï¼š\n",
    "\n",
    "```python\n",
    "ç”¨æˆ·è¾“å…¥é—®é¢˜ï¼š\"Pythonä¸­çš„è£…é¥°å™¨æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "        â†“\n",
    "è°ƒç”¨ query_multiple(\"Pythonä¸­çš„è£…é¥°å™¨æ˜¯ä»€ä¹ˆï¼Ÿ\", max_routes=2)\n",
    "        â†“\n",
    "1. æ‰§è¡Œä¸»è·¯ç”± route(\"Pythonä¸­çš„è£…é¥°å™¨æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "        â†“\n",
    "   [è·¯ç”±é€»è¾‘]ï¼šæ£€æŸ¥é—®é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯\n",
    "        â”œâ”€â”€ åŒ…å«\"Python\" â†’ åŒ¹é…åˆ°python_docsè·¯ç”±\n",
    "        â””â”€â”€ æˆåŠŸè¿”å›ï¼š(\"python_docs\", python_docs_retriever)\n",
    "        â†“\n",
    "2. æ£€æŸ¥è·¯ç”±æ˜¯å¦æˆåŠŸï¼Ÿ â†’ âœ… æˆåŠŸ\n",
    "        â†“\n",
    "   è·å–ä¸»è·¯ç”±æ–‡æ¡£ï¼špython_docs_retriever.get_relevant_documents(...)\n",
    "        â†“\n",
    "   å¾—åˆ°ï¼š[æ–‡æ¡£1, æ–‡æ¡£2, æ–‡æ¡£3]ï¼ˆå…³äºè£…é¥°å™¨çš„æŠ€æœ¯æ–‡æ¡£ï¼‰\n",
    "        â†“\n",
    "3. æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ å›é€€æ•°æ®æºï¼Ÿ\n",
    "        â”œâ”€â”€ å½“å‰ç»“æœæ•°ï¼š1ä¸ªæ•°æ®æº (python_docs)\n",
    "        â”œâ”€â”€ max_routesï¼š2ä¸ªæ•°æ®æº\n",
    "        â”œâ”€â”€ ä¸»è·¯ç”±(python_docs) â‰  å›é€€è·¯ç”±(web_search) â†’ âœ…\n",
    "        â””â†’ éœ€è¦æ·»åŠ å›é€€æ•°æ®æº\n",
    "        â†“\n",
    "   è·å–å›é€€è·¯ç”±æ–‡æ¡£ï¼šweb_search_retriever.get_relevant_documents(...)\n",
    "        â†“\n",
    "   å¾—åˆ°ï¼š[æ–‡æ¡£A, æ–‡æ¡£B]ï¼ˆç½‘é¡µä¸Šçš„è£…é¥°å™¨æ•™ç¨‹ï¼‰\n",
    "        â†“\n",
    "4. è¿”å›å¤šæ•°æ®æºç»“æœ\n",
    "        â”œâ”€â”€ \"python_docs\": [æ–‡æ¡£1, æ–‡æ¡£2, æ–‡æ¡£3]\n",
    "        â””â”€â”€ \"web_search\": [æ–‡æ¡£A, æ–‡æ¡£B]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51103cfa-0a77-4e61-8cf7-af4f8096a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "class LogicalRouterWithFallback:\n",
    "    \"\"\"å¸¦å›é€€æœºåˆ¶çš„é€»è¾‘è·¯ç”±å™¨ï¼ˆé€‚é…æ‚¨çš„é…ç½®ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, embeddings, fallback_route: str = \"web_content\", persist_directory=\"./chroma_db\"):\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.persist_directory = persist_directory\n",
    "        self.fallback_route = fallback_route\n",
    "        self.routes = {}\n",
    "        self.route_descriptions = {}\n",
    "        \n",
    "        # åˆå§‹åŒ–æ‚¨çš„ä¸¤ä¸ªé›†åˆ\n",
    "        self._init_collections()\n",
    "    \n",
    "    def _init_collections(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ‚¨çš„ä¸¤ä¸ªå‘é‡æ•°æ®åº“é›†åˆ\"\"\"\n",
    "        # 1. langchain é›†åˆï¼ˆè®ºæ–‡ï¼‰\n",
    "        self.langchain_collection = Chroma(\n",
    "            collection_name=\"langchain\",\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # 2. web_content é›†åˆï¼ˆç½‘é¡µå†…å®¹ï¼‰\n",
    "        self.web_content_collection = Chroma(\n",
    "            collection_name=\"web_content\",\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # è‡ªåŠ¨æ·»åŠ è·¯ç”±\n",
    "        self.add_route(\"langchain\", self.langchain_collection.as_retriever(), \n",
    "                      \"å­¦æœ¯è®ºæ–‡å’ŒæŠ€æœ¯æ–‡æ¡£ï¼ŒåŒ…å«LangChainç›¸å…³çš„ç ”ç©¶è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£\")\n",
    "        self.add_route(\"web_content\", self.web_content_collection.as_retriever(), \n",
    "                      \"ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«CSDNåšå®¢ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ç­‰ç½‘é¡µæ–‡ç« \")\n",
    "    \n",
    "    def add_route(self, name: str, retriever, description: str = None):\n",
    "        \"\"\"æ·»åŠ è·¯ç”±\"\"\"\n",
    "        self.routes[name] = retriever\n",
    "        if description:\n",
    "            self.route_descriptions[name] = description\n",
    "    \n",
    "    def _call_llm(self, prompt: str, max_tokens: int = 100) -> str:\n",
    "        \"\"\"ç›´æ¥è°ƒç”¨vLLMæ¨¡å‹\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        \n",
    "        if outputs and outputs[0].outputs:\n",
    "            return outputs[0].outputs[0].text.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def route(self, question: str):\n",
    "        \"\"\"æ‰§è¡Œè·¯ç”±ï¼Œå¸¦å›é€€æœºåˆ¶\"\"\"\n",
    "        try:\n",
    "            # æ„å»ºè·¯ç”±æç¤ºè¯\n",
    "            route_descriptions = self._get_route_descriptions()\n",
    "            \n",
    "            prompt = f\"\"\"<|im_start|>system\n",
    "                ä½ æ˜¯ä¸€ä¸ªè·¯ç”±åŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·æŸ¥è¯¢å‘é€åˆ°æ­£ç¡®çš„æ•°æ®æºã€‚\n",
    "                \n",
    "                å¯ç”¨çš„æ•°æ®æºï¼š\n",
    "                {route_descriptions}\n",
    "                \n",
    "                è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›æœ€åˆé€‚çš„æ•°æ®æºåç§°ï¼ˆåªè¿”å›åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ã€‚<|im_end|>\n",
    "                <|im_start|>user\n",
    "                é—®é¢˜ï¼š{question}<|im_end|>\n",
    "                <|im_start|>assistant\n",
    "                æ•°æ®æºï¼š\"\"\"\n",
    "            \n",
    "            # è°ƒç”¨vLLMæ¨¡å‹\n",
    "            route_name = self._call_llm(prompt, max_tokens=50)\n",
    "            route_name = route_name.strip().lower()\n",
    "            \n",
    "            print(f\"ğŸ¤– LLMè·¯ç”±å†³ç­–: '{route_name}'\")\n",
    "            \n",
    "            # æ£€æŸ¥è·¯ç”±æ˜¯å¦å­˜åœ¨\n",
    "            if route_name not in self.routes:\n",
    "                # å°è¯•æ¨¡ç³ŠåŒ¹é…\n",
    "                for name in self.routes.keys():\n",
    "                    if name in route_name or route_name in name:\n",
    "                        route_name = name\n",
    "                        print(f\"ğŸ”„ æ¨¡ç³ŠåŒ¹é…åˆ°: {route_name}\")\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"æœªæ‰¾åˆ°è·¯ç”±: {route_name}\")\n",
    "            \n",
    "            return route_name, self.routes[route_name]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è·¯ç”±å¤±è´¥: {e}, ä½¿ç”¨å›é€€è·¯ç”±: {self.fallback_route}\")\n",
    "            return self.fallback_route, self.routes[self.fallback_route]\n",
    "    \n",
    "    def _get_route_descriptions(self) -> str:\n",
    "        \"\"\"è·å–è·¯ç”±æè¿°\"\"\"\n",
    "        descriptions = []\n",
    "        for name, retriever in self.routes.items():\n",
    "            desc = self.route_descriptions.get(name, f'{name}æ•°æ®æº')\n",
    "            descriptions.append(f\"- {name}: {desc}\")\n",
    "        return \"\\n\".join(descriptions)\n",
    "    \n",
    "    def query_multiple(self, question: str, max_routes: int = 2, k: int = 3):\n",
    "        \"\"\"æŸ¥è¯¢å¤šä¸ªæ•°æ®æºï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰\"\"\"\n",
    "        print(f\"ğŸ¯ å¤šæ•°æ®æºæŸ¥è¯¢: {question}\")\n",
    "        \n",
    "        # è·å–ä¸»è·¯ç”±\n",
    "        primary_route, primary_retriever = self.route(question)\n",
    "        print(f\"ğŸ“ ä¸»è·¯ç”±: {primary_route}\")\n",
    "        \n",
    "        # è·å–ä¸»è·¯ç”±æ–‡æ¡£\n",
    "        primary_docs = primary_retriever.get_relevant_documents(question, k=k)\n",
    "        results = {\n",
    "            primary_route: primary_docs\n",
    "        }\n",
    "        print(f\"ğŸ“š {primary_route}: {len(primary_docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        # å¦‚æœéœ€è¦ï¼Œæ·»åŠ å›é€€è·¯ç”±\n",
    "        if len(results) < max_routes and primary_route != self.fallback_route:\n",
    "            print(f\"ğŸ”„ æ·»åŠ å›é€€è·¯ç”±: {self.fallback_route}\")\n",
    "            fallback_docs = self.routes[self.fallback_route].get_relevant_documents(question, k=k)\n",
    "            results[self.fallback_route] = fallback_docs\n",
    "            print(f\"ğŸ“š {self.fallback_route}: {len(fallback_docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def query_single(self, question: str, k: int = 4):\n",
    "        \"\"\"å•æ•°æ®æºæŸ¥è¯¢ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
    "        route_name, retriever = self.route(question)\n",
    "        docs = retriever.get_relevant_documents(question, k=k)\n",
    "        \n",
    "        return {\n",
    "            \"route\": route_name,\n",
    "            \"documents\": docs,\n",
    "            \"question\": question\n",
    "        }\n",
    "    \n",
    "    def query_both(self, question: str, k: int = 3):\n",
    "        \"\"\"åŒæ—¶æŸ¥è¯¢ä¸¤ä¸ªæ•°æ®æº\"\"\"\n",
    "        print(f\"ğŸ¯ æŸ¥è¯¢ä¸¤ä¸ªæ•°æ®æº: {question}\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for route_name, retriever in self.routes.items():\n",
    "            print(f\"ğŸ” æŸ¥è¯¢ {route_name}...\")\n",
    "            docs = retriever.get_relevant_documents(question, k=k)\n",
    "            results[route_name] = docs\n",
    "            print(f\"   ğŸ“Š {route_name}: {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_collection_info(self):\n",
    "        \"\"\"è·å–é›†åˆä¿¡æ¯\"\"\"\n",
    "        info = {}\n",
    "        for name in [\"langchain\", \"web_content\"]:\n",
    "            try:\n",
    "                if name == \"langchain\":\n",
    "                    collection = self.langchain_collection._client.get_collection(\"langchain\")\n",
    "                else:\n",
    "                    collection = self.web_content_collection._client.get_collection(\"web_content\")\n",
    "                info[name] = {\n",
    "                    \"document_count\": collection.count(),\n",
    "                    \"description\": self.route_descriptions.get(name, 'N/A')\n",
    "                }\n",
    "            except Exception as e:\n",
    "                info[name] = {\"error\": str(e)}\n",
    "        \n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e1a5cdb-5e04-4c6c-9203-129902e88825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š é›†åˆä¿¡æ¯:\n",
      "   langchain: 12721 ä¸ªæ–‡æ¡£ - å­¦æœ¯è®ºæ–‡å’ŒæŠ€æœ¯æ–‡æ¡£ï¼ŒåŒ…å«LangChainç›¸å…³çš„ç ”ç©¶è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£\n",
      "   web_content: 5 ä¸ªæ–‡æ¡£ - ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«CSDNåšå®¢ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ç­‰ç½‘é¡µæ–‡ç« \n"
     ]
    }
   ],
   "source": [
    "#  åˆ›å»ºå¸¦å›é€€æœºåˆ¶çš„è·¯ç”±å™¨\n",
    "router = LogicalRouterWithFallback(\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    "    fallback_route=\"web_content\",  # é»˜è®¤å›é€€åˆ°ç½‘é¡µå†…å®¹\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "#  æŸ¥çœ‹é›†åˆä¿¡æ¯\n",
    "print(\"ğŸ“Š é›†åˆä¿¡æ¯:\")\n",
    "info = router.get_collection_info()\n",
    "for name, data in info.items():\n",
    "    if \"document_count\" in data:\n",
    "        print(f\"   {name}: {data['document_count']} ä¸ªæ–‡æ¡£ - {data['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2b75a66-1b2f-42da-a346-358bdd584eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å¤šæ•°æ®æºæŸ¥è¯¢: è®ºæ–‡ä¸­æŠ“å–æ£€æµ‹çš„å®šä¹‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLMè·¯ç”±å†³ç­–: 'langchain'\n",
      "ğŸ“ ä¸»è·¯ç”±: langchain\n",
      "ğŸ“š langchain: 4 ä¸ªæ–‡æ¡£\n",
      "ğŸ”„ æ·»åŠ å›é€€è·¯ç”±: web_content\n",
      "ğŸ“š web_content: 4 ä¸ªæ–‡æ¡£\n",
      "\n",
      "langchain: 4 ä¸ªæ–‡æ¡£\n",
      "  1. æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦è§‰èåˆçš„æœºå™¨äººç‰©ä½“è¯†åˆ«å’ŒæŠ“å–ç¨³å®šæ€§æ£€æµ‹çš„ç ”ç©¶ä¸åº”ç”¨_ä¸Šå®˜æ˜é›¨.pdf\n",
      "     å†…å®¹: 1.4 æœ¬è®ºæ–‡ç»„ç»‡ç»“æ„  æœ¬æ–‡æ€»ä½“æ¡†æ¶å›¾å¦‚å›¾ 1.4 æ‰€ç¤ºã€‚æœ¬æ–‡å°†ç ”ç©¶å†…å®¹åˆ†ä¸ºå…­ä¸ªç« èŠ‚è¿›è¡Œé˜è¿°ï¼š  ç¬¬ä¸€ç« ï¼Œç»ªè®ºã€‚æœ¬ç« ä¸»è¦ä»‹ç»äº†è¯¾é¢˜çš„ç ”ç©¶èƒŒæ™¯ä¸ç ”ç©¶æ„ä¹‰ï¼Œåˆ†æäº†...\n",
      "  2. æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n",
      "     å†…å®¹: è¥¿å—ç§‘æŠ€å¤§å­¦ç¡•å£«å­¦ä½è®ºæ–‡  32    å›¾ 4-2 æŠ“å–è¿‡ç¨‹   Fig.4-2 Grabbing Process   ï¼ˆ1ï¼‰æŠ“æ‰‹æ‰“å¼€é˜¶æ®µï¼šæŠ“æ‰‹åœ¨åˆå§‹ä½ç½® å°†æŠ“...\n",
      "\n",
      "web_content: 4 ä¸ªæ–‡æ¡£\n",
      "  1. æ¥æº: https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\n",
      "     å†…å®¹: æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåš...\n",
      "  2. æ¥æº: https://blog.csdn.net/qq_40081208/article/details/111053208?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111053208.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\n",
      "     å†…å®¹: æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0_dexnet-CSDNåšå®¢...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# å¤šæ•°æ®æºæŸ¥è¯¢\n",
    "query = \"è®ºæ–‡ä¸­æŠ“å–æ£€æµ‹çš„å®šä¹‰\"\n",
    "results = router.query_multiple(query, max_routes=2, k=3)\n",
    "\n",
    "for route_name, docs in results.items():\n",
    "    print(f\"\\n{route_name}: {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "    for i, doc in enumerate(docs[:2], 1):\n",
    "        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')\n",
    "        print(f\"  {i}. æ¥æº: {source}\")\n",
    "        clean_content = doc.page_content[:80].replace('\\n', ' ').replace('\\r', '').strip()\n",
    "        print(f\"     å†…å®¹: {clean_content}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c6813-7acb-48a1-80c2-8acd9a4b0798",
   "metadata": {},
   "source": [
    "é€»è¾‘è·¯ç”±çš„ä¼˜ç¼ºç‚¹ï¼š\n",
    "\n",
    "ä¼˜ç‚¹ï¼šå¯é¢„æµ‹å’Œå¯æ§ï¼Œæ˜“äºç†è§£å’Œè°ƒè¯•ï¼Œé€‚åˆç¡®å®šæ€§åœºæ™¯ï¼Œå¿«é€Ÿä¸”é«˜æ•ˆ\n",
    "\n",
    "ç¼ºç‚¹ï¼šçµæ´»æ€§æœ‰é™ï¼Œéœ€è¦é¢„å®šä¹‰è§„åˆ™ï¼Œéš¾ä»¥å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œå¯èƒ½éœ€è¦é¢‘ç¹æ›´æ–°è§„åˆ™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de94d0-c3dd-4d4b-b6a7-a5bfde7f54a5",
   "metadata": {},
   "source": [
    "## Part 2: è¯­ä¹‰è·¯ç”± - Semantic Routing\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šè¯­ä¹‰è·¯ç”±ä½¿ç”¨åµŒå…¥å‘é‡æ¥è·¯ç”±æŸ¥è¯¢ã€‚å®ƒä¸ºæ¯ä¸ªè·¯ç”±åˆ›å»ºæè¿°æ€§æ–‡æœ¬çš„åµŒå…¥ï¼Œç„¶åå°†æŸ¥è¯¢åµŒå…¥ä¸è·¯ç”±åµŒå…¥è¿›è¡Œç›¸ä¼¼åº¦æ¯”è¾ƒã€‚\n",
    "\n",
    "å·¥ä½œåŸç†ï¼š\n",
    "```python\n",
    "ç”¨æˆ·æŸ¥è¯¢\n",
    "    â†“\n",
    "è®¡ç®—æŸ¥è¯¢åµŒå…¥\n",
    "    â†“\n",
    "è®¡ç®—ä¸å„è·¯ç”±æè¿°çš„ç›¸ä¼¼åº¦\n",
    "    â†“\n",
    "é€‰æ‹©æœ€ç›¸ä¼¼çš„è·¯ç”±\n",
    "    â†“\n",
    "æ‰§è¡Œæ£€ç´¢\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e63838-befb-47f7-bad0-50b8ba6e5d5c",
   "metadata": {},
   "source": [
    "### å®ç°è¯­ä¹‰è·¯ç”±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff68c27-6c4f-4a79-b48d-2e747aba1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "class SemanticRouter:\n",
    "    \"\"\"è¯­ä¹‰è·¯ç”±å™¨ï¼ˆé€‚é…æ‚¨çš„æœ¬åœ°é…ç½®ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, persist_directory=\"./chroma_db\"):\n",
    "        self.embeddings = embeddings\n",
    "        self.persist_directory = persist_directory\n",
    "        self.routes = {}\n",
    "        self.route_embeddings = {}\n",
    "        \n",
    "        # åˆå§‹åŒ–æ‚¨çš„ä¸¤ä¸ªé›†åˆ\n",
    "        self._init_collections()\n",
    "    \n",
    "    def _init_collections(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ‚¨çš„ä¸¤ä¸ªå‘é‡æ•°æ®åº“é›†åˆ\"\"\"\n",
    "        # 1. langchain é›†åˆï¼ˆè®ºæ–‡ï¼‰\n",
    "        self.langchain_collection = Chroma(\n",
    "            collection_name=\"langchain\",\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # 2. web_content é›†åˆï¼ˆç½‘é¡µå†…å®¹ï¼‰\n",
    "        self.web_content_collection = Chroma(\n",
    "            collection_name=\"web_content\",\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "    \n",
    "    def add_route(self, name: str, description: str, retriever):\n",
    "        \"\"\"æ·»åŠ è·¯ç”±\n",
    "        \n",
    "        Args:\n",
    "            name: è·¯ç”±åç§°\n",
    "            description: è·¯ç”±æè¿°ï¼ˆå°†è¢«åµŒå…¥ï¼‰\n",
    "            retriever: æ£€ç´¢å™¨\n",
    "        \"\"\"\n",
    "        self.routes[name] = {\n",
    "            'description': description,\n",
    "            'retriever': retriever\n",
    "        }\n",
    "        \n",
    "        # è®¡ç®—æè¿°çš„åµŒå…¥ï¼ˆä½¿ç”¨æ‚¨çš„æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼‰\n",
    "        self.route_embeddings[name] = self.embeddings.embed_query(description)\n",
    "        print(f\"âœ… æ·»åŠ è·¯ç”±: {name} - {description}\")\n",
    "    \n",
    "    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "        vec1 = np.array(vec1)\n",
    "        vec2 = np.array(vec2)\n",
    "        \n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def route(self, question: str, threshold: float = 0.3) -> Tuple[str, float]:\n",
    "        \"\"\"æ‰§è¡Œè¯­ä¹‰è·¯ç”±\n",
    "        \n",
    "        Args:\n",
    "            question: ç”¨æˆ·æŸ¥è¯¢\n",
    "            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆæœ¬åœ°æ¨¡å‹é˜ˆå€¼è¾ƒä½ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "            (è·¯ç”±åç§°, ç›¸ä¼¼åº¦åˆ†æ•°)\n",
    "        \"\"\"\n",
    "        # è®¡ç®—æŸ¥è¯¢åµŒå…¥\n",
    "        query_embedding = self.embeddings.embed_query(question)\n",
    "        \n",
    "        # è®¡ç®—ä¸æ‰€æœ‰è·¯ç”±çš„ç›¸ä¼¼åº¦\n",
    "        similarities = {}\n",
    "        for name, route_embedding in self.route_embeddings.items():\n",
    "            similarity = self._cosine_similarity(query_embedding, route_embedding)\n",
    "            similarities[name] = similarity\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è·¯ç”±\n",
    "        best_route = max(similarities, key=similarities.get)\n",
    "        best_score = similarities[best_route]\n",
    "        \n",
    "        print(f\"ğŸ” è·¯ç”±åˆ†æ:\")\n",
    "        for route, score in sorted(similarities.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"   {route}: {score:.3f}\")\n",
    "        \n",
    "        # æ£€æŸ¥é˜ˆå€¼ï¼ˆæœ¬åœ°æ¨¡å‹é˜ˆå€¼è¾ƒä½ï¼‰\n",
    "        if best_score < threshold:\n",
    "            print(f\"âš ï¸ æœ€ä½³è·¯ç”±åˆ†æ•° {best_score:.3f} ä½äºé˜ˆå€¼ {threshold}ï¼Œä½†ç»§ç»­ä½¿ç”¨\")\n",
    "        \n",
    "        return best_route, best_score\n",
    "    \n",
    "    def route_with_scores(self, question: str) -> Dict[str, float]:\n",
    "        \"\"\"è¿”å›æ‰€æœ‰è·¯ç”±åŠå…¶åˆ†æ•°\"\"\"\n",
    "        query_embedding = self.embeddings.embed_query(question)\n",
    "        \n",
    "        similarities = {}\n",
    "        for name, route_embedding in self.route_embeddings.items():\n",
    "            similarity = self._cosine_similarity(query_embedding, route_embedding)\n",
    "            similarities[name] = similarity\n",
    "        \n",
    "        # æŒ‰åˆ†æ•°æ’åº\n",
    "        return dict(sorted(similarities.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    def query(self, question: str, k: int = 4):\n",
    "        \"\"\"æ‰§è¡Œå®Œæ•´æŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¯ æŸ¥è¯¢: {question}\")\n",
    "        \n",
    "        # è¯­ä¹‰è·¯ç”±\n",
    "        route_name, score = self.route(question)\n",
    "        print(f\"ğŸ“ è·¯ç”±åˆ°: {route_name} (ç›¸ä¼¼åº¦: {score:.3f})\")\n",
    "        \n",
    "        # æ£€ç´¢\n",
    "        retriever = self.routes[route_name]['retriever']\n",
    "        docs = retriever.get_relevant_documents(question, k=k)\n",
    "        \n",
    "        print(f\"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        return {\n",
    "            \"route\": route_name,\n",
    "            \"score\": score,\n",
    "            \"documents\": docs,\n",
    "            \"question\": question\n",
    "        }\n",
    "    \n",
    "    def query_multiple(self, question: str, k: int = 3):\n",
    "        \"\"\"æŸ¥è¯¢å¤šä¸ªæ•°æ®æºï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰\"\"\"\n",
    "        print(f\"ğŸ¯ å¤šæ•°æ®æºæŸ¥è¯¢: {question}\")\n",
    "        \n",
    "        # è·å–æ‰€æœ‰è·¯ç”±å¾—åˆ†\n",
    "        scores = self.route_with_scores(question)\n",
    "        \n",
    "        results = {}\n",
    "        for route_name, score in scores.items():\n",
    "            if score > 0.2:  # ç›¸ä¼¼åº¦é˜ˆå€¼\n",
    "                print(f\"ğŸ” æŸ¥è¯¢ {route_name} (ç›¸ä¼¼åº¦: {score:.3f})...\")\n",
    "                retriever = self.routes[route_name]['retriever']\n",
    "                docs = retriever.get_relevant_documents(question, k=k)\n",
    "                results[route_name] = {\n",
    "                    'documents': docs,\n",
    "                    'score': score,\n",
    "                    'count': len(docs)\n",
    "                }\n",
    "                print(f\"   ğŸ“Š æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    def get_route_info(self):\n",
    "        \"\"\"è·å–è·¯ç”±ä¿¡æ¯\"\"\"\n",
    "        info = {}\n",
    "        for name, route_data in self.routes.items():\n",
    "            info[name] = {\n",
    "                'description': route_data['description'],\n",
    "                'embedding_dim': len(self.route_embeddings[name]) if name in self.route_embeddings else 0\n",
    "            }\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c70b29a-8093-46ed-a393-fb5e053b4d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ·»åŠ è·¯ç”±: langchain - å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç ”ç©¶è®ºæ–‡ã€æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹ç›¸å…³æŠ€æœ¯\n",
      "âœ… æ·»åŠ è·¯ç”±: web_content - ç½‘é¡µå†…å®¹ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ã€ç¼–ç¨‹æ•™ç¨‹ã€CSDNåšå®¢ã€å®é™…æ“ä½œæ­¥éª¤\n",
      "ğŸ“Š è·¯ç”±ä¿¡æ¯:\n",
      "   langchain: å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç ”ç©¶è®ºæ–‡ã€æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹ç›¸å…³æŠ€æœ¯\n",
      "       åµŒå…¥ç»´åº¦: 768\n",
      "   web_content: ç½‘é¡µå†…å®¹ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ã€ç¼–ç¨‹æ•™ç¨‹ã€CSDNåšå®¢ã€å®é™…æ“ä½œæ­¥éª¤\n",
      "       åµŒå…¥ç»´åº¦: 768\n"
     ]
    }
   ],
   "source": [
    "#  åˆ›å»ºè¯­ä¹‰è·¯ç”±å™¨\n",
    "router = SemanticRouter(embeddings, persist_directory=\"./chroma_db\")\n",
    "\n",
    "#  æ·»åŠ ä¸¤ä¸ªè·¯ç”±\n",
    "router.add_route(\n",
    "    name=\"langchain\",\n",
    "    description=\"å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç ”ç©¶è®ºæ–‡ã€æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹ç›¸å…³æŠ€æœ¯\",\n",
    "    retriever=router.langchain_collection.as_retriever()\n",
    ")\n",
    "\n",
    "router.add_route(\n",
    "    name=\"web_content\", \n",
    "    description=\"ç½‘é¡µå†…å®¹ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ã€ç¼–ç¨‹æ•™ç¨‹ã€CSDNåšå®¢ã€å®é™…æ“ä½œæ­¥éª¤\",\n",
    "    retriever=router.web_content_collection.as_retriever()\n",
    ")\n",
    "\n",
    "#  æŸ¥çœ‹è·¯ç”±ä¿¡æ¯\n",
    "print(\"ğŸ“Š è·¯ç”±ä¿¡æ¯:\")\n",
    "info = router.get_route_info()\n",
    "for name, data in info.items():\n",
    "    print(f\"   {name}: {data['description']}\")\n",
    "    print(f\"       åµŒå…¥ç»´åº¦: {data['embedding_dim']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fe74cf5-64d5-49c5-9faa-c83af872cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ æŸ¥è¯¢: å¦‚ä½•å®ç°ä¸€ä¸ªæŠ“å–æ£€æµ‹ç®—æ³•ï¼Ÿ\n",
      "ğŸ” è·¯ç”±åˆ†æ:\n",
      "   langchain: 0.501\n",
      "   web_content: 0.270\n",
      "ğŸ“ è·¯ç”±åˆ°: langchain (ç›¸ä¼¼åº¦: 0.501)\n",
      "ğŸ“š æ£€ç´¢åˆ° 4 ä¸ªæ–‡æ¡£\n",
      "  1. æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n",
      "     å†…å®¹: è¥¿å—ç§‘æŠ€å¤§å­¦ç¡•å£«å­¦ä½è®ºæ–‡ \n",
      "32 \n",
      " \n",
      "å›¾ 4-2 æŠ“å–è¿‡ç¨‹  \n",
      "Fig.4-2 Grabbing Process  \n",
      "ï¼ˆ1ï¼‰æŠ“æ‰‹æ‰“å¼€é˜¶æ®µï¼šæŠ“æ‰‹åœ¨åˆå§‹ä½ç½® å°†æŠ“...\n",
      "  2. æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n",
      "     å†…å®¹: çš„é‡‡é›†ï¼Œ ä½¿ç”¨ Savitzky -Golay æ»¤æ³¢ç®—æ³• è¿›è¡Œæ•°æ®æ»¤æ³¢ ï¼Œå¹¶è¿›è¡Œäº†æµ‹è¯• ã€‚ç„¶åï¼Œç ”\n",
      "ç©¶äº† TSF ç‰©ä½“ç¡¬åº¦è¯†åˆ«ç®—æ³•å’Œ DWT æ»‘åŠ¨æ£€æµ‹ç®—æ³•ï¼Œæœ‰æ•ˆ...\n"
     ]
    }
   ],
   "source": [
    "result = router.query(\"å¦‚ä½•å®ç°ä¸€ä¸ªæŠ“å–æ£€æµ‹ç®—æ³•ï¼Ÿ\")\n",
    "        \n",
    "# æ˜¾ç¤ºæ–‡æ¡£é¢„è§ˆ\n",
    "for i, doc in enumerate(result['documents'][:2], 1):\n",
    "    source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')\n",
    "    print(f\"  {i}. æ¥æº: {source}\")\n",
    "    print(f\"     å†…å®¹: {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "baf91449-6cdc-4c8a-a0da-429b6409049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ æŸ¥è¯¢: å¦‚ä½•å†™ä¸€ä¸ªæŠ€æœ¯åšå®¢ï¼Ÿ\n",
      "ğŸ” è·¯ç”±åˆ†æ:\n",
      "   web_content: 0.487\n",
      "   langchain: 0.401\n",
      "ğŸ“ è·¯ç”±åˆ°: web_content (ç›¸ä¼¼åº¦: 0.487)\n",
      "ğŸ“š æ£€ç´¢åˆ° 4 ä¸ªæ–‡æ¡£\n",
      "  1. æ¥æº: https://blog.csdn.net/WhiffeYF/article/details/110829105\n",
      "     å†…å®¹: æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutiona...\n",
      "  2. æ¥æº: https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187\n",
      "     å†…å®¹: æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåš...\n"
     ]
    }
   ],
   "source": [
    "result = router.query(\"å¦‚ä½•å†™ä¸€ä¸ªæŠ€æœ¯åšå®¢ï¼Ÿ\")\n",
    "        \n",
    "# æ˜¾ç¤ºæ–‡æ¡£é¢„è§ˆ\n",
    "for i, doc in enumerate(result['documents'][:2], 1):\n",
    "    source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')\n",
    "    print(f\"  {i}. æ¥æº: {source}\")\n",
    "    clean_content = doc.page_content[:80].replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    print(f\"     å†…å®¹: {clean_content}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56436a96-7ade-4986-bbac-2d1efde56fb8",
   "metadata": {},
   "source": [
    "### å®ç°æ··åˆè·¯ç”±\n",
    "\n",
    "å·¥ä½œæµç¨‹ï¼š simpleé€‰æ‹©è¯­ä¹‰è·¯ç”±ï¼Œcomplexé€‰æ‹©é€»è¾‘è·¯ç”±\n",
    "```python\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"å¦‚ä½•å­¦ä¹ æŠ“å–æ£€æµ‹ç®—æ³•ï¼Ÿ\"\n",
    "    â†“\n",
    "è‡ªé€‚åº”è·¯ç”±åˆ†æ\n",
    "    â†“\n",
    "å¤æ‚åº¦åˆ¤æ–­: simple (ç®€å•æŸ¥è¯¢)\n",
    "    â†“\n",
    "é€‰æ‹©è¯­ä¹‰è·¯ç”±ç­–ç•¥\n",
    "    â†“\n",
    "è¯­ä¹‰è·¯ç”±æµç¨‹å¼€å§‹\n",
    "    â†“\n",
    "è®¡ç®—æŸ¥è¯¢åµŒå…¥\n",
    "    â†“\n",
    "è®¡ç®—ä¸å„è·¯ç”±æè¿°çš„ç›¸ä¼¼åº¦\n",
    "    â†“\n",
    "é€‰æ‹©æœ€ç›¸ä¼¼çš„è·¯ç”±: langchain (åˆ†æ•°: 0.45)\n",
    "    â†“\n",
    "æ‰§è¡Œæ£€ç´¢: ä»langchainé›†åˆæ£€ç´¢æ–‡æ¡£\n",
    "    â†“\n",
    "è¿”å›ç»“æœ: 4ä¸ªç›¸å…³æ–‡æ¡£\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8309ea8b-b90d-4daf-b530-920a53bfbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "import numpy as np\n",
    "\n",
    "class HybridRouter:\n",
    "    \"\"\"æ··åˆè·¯ç”±å™¨ï¼šç»“åˆé€»è¾‘å’Œè¯­ä¹‰è·¯ç”±ï¼ˆä½¿ç”¨ç°æœ‰å®ç°ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, llm, persist_directory=\"./chroma_db\"):\n",
    "        self.embeddings = embeddings\n",
    "        self.llm = llm\n",
    "        self.persist_directory = persist_directory\n",
    "        self.routes = {}\n",
    "        \n",
    "        # ä½¿ç”¨ä¹‹å‰å®ç°çš„è¯­ä¹‰è·¯ç”±å™¨å’Œé€»è¾‘è·¯ç”±å™¨\n",
    "        self.semantic_router = SemanticRouter(embeddings, persist_directory)\n",
    "        self.logical_router = LogicalRouter(llm, embeddings, persist_directory)\n",
    "    \n",
    "    def add_route(self, name: str, description: str, retriever):\n",
    "        \"\"\"æ·»åŠ è·¯ç”±\"\"\"\n",
    "        # æ·»åŠ åˆ°è¯­ä¹‰è·¯ç”±å™¨\n",
    "        self.semantic_router.add_route(name, description, retriever)\n",
    "        # æ·»åŠ åˆ°é€»è¾‘è·¯ç”±å™¨\n",
    "        self.logical_router.add_route(name, retriever)\n",
    "        # æ·»åŠ åˆ°æœ¬åœ°è·¯ç”±è¡¨\n",
    "        self.routes[name] = retriever\n",
    "        print(f\"âœ… æ·»åŠ æ··åˆè·¯ç”±: {name} - {description}\")\n",
    "    \n",
    "    def route(self, question: str, use_semantic: bool = True, semantic_threshold: float = 0.3):\n",
    "        \"\"\"æ‰§è¡Œæ··åˆè·¯ç”±\n",
    "        \n",
    "        Args:\n",
    "            question: ç”¨æˆ·æŸ¥è¯¢\n",
    "            use_semantic: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰è·¯ç”±\n",
    "            semantic_threshold: è¯­ä¹‰è·¯ç”±é˜ˆå€¼ï¼ˆæœ¬åœ°æ¨¡å‹é˜ˆå€¼è¾ƒä½ï¼‰\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ¯ æ··åˆè·¯ç”±æŸ¥è¯¢: {question}\")\n",
    "        \n",
    "        if use_semantic:\n",
    "            # å°è¯•è¯­ä¹‰è·¯ç”±\n",
    "            try:\n",
    "                route_name, score = self.semantic_router.route(question, threshold=semantic_threshold)\n",
    "                \n",
    "                if score >= semantic_threshold:\n",
    "                    print(f\"âœ… ä½¿ç”¨è¯­ä¹‰è·¯ç”±: {route_name} (åˆ†æ•°: {score:.3f})\")\n",
    "                    return route_name, self.routes[route_name]\n",
    "                else:\n",
    "                    print(f\"âš ï¸ è¯­ä¹‰è·¯ç”±åˆ†æ•°è¿‡ä½ ({score:.3f} < {semantic_threshold})ï¼Œåˆ‡æ¢åˆ°é€»è¾‘è·¯ç”±\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ è¯­ä¹‰è·¯ç”±å¤±è´¥: {e}ï¼Œåˆ‡æ¢åˆ°é€»è¾‘è·¯ç”±\")\n",
    "        \n",
    "        # ä½¿ç”¨é€»è¾‘è·¯ç”±ä½œä¸ºåå¤‡\n",
    "        try:\n",
    "            route_name, retriever = self.logical_router.route(question)\n",
    "            print(f\"âœ… ä½¿ç”¨é€»è¾‘è·¯ç”±: {route_name}\")\n",
    "            return route_name, retriever\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ é€»è¾‘è·¯ç”±å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤è·¯ç”±\")\n",
    "            # å›é€€åˆ°web_content\n",
    "            return \"web_content\", self.routes[\"web_content\"]\n",
    "    \n",
    "    def query(self, question: str, use_semantic: bool = True, k: int = 4):\n",
    "        \"\"\"æ‰§è¡ŒæŸ¥è¯¢\"\"\"\n",
    "        route_name, retriever = self.route(question, use_semantic)\n",
    "        docs = retriever.get_relevant_documents(question, k=k)\n",
    "        \n",
    "        print(f\"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        return {\n",
    "            \"route\": route_name,\n",
    "            \"documents\": docs,\n",
    "            \"question\": question\n",
    "        }\n",
    "    \n",
    "    def query_adaptive(self, question: str, k: int = 4):\n",
    "        \"\"\"è‡ªé€‚åº”æŸ¥è¯¢ï¼šæ ¹æ®æŸ¥è¯¢å¤æ‚åº¦é€‰æ‹©æœ€ä½³è·¯ç”±\"\"\"\n",
    "        print(f\"ğŸ¯ è‡ªé€‚åº”æŸ¥è¯¢: {question}\")\n",
    "        \n",
    "        # åˆ†ææŸ¥è¯¢å¤æ‚åº¦\n",
    "        complexity = self._analyze_complexity(question)\n",
    "        print(f\"ğŸ“Š æŸ¥è¯¢å¤æ‚åº¦: {complexity}\")\n",
    "        \n",
    "        if complexity == \"simple\":\n",
    "            # ç®€å•æŸ¥è¯¢ï¼šä½¿ç”¨è¯­ä¹‰è·¯ç”±\n",
    "            return self.query(question, use_semantic=True, k=k)\n",
    "        else:\n",
    "            # å¤æ‚æŸ¥è¯¢ï¼šä½¿ç”¨é€»è¾‘è·¯ç”±\n",
    "            return self.query(question, use_semantic=False, k=k)\n",
    "    \n",
    "    def _analyze_complexity(self, question: str) -> str:\n",
    "        \"\"\"åˆ†ææŸ¥è¯¢å¤æ‚åº¦\"\"\"\n",
    "        # ç®€å•è§„åˆ™ï¼šæ ¹æ®æŸ¥è¯¢é•¿åº¦å’Œå…³é”®è¯åˆ¤æ–­\n",
    "        if len(question) < 20 and any(keyword in question.lower() for keyword in [\"æ˜¯ä»€ä¹ˆ\", \"æ€ä¹ˆç”¨\", \"å¦‚ä½•\", \"æ•™ç¨‹\"]):\n",
    "            return \"simple\"\n",
    "        else:\n",
    "            return \"complex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6e04220-b66a-44a8-a4db-c8383c58b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ·»åŠ è·¯ç”±: langchain - å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç ”ç©¶è®ºæ–‡ã€æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹ç›¸å…³æŠ€æœ¯\n",
      "âœ… æ·»åŠ æ··åˆè·¯ç”±: langchain - å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç ”ç©¶è®ºæ–‡ã€æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹ç›¸å…³æŠ€æœ¯\n",
      "âœ… æ·»åŠ è·¯ç”±: web_content - ç½‘é¡µå†…å®¹ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ã€ç¼–ç¨‹æ•™ç¨‹ã€CSDNåšå®¢ã€å®é™…æ“ä½œæ­¥éª¤\n",
      "âœ… æ·»åŠ æ··åˆè·¯ç”±: web_content - ç½‘é¡µå†…å®¹ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ã€ç¼–ç¨‹æ•™ç¨‹ã€CSDNåšå®¢ã€å®é™…æ“ä½œæ­¥éª¤\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºæ··åˆè·¯ç”±å™¨\n",
    "hybrid_router = HybridRouter(embeddings, llm, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# æ·»åŠ ä¸¤ä¸ªè·¯ç”±\n",
    "hybrid_router.add_route(\n",
    "    name=\"langchain\",\n",
    "    description=\"å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç ”ç©¶è®ºæ–‡ã€æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹ç›¸å…³æŠ€æœ¯\",\n",
    "    retriever=hybrid_router.semantic_router.langchain_collection.as_retriever()\n",
    ")\n",
    "\n",
    "hybrid_router.add_route(\n",
    "    name=\"web_content\", \n",
    "    description=\"ç½‘é¡µå†…å®¹ã€æŠ€æœ¯æ•™ç¨‹ã€å®è·µæŒ‡å—ã€ç¼–ç¨‹æ•™ç¨‹ã€CSDNåšå®¢ã€å®é™…æ“ä½œæ­¥éª¤\",\n",
    "    retriever=hybrid_router.semantic_router.web_content_collection.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2e7b159-6e8b-4f38-bb7f-dd8f81d4ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ğŸ¯ è‡ªé€‚åº”è·¯ç”±:\n",
      "ğŸ¯ è‡ªé€‚åº”æŸ¥è¯¢: å¦‚ä½•å­¦ä¹ æŠ“å–æ£€æµ‹ç®—æ³•ï¼Ÿ\n",
      "ğŸ“Š æŸ¥è¯¢å¤æ‚åº¦: simple\n",
      "ğŸ¯ æ··åˆè·¯ç”±æŸ¥è¯¢: å¦‚ä½•å­¦ä¹ æŠ“å–æ£€æµ‹ç®—æ³•ï¼Ÿ\n",
      "ğŸ” è·¯ç”±åˆ†æ:\n",
      "   langchain: 0.497\n",
      "   web_content: 0.305\n",
      "âœ… ä½¿ç”¨è¯­ä¹‰è·¯ç”±: langchain (åˆ†æ•°: 0.497)\n",
      "ğŸ“š æ£€ç´¢åˆ° 4 ä¸ªæ–‡æ¡£\n",
      "   è·¯ç”±ç»“æœ: langchain\n",
      "   æ–‡æ¡£æ•°é‡: 4\n"
     ]
    }
   ],
   "source": [
    "query= \"å¦‚ä½•å­¦ä¹ æŠ“å–æ£€æµ‹ç®—æ³•ï¼Ÿ\"\n",
    "# æµ‹è¯•è‡ªé€‚åº”è·¯ç”±\n",
    "print(\"\\n ğŸ¯ è‡ªé€‚åº”è·¯ç”±:\")\n",
    "result3 = hybrid_router.query_adaptive(query)\n",
    "print(f\"   è·¯ç”±ç»“æœ: {result3['route']}\")\n",
    "print(f\"   æ–‡æ¡£æ•°é‡: {len(result3['documents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3877315-58e7-469e-8800-eb0b85fa6e16",
   "metadata": {},
   "source": [
    "è¯­ä¹‰è·¯ç”± vs é€»è¾‘è·¯ç”±\n",
    "\n",
    "| ç‰¹æ€§ | é€»è¾‘è·¯ç”± | è¯­ä¹‰è·¯ç”± |\n",
    "|------|---------|---------|\n",
    "| å†³ç­–ä¾æ® | è§„åˆ™/LLMåˆ†ç±» | åµŒå…¥ç›¸ä¼¼åº¦ |\n",
    "| çµæ´»æ€§ | ä¸­ç­‰ | é«˜ |\n",
    "| å‡†ç¡®æ€§ | é«˜(è§„åˆ™æ˜ç¡®æ—¶) | ä¸­é«˜ |\n",
    "| é€Ÿåº¦ | å¿« | å¾ˆå¿« âš¡ |\n",
    "| æˆæœ¬ | éœ€LLMè°ƒç”¨ | ä»…éœ€åµŒå…¥ |\n",
    "| å¯è§£é‡Šæ€§ | é«˜ | ä¸­ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984a34a-3233-40e6-a150-29b2796f798a",
   "metadata": {},
   "source": [
    "## Part3ï¼šæŸ¥è¯¢æ„å»º - Query Construction\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šæŸ¥è¯¢æ„å»ºæ˜¯å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–æŸ¥è¯¢çš„è¿‡ç¨‹ã€‚å®ƒå…è®¸æˆ‘ä»¬ç»“åˆè¯­ä¹‰æœç´¢å’Œç»“æ„åŒ–è¿‡æ»¤ã€‚\n",
    "\n",
    "ä¸ºä»€ä¹ˆéœ€è¦æŸ¥è¯¢æ„å»ºï¼Ÿ\n",
    "\n",
    "```python\n",
    "# åœºæ™¯: å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£æ£€ç´¢\n",
    "\n",
    "æ–‡æ¡£å…ƒæ•°æ®ç¤ºä¾‹:\n",
    "{\n",
    "    \"content\": \"æ·±åº¦å­¦ä¹ å…¥é—¨æ•™ç¨‹\",\n",
    "    \"metadata\": {\n",
    "        \"author\": \"å¼ ä¸‰\",\n",
    "        \"date\": \"2023-06-15\",\n",
    "        \"category\": \"æœºå™¨å­¦ä¹ \",\n",
    "        \"tags\": [\"æ·±åº¦å­¦ä¹ \", \"ç¥ç»ç½‘ç»œ\"],\n",
    "        \"views\": 1500\n",
    "    }\n",
    "}\n",
    "\n",
    "ç”¨æˆ·æŸ¥è¯¢:\n",
    "\"æ‰¾å‡ºå¼ ä¸‰åœ¨2023å¹´å†™çš„å…³äºæ·±åº¦å­¦ä¹ çš„æ–‡ç« \"\n",
    "\n",
    "éœ€è¦:\n",
    "1. è¯­ä¹‰æœç´¢: \"æ·±åº¦å­¦ä¹ \"\n",
    "2. ç»“æ„åŒ–è¿‡æ»¤:\n",
    "   - author == \"å¼ ä¸‰\"\n",
    "   - date >= \"2023-01-01\" AND date <= \"2023-12-31\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d124a03b-1eff-4723-9bf7-5805dd6000a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from vllm import SamplingParams\n",
    "import re\n",
    "\n",
    "class CustomSelfQueryRetriever:\n",
    "    \"\"\"è‡ªå®šä¹‰è‡ªæŸ¥è¯¢æ£€ç´¢å™¨ï¼ˆé€‚é…æ‚¨çš„vLLMé…ç½®ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, vectorstore, metadata_field_info, document_content_description):\n",
    "        self.llm = llm\n",
    "        self.vectorstore = vectorstore\n",
    "        self.metadata_field_info = metadata_field_info\n",
    "        self.document_content_description = document_content_description\n",
    "    \n",
    "    def _call_llm(self, prompt: str, max_tokens: int = 200) -> str:\n",
    "        \"\"\"è°ƒç”¨vLLMæ¨¡å‹\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        if outputs and outputs[0].outputs:\n",
    "            return outputs[0].outputs[0].text.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def _parse_filter_query(self, query: str) -> dict:\n",
    "        \"\"\"è§£ææŸ¥è¯¢å¹¶ç”Ÿæˆè¿‡æ»¤æ¡ä»¶\"\"\"\n",
    "        # æ„å»ºè§£ææç¤ºè¯\n",
    "        metadata_fields = \"\\n\".join([\n",
    "            f\"- {field.name}: {field.description} (ç±»å‹: {field.type})\" \n",
    "            for field in self.metadata_field_info\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢è§£æå™¨ï¼Œè´Ÿè´£å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–è¿‡æ»¤æ¡ä»¶ã€‚\n",
    "            \n",
    "            å¯ç”¨çš„å…ƒæ•°æ®å­—æ®µï¼š\n",
    "            {metadata_fields}\n",
    "            \n",
    "            æ–‡æ¡£å†…å®¹æè¿°ï¼š{self.document_content_description}\n",
    "            \n",
    "            è¯·å°†ç”¨æˆ·æŸ¥è¯¢è§£æä¸ºè¿‡æ»¤æ¡ä»¶ï¼Œæ ¼å¼ä¸ºï¼š\n",
    "            filter_type:field_name:value\n",
    "            \n",
    "            æ”¯æŒçš„è¿‡æ»¤æ“ä½œï¼š\n",
    "            - eq: ç­‰äº\n",
    "            - gt: å¤§äº\n",
    "            - lt: å°äº\n",
    "            - contains: åŒ…å«\n",
    "            - in: åœ¨åˆ—è¡¨ä¸­\n",
    "            \n",
    "            ç¤ºä¾‹ï¼š\n",
    "            æŸ¥è¯¢: \"æ‰¾å‡º2023å¹´å…³äºPythonçš„æ–‡ç« \"\n",
    "            è§£æ: eq:date:2023;contains:content:Python\n",
    "            \n",
    "            æŸ¥è¯¢: \"é˜…è¯»é‡è¶…è¿‡1000çš„æŠ€æœ¯æ–‡ç« \"\n",
    "            è§£æ: gt:views:1000;contains:content:æŠ€æœ¯\n",
    "            \n",
    "            åªè¿”å›è§£æåçš„è¿‡æ»¤æ¡ä»¶ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            æŸ¥è¯¢ï¼š{query}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            è§£æï¼š\"\"\"\n",
    "        \n",
    "        response = self._call_llm(prompt)\n",
    "        print(f\"ğŸ¤– LLMè§£æç»“æœ: {response}\")\n",
    "        return self._parse_filter_response(response)\n",
    "    \n",
    "    def _parse_filter_response(self, response: str) -> dict:\n",
    "        \"\"\"è§£æLLMè¿”å›çš„è¿‡æ»¤æ¡ä»¶\"\"\"\n",
    "        filters = {}\n",
    "        \n",
    "        # è§£ææ ¼å¼: eq:date:2023;contains:content:Python\n",
    "        filter_parts = response.split(';')\n",
    "        \n",
    "        for part in filter_parts:\n",
    "            part = part.strip()\n",
    "            if ':' in part:\n",
    "                try:\n",
    "                    op, field, value = part.split(':', 2)\n",
    "                    filters[field] = {op: value}\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        return filters\n",
    "    \n",
    "    def get_relevant_documents(self, query: str, k: int = 4):\n",
    "        \"\"\"è·å–ç›¸å…³æ–‡æ¡£ï¼ˆè‡ªå®šä¹‰å®ç°ï¼‰\"\"\"\n",
    "        print(f\"ğŸ¯ è‡ªæŸ¥è¯¢: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. è§£ææŸ¥è¯¢ç”Ÿæˆè¿‡æ»¤æ¡ä»¶\n",
    "            filters = self._parse_filter_query(query)\n",
    "            print(f\"ğŸ” è§£æçš„è¿‡æ»¤æ¡ä»¶: {filters}\")\n",
    "            \n",
    "            # 2. åº”ç”¨è¿‡æ»¤æ¡ä»¶æ£€ç´¢æ–‡æ¡£\n",
    "            if filters:\n",
    "                # æ„å»ºè¿‡æ»¤æŸ¥è¯¢\n",
    "                where_clauses = []\n",
    "                for field, condition in filters.items():\n",
    "                    for op, value in condition.items():\n",
    "                        if op == 'eq':\n",
    "                            where_clauses.append({field: {\"$eq\": value}})\n",
    "                        elif op == 'gt':\n",
    "                            where_clauses.append({field: {\"$gt\": int(value)}})\n",
    "                        elif op == 'lt':\n",
    "                            where_clauses.append({field: {\"$lt\": int(value)}})\n",
    "                        elif op == 'contains':\n",
    "                            # å¯¹äºå†…å®¹æœç´¢ï¼Œä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢\n",
    "                            search_query = value\n",
    "                \n",
    "                # å¦‚æœæœ‰å†…å®¹æœç´¢ï¼Œä¼˜å…ˆå¤„ç†\n",
    "                if 'content' in filters and 'contains' in filters['content']:\n",
    "                    search_query = filters['content']['contains']\n",
    "                    docs = self.vectorstore.similarity_search(search_query, k=k, filter=where_clauses)\n",
    "                else:\n",
    "                    # çº¯å…ƒæ•°æ®è¿‡æ»¤\n",
    "                    docs = self.vectorstore.similarity_search(query, k=k, filter=where_clauses)\n",
    "            else:\n",
    "                # æ— è¿‡æ»¤æ¡ä»¶ï¼Œç›´æ¥æœç´¢\n",
    "                docs = self.vectorstore.similarity_search(query, k=k)\n",
    "            \n",
    "            return docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è‡ªæŸ¥è¯¢å¤±è´¥: {e}ï¼Œä½¿ç”¨æ™®é€šæ£€ç´¢\")\n",
    "            # å›é€€åˆ°æ™®é€šæ£€ç´¢\n",
    "            return self.vectorstore.similarity_search(query, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f0727f0-e816-47d0-98bf-b66bd5adf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰å…ƒæ•°æ®å­—æ®µä¿¡æ¯\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"author\",\n",
    "        description=\"æ–‡æ¡£ä½œè€…\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"date\",\n",
    "        description=\"å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"æ–‡æ¡£ç±»åˆ«ï¼Œå¦‚: Python, æœºå™¨å­¦ä¹ , Webå¼€å‘ç­‰\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"views\",\n",
    "        description=\"æµè§ˆæ¬¡æ•°\",\n",
    "        type=\"integer\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"æ–‡æ¡£æ¥æº\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"æŠ€æœ¯æ–‡ç« ã€æ•™ç¨‹å’Œç ”ç©¶è®ºæ–‡\"\n",
    "\n",
    "#  åˆ›å»ºè‡ªå®šä¹‰è‡ªæŸ¥è¯¢æ£€ç´¢å™¨\n",
    "self_query_retriever = CustomSelfQueryRetriever(\n",
    "    llm=llm,\n",
    "    vectorstore=vectorstore,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    document_content_description=document_content_description\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6efb674-42f8-4198-a880-6f5f6c393900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æµ‹è¯•æŸ¥è¯¢: æ‰¾å‡ºç‚¹èµ10æ¬¡å…³äºæŠ“å–æ£€æµ‹çš„æ–‡ç« \n",
      "ğŸ¯ è‡ªæŸ¥è¯¢: æ‰¾å‡ºç‚¹èµ10æ¬¡å…³äºæŠ“å–æ£€æµ‹çš„æ–‡ç« \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLMè§£æç»“æœ: gt:views:10;contains:content:æŠ“å–æ£€æµ‹\n",
      "ğŸ” è§£æçš„è¿‡æ»¤æ¡ä»¶: {'views': {'gt': '10'}, 'content': {'contains': 'æŠ“å–æ£€æµ‹'}}\n",
      "âŒ è‡ªæŸ¥è¯¢å¤±è´¥: Expected where to be a dict, got [{'views': {'$gt': 10}}] in query.ï¼Œä½¿ç”¨æ™®é€šæ£€ç´¢\n",
      "âœ… æ‰¾åˆ° 3 ä¸ªæ–‡æ¡£\n",
      "   1. æ ‡é¢˜: æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks...\n",
      "      å…ƒæ•°æ®: {'source': 'https://blog.csdn.net/WhiffeYF/article/details/110829105', 'description': 'æ–‡ç« æµè§ˆé˜…è¯»6.2kæ¬¡ï¼Œç‚¹èµ10æ¬¡ï¼Œæ”¶è—97æ¬¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å®æ—¶æœºå™¨äººæŠ“å–æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå•é˜¶æ®µå›å½’é¢„æµ‹ï¼Œé¿å…äº†ä¼ ç»Ÿæ»‘åŠ¨çª—å£æ–¹æ³•çš„è®¡ç®—æˆæœ¬ã€‚æ¨¡å‹åœ¨åº·å¥ˆå°”æŠ“å–æ•°æ®é›†ä¸Šå®ç°äº†88%çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½åœ¨GPUä¸Šä»¥13å¸§/ç§’çš„é€Ÿåº¦è¿è¡Œã€‚ç›¸æ¯”äºç°æœ‰æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹æå‡äº†14ä¸ªç™¾åˆ†ç‚¹çš„ç²¾åº¦ï¼Œå¹¶ä¸”èƒ½åŒæ—¶è¿›è¡Œç‰©ä½“åˆ†ç±»å’ŒæŠ“å–é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å¤šæŠ“å–ç‰ˆæœ¬å¯ä»¥é¢„æµ‹å•ä¸ªç‰©ä½“çš„å¤šä¸ªæŠ“å–ç‚¹ï¼Œæ˜¾è‘—æé«˜äº†å¯¹å¤šæ ·æŠ“å–æ–¹å¼ç‰©ä½“çš„æ£€æµ‹æ€§èƒ½ã€‚', 'title': 'æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks-CSDNåšå®¢', 'language': 'zh-CN'}\n",
      "   2. æ ‡é¢˜: æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåšå®¢ æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘...\n",
      "      å…ƒæ•°æ®: {'description': 'æ–‡ç« æµè§ˆé˜…è¯»9kæ¬¡ï¼Œç‚¹èµ11æ¬¡ï¼Œæ”¶è—102æ¬¡ã€‚æœ¬æ–‡ç»¼è¿°äº†æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯ï¼Œé‡ç‚¹ä»‹ç»äº†åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæŠ“å–æ£€æµ‹çš„æŠ“å–å’ŒåŸºäºè§†è§‰è¿åŠ¨æ§åˆ¶ç­–ç•¥çš„ç«¯åˆ°ç«¯æŠ“å–ï¼Œå¹¶æ¢è®¨äº†å„ç§æ–¹æ³•çš„ä¼˜åŠ¿ä¸å±€é™ã€‚', 'source': 'https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187', 'title': 'æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåšå®¢', 'language': 'zh-CN'}\n",
      "   3. æ ‡é¢˜: æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0_dexnet-CSDNåšå®¢ æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0 æœ€æ–°æ¨èæ–‡ç« äº 2025-10-11 13:29:58 å‘å¸ƒ åŸ...\n",
      "      å…ƒæ•°æ®: {'language': 'zh-CN', 'title': 'æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0_dexnet-CSDNåšå®¢', 'description': 'æ–‡ç« æµè§ˆé˜…è¯»7.3kæ¬¡ï¼Œç‚¹èµ13æ¬¡ï¼Œæ”¶è—72æ¬¡ã€‚Dex-Net2.0æ˜¯ä¸€ç§å…ˆè¿›çš„æœºå™¨äººæŠ“å–ç®—æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæµç¨‹å®ç°é«˜æ•ˆæŠ“å–ï¼šé¦–å…ˆä»æ·±åº¦å›¾ä¸­é‡‡æ ·æŠ“å–å€™é€‰ï¼Œæ¥ç€è¯„ä¼°æŠ“å–è´¨é‡ï¼Œæœ€ç»ˆé€‰å‡ºæœ€ä½³æŠ“å–é…ç½®ã€‚è¯¥ç®—æ³•å¼•å…¥äº†åŒ…å«670ä¸‡ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡ŒæŠ“å–è´¨é‡è¯„ä¼°ã€‚', 'source': 'https://blog.csdn.net/qq_40081208/article/details/111053208?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111053208.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"æ‰¾å‡ºç‚¹èµ10æ¬¡å…³äºæŠ“å–æ£€æµ‹çš„æ–‡ç« \"\n",
    "print(f\"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}\")\n",
    "results = self_query_retriever.get_relevant_documents(query, k=3)\n",
    "print(f\"âœ… æ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    # æ¸…ç†å†…å®¹ï¼šå»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼\n",
    "    clean_content = doc.page_content.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    # é™åˆ¶é•¿åº¦å¹¶æ¸…ç†ç©ºæ ¼\n",
    "    preview = ' '.join(clean_content.split())[:80]\n",
    "    \n",
    "    print(f\"   {i}. æ ‡é¢˜: {preview}...\")\n",
    "    print(f\"      å…ƒæ•°æ®: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1bd3fb-dcda-4ae7-a11e-fd15e8081c14",
   "metadata": {},
   "source": [
    "## Part 4: è‡ªæŸ¥è¯¢æ£€ç´¢å™¨ - Self-Query Retriever\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ:è‡ªæŸ¥è¯¢æ£€ç´¢å™¨æ˜¯LangChainæä¾›çš„é«˜çº§å·¥å…·ï¼Œå®ƒèƒ½å¤Ÿè‡ªåŠ¨å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ†ç¦»ä¸º:\n",
    "1. è¯­ä¹‰æœç´¢éƒ¨åˆ†\n",
    "2. ç»“æ„åŒ–è¿‡æ»¤éƒ¨åˆ†\n",
    "\n",
    "```python\n",
    "ç”¨æˆ·è‡ªç„¶è¯­è¨€æŸ¥è¯¢\n",
    "    â†“\n",
    "LLMåˆ†ææŸ¥è¯¢\n",
    "    â†“\n",
    "åˆ†ç¦»ä¸ºä¸¤éƒ¨åˆ†:\n",
    "â”œâ”€ è¯­ä¹‰æŸ¥è¯¢å†…å®¹\n",
    "â””â”€ å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶\n",
    "    â†“\n",
    "æ‰§è¡Œæ··åˆæ£€ç´¢\n",
    "    â†“\n",
    "è¿”å›ç»“æœ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5efd906a-22ac-482f-ab61-d0c8b0a8455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import (\n",
    "    StructuredQueryOutputParser,\n",
    "    get_query_constructor_prompt,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from vllm import SamplingParams\n",
    "import re\n",
    "\n",
    "class SimpleSelfQueryRetriever:\n",
    "    \"\"\"ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, vectorstore, metadata_field_info, document_content_description):\n",
    "        self.llm = llm\n",
    "        self.vectorstore = vectorstore\n",
    "        self.metadata_field_info = metadata_field_info\n",
    "        self.document_content_description = document_content_description\n",
    "    \n",
    "    def _call_llm(self, prompt: str, max_tokens: int = 300) -> str:\n",
    "        \"\"\"è°ƒç”¨vLLMæ¨¡å‹\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        if outputs and outputs[0].outputs:\n",
    "            return outputs[0].outputs[0].text.strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def get_relevant_documents(self, query: str, k: int = 4):\n",
    "        \"\"\"ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¯ æŸ¥è¯¢: {query}\")\n",
    "        \n",
    "        # æ„å»ºè‡ªæŸ¥è¯¢æç¤ºè¯\n",
    "        metadata_info = \"\\n\".join([f\"- {field.name}: {field.description} (ç±»å‹: {field.type})\" \n",
    "                                 for field in self.metadata_field_info])\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶æå–ï¼š\n",
    "            \n",
    "            1. æœç´¢å…³é”®è¯ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰\n",
    "            2. è¿‡æ»¤æ¡ä»¶ï¼ˆåŸºäºå…ƒæ•°æ®ï¼‰\n",
    "            \n",
    "            å¯ç”¨çš„å…ƒæ•°æ®å­—æ®µï¼š\n",
    "            {metadata_info}\n",
    "            \n",
    "            æ–‡æ¡£ç±»å‹ï¼š{self.document_content_description}\n",
    "            \n",
    "            è¿”å›æ ¼å¼ï¼š\n",
    "            å…³é”®è¯ï¼š[æœç´¢å…³é”®è¯]\n",
    "            è¿‡æ»¤ï¼š[å­—æ®µå æ“ä½œç¬¦ å€¼] æˆ–å¤šä¸ªæ¡ä»¶ç”¨é€—å·åˆ†éš”\n",
    "            \n",
    "            ç¤ºä¾‹ï¼š\n",
    "            æŸ¥è¯¢ï¼š\"é˜…è¯»é‡è¶…è¿‡100çš„æŠ€æœ¯æ–‡ç« \"  \n",
    "            è¿”å›ï¼š\n",
    "            å…³é”®è¯ï¼šæŠ€æœ¯æ–‡ç« \n",
    "            è¿‡æ»¤ï¼šviews > 100\n",
    "\n",
    "            æŸ¥è¯¢ï¼š\"è¯­è¨€ä¸ºä¸­æ–‡çš„æŠ€æœ¯æ–‡ç« \"  \n",
    "            è¿”å›ï¼š\n",
    "            å…³é”®è¯ï¼šæŠ€æœ¯æ–‡ç« \n",
    "            è¿‡æ»¤ï¼šlanguage == zh-CN\n",
    "            \n",
    "            åªè¿”å›æ ¼å¼åŒ–çš„ç»“æœï¼Œä¸è¦è§£é‡Šã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            æŸ¥è¯¢ï¼š{query}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # è°ƒç”¨LLMè§£ææŸ¥è¯¢\n",
    "            response = self._call_llm(prompt, max_tokens=200)\n",
    "            print(f\"ğŸ¤– LLMè§£æç»“æœ: {response}\")\n",
    "            \n",
    "            # è§£æå“åº”\n",
    "            search_keyword, filter_conditions = self._parse_llm_response(response, query)\n",
    "            print(f\"ğŸ” æœç´¢å…³é”®è¯: {search_keyword}\")\n",
    "            print(f\"ğŸ“‹ è¿‡æ»¤æ¡ä»¶: {filter_conditions}\")\n",
    "            \n",
    "            # æ„å»ºè¿‡æ»¤æ¡ä»¶\n",
    "            filter_dict = self._build_filter_dict(filter_conditions)\n",
    "            \n",
    "            # æ‰§è¡Œæ£€ç´¢\n",
    "            if filter_dict:\n",
    "                docs = self.vectorstore.similarity_search(\n",
    "                    search_keyword, \n",
    "                    k=k, \n",
    "                    filter=filter_dict\n",
    "                )\n",
    "            else:\n",
    "                docs = self.vectorstore.similarity_search(search_keyword, k=k)\n",
    "            \n",
    "            print(f\"âœ… æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "            return docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è‡ªæŸ¥è¯¢å¤±è´¥: {e}ï¼Œä½¿ç”¨æ™®é€šæ£€ç´¢\")\n",
    "            return self.vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    def _parse_llm_response(self, response: str, original_query: str):\n",
    "        \"\"\"è§£æLLMå“åº”\"\"\"\n",
    "        # é»˜è®¤å€¼\n",
    "        search_keyword = original_query\n",
    "        filter_conditions = []\n",
    "        \n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('å…³é”®è¯ï¼š'):\n",
    "                search_keyword = line.replace('å…³é”®è¯ï¼š', '').strip()\n",
    "            elif line.startswith('è¿‡æ»¤ï¼š'):\n",
    "                filters = line.replace('è¿‡æ»¤ï¼š', '').strip()\n",
    "                if filters and filters != 'æ— ':\n",
    "                    filter_conditions = [f.strip() for f in filters.split(',')]\n",
    "        \n",
    "        return search_keyword, filter_conditions\n",
    "    \n",
    "    def _build_filter_dict(self, filter_conditions):\n",
    "        \"\"\"æ„å»ºè¿‡æ»¤å­—å…¸\"\"\"\n",
    "        if not filter_conditions:\n",
    "            return None\n",
    "        \n",
    "        filter_dict = {}\n",
    "        \n",
    "        for condition in filter_conditions:\n",
    "            parts = condition.split()\n",
    "            if len(parts) >= 3:\n",
    "                field, op, value = parts[0], parts[1], ' '.join(parts[2:])\n",
    "                \n",
    "                # å¤„ç†å€¼ç±»å‹\n",
    "                if value.isdigit():\n",
    "                    value = int(value)\n",
    "                \n",
    "                op_map = {\n",
    "                    '>': '$gt', '<': '$lt', '>=': '$gte', '<=': '$lte',\n",
    "                    '=': '$eq', 'contains': '$regex'\n",
    "                }\n",
    "                \n",
    "                if op in op_map:\n",
    "                    if op == 'contains':\n",
    "                        filter_dict[field] = {op_map[op]: f\".*{value}.*\"}\n",
    "                    else:\n",
    "                        filter_dict[field] = {op_map[op]: value}\n",
    "        \n",
    "        return filter_dict if filter_dict else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b66fce38-0751-4f0b-8258-427caa3f024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨æµ‹è¯•\n",
      "============================================================\n",
      "ğŸ“ åˆ›å»ºç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨...\n",
      "âœ… ç®€åŒ–ç‰ˆæ£€ç´¢å™¨åˆ›å»ºæˆåŠŸï¼\n",
      "\n",
      "ğŸ§ª å¼€å§‹æµ‹è¯• 3 ä¸ªæŸ¥è¯¢...\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ æµ‹è¯• 1/3: ç±»åˆ«è¿‡æ»¤\n",
      "ğŸ” æŸ¥è¯¢: åœ¨CSDNä¸Šå‘å¸ƒçš„åšå®¢\n",
      "ğŸ¯ æŸ¥è¯¢: åœ¨CSDNä¸Šå‘å¸ƒçš„åšå®¢\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLMè§£æç»“æœ: å…³é”®è¯ï¼šCSDN\n",
      "             è¿‡æ»¤ï¼šauthor == \"CSDN\"\n",
      "ğŸ” æœç´¢å…³é”®è¯: CSDN\n",
      "ğŸ“‹ è¿‡æ»¤æ¡ä»¶: []\n",
      "âœ… æ‰¾åˆ° 2 ä¸ªæ–‡æ¡£\n",
      "âœ… æ‰¾åˆ° 2 ä¸ªç›¸å…³æ–‡æ¡£\n",
      "   1. æ ‡é¢˜: æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0_dexnet-CSDNåšå®¢ æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0 æœ€æ–°æ¨èæ–‡ç« äº 2025-10-11 13:29:58 å‘å¸ƒ åŸ...\n",
      "      å…ƒæ•°æ®: {'title': 'æŠ“å–æ£€æµ‹ä¹‹Dex-Net 2.0_dexnet-CSDNåšå®¢', 'language': 'zh-CN', 'source': 'https://blog.csdn.net/qq_40081208/article/details/111053208?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111053208.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187', 'description': 'æ–‡ç« æµè§ˆé˜…è¯»7.3kæ¬¡ï¼Œç‚¹èµ13æ¬¡ï¼Œæ”¶è—72æ¬¡ã€‚Dex-Net2.0æ˜¯ä¸€ç§å…ˆè¿›çš„æœºå™¨äººæŠ“å–ç®—æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæµç¨‹å®ç°é«˜æ•ˆæŠ“å–ï¼šé¦–å…ˆä»æ·±åº¦å›¾ä¸­é‡‡æ ·æŠ“å–å€™é€‰ï¼Œæ¥ç€è¯„ä¼°æŠ“å–è´¨é‡ï¼Œæœ€ç»ˆé€‰å‡ºæœ€ä½³æŠ“å–é…ç½®ã€‚è¯¥ç®—æ³•å¼•å…¥äº†åŒ…å«670ä¸‡ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡ŒæŠ“å–è´¨é‡è¯„ä¼°ã€‚'}\n",
      "   2. æ ‡é¢˜: æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks...\n",
      "      å…ƒæ•°æ®: {'description': 'æ–‡ç« æµè§ˆé˜…è¯»6.2kæ¬¡ï¼Œç‚¹èµ10æ¬¡ï¼Œæ”¶è—97æ¬¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å®æ—¶æœºå™¨äººæŠ“å–æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå•é˜¶æ®µå›å½’é¢„æµ‹ï¼Œé¿å…äº†ä¼ ç»Ÿæ»‘åŠ¨çª—å£æ–¹æ³•çš„è®¡ç®—æˆæœ¬ã€‚æ¨¡å‹åœ¨åº·å¥ˆå°”æŠ“å–æ•°æ®é›†ä¸Šå®ç°äº†88%çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½åœ¨GPUä¸Šä»¥13å¸§/ç§’çš„é€Ÿåº¦è¿è¡Œã€‚ç›¸æ¯”äºç°æœ‰æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹æå‡äº†14ä¸ªç™¾åˆ†ç‚¹çš„ç²¾åº¦ï¼Œå¹¶ä¸”èƒ½åŒæ—¶è¿›è¡Œç‰©ä½“åˆ†ç±»å’ŒæŠ“å–é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å¤šæŠ“å–ç‰ˆæœ¬å¯ä»¥é¢„æµ‹å•ä¸ªç‰©ä½“çš„å¤šä¸ªæŠ“å–ç‚¹ï¼Œæ˜¾è‘—æé«˜äº†å¯¹å¤šæ ·æŠ“å–æ–¹å¼ç‰©ä½“çš„æ£€æµ‹æ€§èƒ½ã€‚', 'title': 'æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks-CSDNåšå®¢', 'language': 'zh-CN', 'source': 'https://blog.csdn.net/WhiffeYF/article/details/110829105'}\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ æµ‹è¯• 2/3: æ•°å€¼èŒƒå›´è¿‡æ»¤\n",
      "ğŸ” æŸ¥è¯¢: æ”¶è—è¶…è¿‡100çš„æŠ€æœ¯æ–‡ç« \n",
      "ğŸ¯ æŸ¥è¯¢: æ”¶è—è¶…è¿‡100çš„æŠ€æœ¯æ–‡ç« \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLMè§£æç»“æœ: å…³é”®è¯ï¼šæŠ€æœ¯æ–‡ç« \n",
      "             è¿‡æ»¤ï¼šviews > 100\n",
      "ğŸ” æœç´¢å…³é”®è¯: æŠ€æœ¯æ–‡ç« \n",
      "ğŸ“‹ è¿‡æ»¤æ¡ä»¶: []\n",
      "âœ… æ‰¾åˆ° 2 ä¸ªæ–‡æ¡£\n",
      "âœ… æ‰¾åˆ° 2 ä¸ªç›¸å…³æ–‡æ¡£\n",
      "   1. æ ‡é¢˜: æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks...\n",
      "      å…ƒæ•°æ®: {'language': 'zh-CN', 'title': 'æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks-CSDNåšå®¢', 'description': 'æ–‡ç« æµè§ˆé˜…è¯»6.2kæ¬¡ï¼Œç‚¹èµ10æ¬¡ï¼Œæ”¶è—97æ¬¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å®æ—¶æœºå™¨äººæŠ“å–æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå•é˜¶æ®µå›å½’é¢„æµ‹ï¼Œé¿å…äº†ä¼ ç»Ÿæ»‘åŠ¨çª—å£æ–¹æ³•çš„è®¡ç®—æˆæœ¬ã€‚æ¨¡å‹åœ¨åº·å¥ˆå°”æŠ“å–æ•°æ®é›†ä¸Šå®ç°äº†88%çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½åœ¨GPUä¸Šä»¥13å¸§/ç§’çš„é€Ÿåº¦è¿è¡Œã€‚ç›¸æ¯”äºç°æœ‰æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹æå‡äº†14ä¸ªç™¾åˆ†ç‚¹çš„ç²¾åº¦ï¼Œå¹¶ä¸”èƒ½åŒæ—¶è¿›è¡Œç‰©ä½“åˆ†ç±»å’ŒæŠ“å–é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å¤šæŠ“å–ç‰ˆæœ¬å¯ä»¥é¢„æµ‹å•ä¸ªç‰©ä½“çš„å¤šä¸ªæŠ“å–ç‚¹ï¼Œæ˜¾è‘—æé«˜äº†å¯¹å¤šæ ·æŠ“å–æ–¹å¼ç‰©ä½“çš„æ£€æµ‹æ€§èƒ½ã€‚', 'source': 'https://blog.csdn.net/WhiffeYF/article/details/110829105'}\n",
      "   2. æ ‡é¢˜: æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåšå®¢ æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘...\n",
      "      å…ƒæ•°æ®: {'title': 'æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåšå®¢', 'description': 'æ–‡ç« æµè§ˆé˜…è¯»9kæ¬¡ï¼Œç‚¹èµ11æ¬¡ï¼Œæ”¶è—102æ¬¡ã€‚æœ¬æ–‡ç»¼è¿°äº†æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯ï¼Œé‡ç‚¹ä»‹ç»äº†åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæŠ“å–æ£€æµ‹çš„æŠ“å–å’ŒåŸºäºè§†è§‰è¿åŠ¨æ§åˆ¶ç­–ç•¥çš„ç«¯åˆ°ç«¯æŠ“å–ï¼Œå¹¶æ¢è®¨äº†å„ç§æ–¹æ³•çš„ä¼˜åŠ¿ä¸å±€é™ã€‚', 'language': 'zh-CN', 'source': 'https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187'}\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ æµ‹è¯• 3/3: æ•°å€¼èŒƒå›´è¿‡æ»¤\n",
      "ğŸ” æŸ¥è¯¢: è¯­è¨€ä¸ºä¸­æ–‡çš„æŠ€æœ¯æ–‡ç« \n",
      "ğŸ¯ æŸ¥è¯¢: è¯­è¨€ä¸ºä¸­æ–‡çš„æŠ€æœ¯æ–‡ç« \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLMè§£æç»“æœ: å…³é”®è¯ï¼šæŠ€æœ¯æ–‡ç« \n",
      "             è¿‡æ»¤ï¼šlanguage == zh-CN\n",
      "ğŸ” æœç´¢å…³é”®è¯: æŠ€æœ¯æ–‡ç« \n",
      "ğŸ“‹ è¿‡æ»¤æ¡ä»¶: []\n",
      "âœ… æ‰¾åˆ° 2 ä¸ªæ–‡æ¡£\n",
      "âœ… æ‰¾åˆ° 2 ä¸ªç›¸å…³æ–‡æ¡£\n",
      "   1. æ ‡é¢˜: æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks...\n",
      "      å…ƒæ•°æ®: {'description': 'æ–‡ç« æµè§ˆé˜…è¯»6.2kæ¬¡ï¼Œç‚¹èµ10æ¬¡ï¼Œæ”¶è—97æ¬¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å®æ—¶æœºå™¨äººæŠ“å–æ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå•é˜¶æ®µå›å½’é¢„æµ‹ï¼Œé¿å…äº†ä¼ ç»Ÿæ»‘åŠ¨çª—å£æ–¹æ³•çš„è®¡ç®—æˆæœ¬ã€‚æ¨¡å‹åœ¨åº·å¥ˆå°”æŠ“å–æ•°æ®é›†ä¸Šå®ç°äº†88%çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½åœ¨GPUä¸Šä»¥13å¸§/ç§’çš„é€Ÿåº¦è¿è¡Œã€‚ç›¸æ¯”äºç°æœ‰æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹æå‡äº†14ä¸ªç™¾åˆ†ç‚¹çš„ç²¾åº¦ï¼Œå¹¶ä¸”èƒ½åŒæ—¶è¿›è¡Œç‰©ä½“åˆ†ç±»å’ŒæŠ“å–é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å¤šæŠ“å–ç‰ˆæœ¬å¯ä»¥é¢„æµ‹å•ä¸ªç‰©ä½“çš„å¤šä¸ªæŠ“å–ç‚¹ï¼Œæ˜¾è‘—æé«˜äº†å¯¹å¤šæ ·æŠ“å–æ–¹å¼ç‰©ä½“çš„æ£€æµ‹æ€§èƒ½ã€‚', 'title': 'æœºæ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆäºŒï¼‰ã€å®æ—¶æŠ“å–ç‚¹æ£€æµ‹ã€‘Real-Time Grasp Detection Using Convolutional Neural Networks-CSDNåšå®¢', 'language': 'zh-CN', 'source': 'https://blog.csdn.net/WhiffeYF/article/details/110829105'}\n",
      "   2. æ ‡é¢˜: æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåšå®¢ æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘...\n",
      "      å…ƒæ•°æ®: {'title': 'æ¢°è‡‚è®ºæ–‡ç¬”è®°ï¼ˆä¸‰ï¼‰ã€æŠ“å–æ£€æµ‹ã€‘æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯çš„ç ”ç©¶ç°çŠ¶ åˆ˜äºšæ¬£_åŸºäºæ·±åº¦å›¾åƒçš„æœºæ¢°è‡‚æŠ“å–ä½å§¿ä¼°è®¡ä¸è½¨è¿¹ä¼˜åŒ–ç ”ç©¶-CSDNåšå®¢', 'description': 'æ–‡ç« æµè§ˆé˜…è¯»9kæ¬¡ï¼Œç‚¹èµ11æ¬¡ï¼Œæ”¶è—102æ¬¡ã€‚æœ¬æ–‡ç»¼è¿°äº†æœºå™¨äººæŠ“å–æ£€æµ‹æŠ€æœ¯ï¼Œé‡ç‚¹ä»‹ç»äº†åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæŠ“å–æ£€æµ‹çš„æŠ“å–å’ŒåŸºäºè§†è§‰è¿åŠ¨æ§åˆ¶ç­–ç•¥çš„ç«¯åˆ°ç«¯æŠ“å–ï¼Œå¹¶æ¢è®¨äº†å„ç§æ–¹æ³•çš„ä¼˜åŠ¿ä¸å±€é™ã€‚', 'source': 'https://blog.csdn.net/WhiffeYF/article/details/111031270?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%8A%93%E5%8F%96%E6%A3%80%E6%B5%8B&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-7-111031270.142^v102^pc_search_result_base7&spm=1018.2226.3001.4187', 'language': 'zh-CN'}\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨æµ‹è¯•å®Œæˆï¼\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨æµ‹è¯•\n",
    "def demo_simple_self_query():\n",
    "    \"\"\"æ¼”ç¤ºç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨æµ‹è¯•\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 4. å®šä¹‰å…ƒæ•°æ®\n",
    "    metadata_field_info = [\n",
    "        AttributeInfo(\n",
    "            name=\"author\",\n",
    "            description=\"æ–‡æ¡£ä½œè€…\",\n",
    "            type=\"string\"\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"date\", \n",
    "            description=\"å‘å¸ƒæ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD\",\n",
    "            type=\"string\"\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"category\",\n",
    "            description=\"æ–‡æ¡£ç±»åˆ«ï¼Œå¦‚: Python, æœºå™¨å­¦ä¹ , Webå¼€å‘ç­‰\",\n",
    "            type=\"string\" \n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"views\",\n",
    "            description=\"æµè§ˆæ¬¡æ•°\",\n",
    "            type=\"integer\"\n",
    "        ),\n",
    "        AttributeInfo(\n",
    "            name=\"language\",\n",
    "            description=\"è¯­è¨€\",\n",
    "            type=\"string\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    document_content_description = \"æŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹\"\n",
    "    \n",
    "    # 5. ç›´æ¥åˆ›å»ºç®€åŒ–ç‰ˆæ£€ç´¢å™¨\n",
    "    print(\"ğŸ“ åˆ›å»ºç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨...\")\n",
    "    retriever = SimpleSelfQueryRetriever(\n",
    "        llm=llm,\n",
    "        vectorstore=vectorstore, \n",
    "        metadata_field_info=metadata_field_info,\n",
    "        document_content_description=document_content_description\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ç®€åŒ–ç‰ˆæ£€ç´¢å™¨åˆ›å»ºæˆåŠŸï¼\")\n",
    "    \n",
    "    # 6. æµ‹è¯•ä¸åŒç±»å‹æŸ¥è¯¢\n",
    "    test_queries = [\n",
    "        # æ—¶é—´è¿‡æ»¤æŸ¥è¯¢\n",
    "        {\n",
    "            \"query\": \"åœ¨CSDNä¸Šå‘å¸ƒçš„åšå®¢\",\n",
    "            \"description\": \"ç±»åˆ«è¿‡æ»¤\"\n",
    "        },\n",
    "        # æ•°å€¼è¿‡æ»¤æŸ¥è¯¢\n",
    "        {\n",
    "            \"query\": \"æ”¶è—è¶…è¿‡100çš„æŠ€æœ¯æ–‡ç« \",\n",
    "            \"description\": \"æ•°å€¼èŒƒå›´è¿‡æ»¤\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"è¯­è¨€ä¸ºä¸­æ–‡çš„æŠ€æœ¯æ–‡ç« \",\n",
    "            \"description\": \"æ•°å€¼èŒƒå›´è¿‡æ»¤\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ§ª å¼€å§‹æµ‹è¯• {len(test_queries)} ä¸ªæŸ¥è¯¢...\")\n",
    "    \n",
    "    for i, test_case in enumerate(test_queries, 1):\n",
    "        query = test_case[\"query\"]\n",
    "        description = test_case[\"description\"]\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"ğŸ“‹ æµ‹è¯• {i}/{len(test_queries)}: {description}\")\n",
    "        print(f\"ğŸ” æŸ¥è¯¢: {query}\")\n",
    "        \n",
    "        # æ‰§è¡ŒæŸ¥è¯¢\n",
    "        results = retriever.get_relevant_documents(query, k=2)\n",
    "        \n",
    "        # æ˜¾ç¤ºç»“æœ\n",
    "        if results:\n",
    "            print(f\"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "            for j, doc in enumerate(results, 1):\n",
    "                # æ¸…ç†è¾“å‡º\n",
    "                preview = ' '.join(doc.page_content.split())[:80]\n",
    "                print(f\"   {j}. æ ‡é¢˜: {preview}...\")\n",
    "                print(f\"      å…ƒæ•°æ®: {doc.metadata}\")\n",
    "        else:\n",
    "            print(\"âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£\")\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    # æµ‹è¯•ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨\n",
    "    retriever = demo_simple_self_query()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ ç®€åŒ–ç‰ˆè‡ªæŸ¥è¯¢æ£€ç´¢å™¨æµ‹è¯•å®Œæˆï¼\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8cea54-22a3-446d-ada4-ad3eb739a185",
   "metadata": {},
   "source": [
    "è‡ªæŸ¥è¯¢æ£€ç´¢å™¨çš„ä¼˜åŠ¿\n",
    "ä¼˜ç‚¹ âœ…:\n",
    "\n",
    "1. è‡ªåŠ¨åˆ†ç¦»è¯­ä¹‰å’Œç»“æ„åŒ–æŸ¥è¯¢\n",
    "2. æ— éœ€æ‰‹åŠ¨è§£ææŸ¥è¯¢æ„å›¾\n",
    "3. æ”¯æŒå¤æ‚çš„è¿‡æ»¤æ¡ä»¶\n",
    "4. æ˜“äºä½¿ç”¨å’Œé›†æˆ\n",
    "\n",
    "æ³¨æ„äº‹é¡¹ âš ï¸:\n",
    "\n",
    "1. éœ€è¦æ¸…æ™°çš„å…ƒæ•°æ®å­—æ®µå®šä¹‰\n",
    "2. LLMå¯èƒ½è¯¯è§£æŸ¥è¯¢æ„å›¾\n",
    "3. ä¾èµ–LLMæ€§èƒ½\n",
    "4. éœ€è¦è¶³å¤Ÿçš„tokené¢„ç®—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc628d9-50c4-4e0c-83d8-b513a3406e8d",
   "metadata": {},
   "source": [
    "# æ€§èƒ½ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49ae86-4f7e-460c-8441-3d86a0bf8b49",
   "metadata": {},
   "source": [
    "## 1.è·¯ç”±ç¼“å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ccb3292-3e42-4b0d-8de7-2af053ad3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class CachedRouter:\n",
    "    \"\"\"å¸¦ç¼“å­˜çš„è·¯ç”±å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, semantic_router, cache_size=100):\n",
    "        self.semantic_router = semantic_router\n",
    "        self.cache_size = cache_size\n",
    "        self._cache = {}\n",
    "    \n",
    "    def _get_cache_key(self, question: str) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¼“å­˜é”®\"\"\"\n",
    "        return hashlib.md5(question.encode()).hexdigest()\n",
    "    \n",
    "    @lru_cache(maxsize=100)\n",
    "    def route(self, question: str):\n",
    "        \"\"\"å¸¦ç¼“å­˜çš„è·¯ç”±\"\"\"\n",
    "        cache_key = self._get_cache_key(question)\n",
    "        \n",
    "        if cache_key in self._cache:\n",
    "            print(\"ğŸ’¾ ä½¿ç”¨ç¼“å­˜çš„è·¯ç”±ç»“æœ\")\n",
    "            return self._cache[cache_key]\n",
    "        \n",
    "        # æ‰§è¡Œè·¯ç”±\n",
    "        route_name, score = self.semantic_router.route(question)\n",
    "        \n",
    "        # å­˜å…¥ç¼“å­˜\n",
    "        self._cache[cache_key] = (route_name, score)\n",
    "        \n",
    "        return route_name, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db768fc1-4c39-411e-9ad6-cfd8244655a2",
   "metadata": {},
   "source": [
    "## 2. å¹¶è¡Œè·¯ç”±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdcd68-6507-44c0-9b1d-6bba7d1e82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List, Tuple\n",
    "\n",
    "class ParallelRouter:\n",
    "    \"\"\"å¹¶è¡Œè·¯ç”±å™¨\"\"\"\n",
    "    \n",
    "    async def route_multiple(\n",
    "        self,\n",
    "        questions: List[str]\n",
    "    ) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"å¹¶è¡Œè·¯ç”±å¤šä¸ªæŸ¥è¯¢\n",
    "        \n",
    "        Returns:\n",
    "            List[(question, route_name, score)]\n",
    "        \"\"\"\n",
    "        async def route_single(question: str):\n",
    "            # å¼‚æ­¥è·¯ç”±ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦å¼‚æ­¥åµŒå…¥ï¼‰\n",
    "            route_name, score = self.semantic_router.route(question)\n",
    "            return question, route_name, score\n",
    "        \n",
    "        tasks = [route_single(q) for q in questions]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# ä½¿ç”¨\n",
    "# results = asyncio.run(router.route_multiple([\"é—®é¢˜1\", \"é—®é¢˜2\", \"é—®é¢˜3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80be0b0-164a-4480-8e69-20d32a7f1b8f",
   "metadata": {},
   "source": [
    "# æ€»ç»“\n",
    "\n",
    "é€»è¾‘è·¯ç”±: é€‚åˆè§„åˆ™æ˜ç¡®çš„åœºæ™¯\n",
    "è¯­ä¹‰è·¯ç”±: é€‚åˆéœ€è¦çµæ´»æ€§çš„åœºæ™¯\n",
    "æŸ¥è¯¢æ„å»º: ç»“åˆè¯­ä¹‰å’Œç»“æ„åŒ–æœç´¢\n",
    "è‡ªæŸ¥è¯¢æ£€ç´¢å™¨: è‡ªåŠ¨åŒ–çš„æŸ¥è¯¢åˆ†ç¦»å·¥å…·\n",
    "\n",
    "æŠ€æœ¯é€‰æ‹©æŒ‡å—ï¼š\n",
    "\n",
    "```python\n",
    "é€‰æ‹©è·¯ç”±æ–¹å¼:\n",
    "â”œâ”€ è§„åˆ™æ˜ç¡®ä¸”å›ºå®š\n",
    "â”‚  â†’ é€»è¾‘è·¯ç”±\n",
    "â”œâ”€ éœ€è¦çµæ´»æ€§\n",
    "â”‚  â†’ è¯­ä¹‰è·¯ç”±\n",
    "â””â”€ æœ€ä½³æ•ˆæœ\n",
    "   â†’ æ··åˆè·¯ç”±\n",
    "\n",
    "é€‰æ‹©æŸ¥è¯¢æ–¹å¼:\n",
    "â”œâ”€ åªéœ€è¯­ä¹‰æœç´¢\n",
    "â”‚  â†’ æ ‡å‡†æ£€ç´¢å™¨\n",
    "â”œâ”€ éœ€è¦å…ƒæ•°æ®è¿‡æ»¤\n",
    "â”‚  â†’ æŸ¥è¯¢æ„å»º\n",
    "â””â”€ å¤æ‚æŸ¥è¯¢\n",
    "   â†’ è‡ªæŸ¥è¯¢æ£€ç´¢å™¨\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee08ca-6207-4aab-bc72-5a99e935f67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
