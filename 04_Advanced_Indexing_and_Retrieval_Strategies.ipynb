{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dc7cd6-9b44-45bc-89b9-332b39007298",
   "metadata": {},
   "source": [
    "# RAGé«˜çº§ç´¢å¼•ä¸æ£€ç´¢ç­–ç•¥ï¼šæå‡æ£€ç´¢è´¨é‡çš„å…³é”®\n",
    "\n",
    "åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†æŸ¥è¯¢ä¼˜åŒ–å’Œè·¯ç”±æŠ€æœ¯ã€‚ä½†æ˜¯ï¼Œæ£€ç´¢è´¨é‡ä¸ä»…å–å†³äºæŸ¥è¯¢ï¼Œè¿˜å–å†³äºå¦‚ä½•ç»„ç»‡å’Œç´¢å¼•æ–‡æ¡£ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨é«˜çº§ç´¢å¼•å’Œæ£€ç´¢ç­–ç•¥ã€‚\n",
    "\n",
    "## ä¸ºä»€ä¹ˆéœ€è¦é«˜çº§ç´¢å¼•ï¼Ÿ\n",
    "\n",
    "åŸºç¡€ç´¢å¼•çš„å±€é™æ€§\n",
    "\n",
    "```python\n",
    "# åŸºç¡€ç´¢å¼•æ–¹æ³•çš„é—®é¢˜\n",
    "\n",
    "é—®é¢˜1: æ–‡æ¡£è¿‡é•¿\n",
    "â†’ ä¸€ä¸ª10000å­—çš„æ–‡æ¡£è¢«æ•´ä½“åµŒå…¥\n",
    "â†’ è¯­ä¹‰ä¿¡æ¯è¿‡äºç²—ç³™\n",
    "â†’ æ£€ç´¢ä¸ç²¾ç¡®\n",
    "\n",
    "é—®é¢˜2: ä¸Šä¸‹æ–‡ä¸¢å¤±\n",
    "â†’ å°†æ–‡æ¡£åˆ†æˆå°å—\n",
    "â†’ æ¯å—ç‹¬ç«‹æ£€ç´¢\n",
    "â†’ ä¸¢å¤±äº†å—ä¸å—ä¹‹é—´çš„å…³ç³»\n",
    "â†’ æ— æ³•ç†è§£å®Œæ•´ä¸Šä¸‹æ–‡\n",
    "\n",
    "é—®é¢˜3: å¤šè¯­ä¹‰å†…å®¹\n",
    "â†’ ä¸€ä¸ªæ–‡æ¡£åŒ…å«å¤šä¸ªä¸»é¢˜\n",
    "â†’ å•ä¸€å‘é‡æ— æ³•è¡¨ç¤ºæ‰€æœ‰è¯­ä¹‰\n",
    "â†’ ç›¸å…³å†…å®¹å¯èƒ½è¢«é—æ¼\n",
    "\n",
    "é—®é¢˜4: æ£€ç´¢å†—ä½™\n",
    "â†’ æ£€ç´¢åˆ°å¤§é‡æ–‡æ¡£\n",
    "â†’ å¾ˆå¤šå†…å®¹é‡å¤æˆ–ä¸ç›¸å…³\n",
    "â†’ å½±å“æœ€ç»ˆç­”æ¡ˆè´¨é‡\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2c14b-752a-45ae-b4c7-215eba5b6efe",
   "metadata": {},
   "source": [
    "## æœ¬ç« æŠ€æœ¯æ¦‚è§ˆ\n",
    "\n",
    "| æŠ€æœ¯ | æ ¸å¿ƒç›®æ ‡ | é€‚ç”¨åœºæ™¯ | å¤æ‚åº¦ |\n",
    "|------|----------|----------|--------|\n",
    "| æ™ºèƒ½åˆ†å— | ä¼˜åŒ–å—å¤§å°å’Œè¾¹ç•Œ | æ‰€æœ‰RAGç³»ç»Ÿ | â­ |\n",
    "| å¤šå‘é‡ç´¢å¼• | ä¸€ä¸ªæ–‡æ¡£å¤šä¸ªå‘é‡ | å¤šä¸»é¢˜æ–‡æ¡£ | â­â­â­ |\n",
    "| çˆ¶æ–‡æ¡£æ£€ç´¢ | æ£€ç´¢å°å—è¿”å›å¤§å— | ä¿æŒä¸Šä¸‹æ–‡ | â­â­ |\n",
    "| ä¸Šä¸‹æ–‡å‹ç¼© | å‹ç¼©æ£€ç´¢ç»“æœ | å‡å°‘å†—ä½™ | â­â­â­ |\n",
    "| æ—¶é—´è¡°å‡æ£€ç´¢ | è€ƒè™‘æ–‡æ¡£æ–°é²œåº¦ | æ—¶æ•ˆæ€§å†…å®¹ | â­â­ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079dc0e9-13c0-4920-bc13-aba30a827969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/shared-nvme/conda-envs/py310/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 20:09:39 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 11-11 20:09:39 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', speculative_config=None, tokenizer='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8)\n",
      "WARNING 11-11 20:09:39 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-11 20:09:39 utils.py:660] Found nccl from library /usr/lib/x86_64-linux-gnu/libnccl.so.2\n",
      "INFO 11-11 20:09:39 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 11-11 20:09:39 selector.py:32] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 20:09:41,615 - modelscope - INFO - PyTorch version 2.3.0+cu118 Found.\n",
      "2025-11-11 20:09:41,617 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-11-11 20:09:41,668 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 298ceecce207285dd10b135af16e71cc and a total number of 964 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 20:09:47 model_runner.py:175] Loading model weights took 8.4983 GB\n",
      "INFO 11-11 20:09:49 gpu_executor.py:114] # GPU blocks: 1404, # CPU blocks: 512\n",
      "INFO 11-11 20:09:53 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-11 20:09:53 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-11 20:10:02 model_runner.py:1017] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹å‡†å¤‡å·¥ä½œ\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 1. åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹\n",
    "local_model_path = \"./Models/maidalun/bce-embedding-base_v1\" \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=local_model_path,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 2. åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“\n",
    "vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# 3. åŠ è½½æœ¬åœ°å¤§æ¨¡å‹\n",
    "model_dir=\"../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8\"\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "llm = LLM(\n",
    "    model=model_dir,\n",
    "    tokenizer=model_dir,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f312fc-d169-4510-9e38-89625d29af5b",
   "metadata": {},
   "source": [
    "## Part 1: æ™ºèƒ½æ–‡æ¡£åˆ†å—ç­–ç•¥\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šæ–‡æ¡£åˆ†å—ï¼ˆChunkingï¼‰æ˜¯å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆæ›´å°ç‰‡æ®µçš„è¿‡ç¨‹ã€‚å¥½çš„åˆ†å—ç­–ç•¥èƒ½æ˜¾è‘—æå‡æ£€ç´¢è´¨é‡ã€‚\n",
    "\n",
    "åˆ†å—æ–¹æ³•å¯¹æ¯”ï¼š\n",
    "```python\n",
    "# æ–¹æ³•1: å›ºå®šé•¿åº¦åˆ†å—\n",
    "def fixed_length_split(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"ç®€å•ä½†å¯èƒ½åˆ‡æ–­è¯­ä¹‰\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# æ–¹æ³•2: å¥å­çº§åˆ†å—\n",
    "def sentence_split(text: str, max_sentences: int = 5) -> List[str]:\n",
    "    \"\"\"ä¿æŒè¯­ä¹‰å®Œæ•´ä½†é•¿åº¦ä¸å‡\"\"\"\n",
    "    sentences = text.split('ã€‚')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        current_chunk.append(sent)\n",
    "        if len(current_chunk) >= max_sentences:\n",
    "            chunks.append('ã€‚'.join(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('ã€‚'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# æ–¹æ³•3: è¯­ä¹‰åˆ†å— âœ… æ¨è\n",
    "def semantic_split(text: str, embeddings) -> List[str]:\n",
    "    \"\"\"åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—\"\"\"\n",
    "    # å°†åœ¨ä¸‹é¢å®ç°\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ecbe0-abce-41f9-baf0-9c4ace72eab5",
   "metadata": {},
   "source": [
    "### æ™®é€šæ–‡æœ¬åˆ†å‰²å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b06364-9a2b-4801-a57b-e6c4cf393b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æ¡£è¢«åˆ†æˆ 1 å—\n",
      "\n",
      "å— 1:\n",
      "# Pythonç¼–ç¨‹åŸºç¡€\n",
      "\n",
      "## å˜é‡å’Œæ•°æ®ç±»å‹\n",
      "\n",
      "Pythonæ˜¯ä¸€ç§åŠ¨æ€ç±»å‹è¯­è¨€ï¼Œè¿™æ„å‘³ç€ä½ ä¸éœ€è¦å£°æ˜å˜é‡çš„ç±»å‹ã€‚\n",
      "Pythonæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬æ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ç­‰ã€‚\n",
      "\n",
      "## æ§åˆ¶æµ\n",
      "\n",
      "Py...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # æ¯å—çš„ç›®æ ‡å¤§å°\n",
    "    chunk_overlap=200,      # å—ä¹‹é—´çš„é‡å \n",
    "    length_function=len,    # é•¿åº¦è®¡ç®—å‡½æ•°\n",
    "    separators=[            # åˆ†éš”ç¬¦ä¼˜å…ˆçº§\n",
    "        \"\\n\\n\",             # æ®µè½\n",
    "        \"\\n\",               # è¡Œ\n",
    "        \" \",                # å•è¯\n",
    "        \"\"                  # å­—ç¬¦\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æ¡£\n",
    "document = \"\"\"\n",
    "# Pythonç¼–ç¨‹åŸºç¡€\n",
    "\n",
    "## å˜é‡å’Œæ•°æ®ç±»å‹\n",
    "\n",
    "Pythonæ˜¯ä¸€ç§åŠ¨æ€ç±»å‹è¯­è¨€ï¼Œè¿™æ„å‘³ç€ä½ ä¸éœ€è¦å£°æ˜å˜é‡çš„ç±»å‹ã€‚\n",
    "Pythonæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬æ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ç­‰ã€‚\n",
    "\n",
    "## æ§åˆ¶æµ\n",
    "\n",
    "Pythonä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ã€‚\n",
    "ifè¯­å¥ç”¨äºæ¡ä»¶åˆ¤æ–­ï¼Œforå¾ªç¯ç”¨äºè¿­ä»£ã€‚\n",
    "\n",
    "## å‡½æ•°\n",
    "\n",
    "å‡½æ•°æ˜¯å¯é‡ç”¨çš„ä»£ç å—ã€‚\n",
    "ä½¿ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# åˆ†å—\n",
    "chunks = text_splitter.split_text(document)\n",
    "\n",
    "print(f\"æ–‡æ¡£è¢«åˆ†æˆ {len(chunks)} å—\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nå— {i+1}:\")\n",
    "    print(chunk[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b7de0-ad85-4b3e-98d8-1a3c4dc5128c",
   "metadata": {},
   "source": [
    "### è¯­ä¹‰åˆ†å—\n",
    "\n",
    "\n",
    "å¤„ç†æµç¨‹ï¼š\n",
    "```python\n",
    "è¾“å…¥æ–‡æœ¬\n",
    "    â†“\n",
    "åˆ†å‰²å¥å­ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰\n",
    "    â†“\n",
    "è®¡ç®—æ¯ä¸ªå¥å­çš„åµŒå…¥\n",
    "    â†“\n",
    "è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦\n",
    "    â†“\n",
    "åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ‡åˆ†\n",
    "    â†“\n",
    "è¾“å‡ºè¯­ä¹‰åˆ†å—ç»“æœ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f7301a-d2f7-4f6e-a98c-c48afb2f844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "class SemanticChunker:\n",
    "    \"\"\"åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†å—å™¨ï¼ˆé€‚é…æœ¬åœ°æ¨¡å‹ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, similarity_threshold: float = 0.6):\n",
    "        self.embeddings = embeddings\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "    \n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
    "            return 0.0\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"åˆ†å‰²å¥å­ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰\"\"\"\n",
    "        sentence_endings = r'[ã€‚ï¼ï¼Ÿï¼›\\n]'\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        sentences = [s + 'ã€‚' for s in sentences if not s.endswith(('ã€‚', '!', '?', ';'))]\n",
    "        return sentences\n",
    "    \n",
    "    def _get_sentence_embeddings(self, sentences: List[str]) -> List[np.ndarray]:\n",
    "        \"\"\"è·å–å¥å­åµŒå…¥\"\"\"\n",
    "        embeddings = []\n",
    "        for sent in sentences:\n",
    "            emb = self.embeddings.embed_query(sent)\n",
    "            embeddings.append(np.array(emb))\n",
    "        return embeddings\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å—\"\"\"\n",
    "        if not text:\n",
    "            return [text]\n",
    "        \n",
    "        # 1. æŒ‰å¥å­åˆ†å‰²\n",
    "        sentences = self._split_sentences(text)\n",
    "        if len(sentences) <= 1:\n",
    "            return [text]\n",
    "        \n",
    "        # 2. è®¡ç®—æ¯ä¸ªå¥å­çš„åµŒå…¥\n",
    "        embeddings = self._get_sentence_embeddings(sentences)\n",
    "        \n",
    "        # 3. è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            sim = self.cosine_similarity(embeddings[i], embeddings[i+1])\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # 4. åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ‡åˆ†\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        \n",
    "        for i, sim in enumerate(similarities):\n",
    "            if sim < self.similarity_threshold:\n",
    "                chunks.append(''.join(current_chunk))\n",
    "                current_chunk = [sentences[i+1]]\n",
    "            else:\n",
    "                current_chunk.append(sentences[i+1])\n",
    "        \n",
    "        # æ·»åŠ æœ€åä¸€å—\n",
    "        if current_chunk:\n",
    "            chunks.append(''.join(current_chunk))\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d947e3eb-1fe0-483d-8348-2c594abac1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  åˆ›å»ºè¯­ä¹‰åˆ†å—å™¨\n",
    "semantic_chunker = SemanticChunker(embeddings, similarity_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc74488-1084-472e-a782-461335de9ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯­ä¹‰åˆ†å—ç»“æœ: 7 å—\n",
      "\n",
      "å— 1:\n",
      "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\n",
      "\n",
      "å— 2:\n",
      "å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚\n",
      "\n",
      "å— 3:\n",
      "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚\n",
      "\n",
      "å— 4:\n",
      "å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡å¤æ‚æ¨¡å¼ã€‚\n",
      "\n",
      "å— 5:\n",
      "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ã€‚\n",
      "\n",
      "å— 6:\n",
      "å®ƒå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ã€‚\n",
      "\n",
      "å— 7:\n",
      "Pythonæœ‰ä¸°å¯Œçš„åº“ç”Ÿæ€ç³»ç»Ÿã€‚NumPyå’ŒPandasæ˜¯å¸¸ç”¨çš„æ•°æ®å¤„ç†åº“ã€‚\n"
     ]
    }
   ],
   "source": [
    "#  æµ‹è¯•æ–‡æœ¬\n",
    "text = \"\"\"\n",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚\n",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡å¤æ‚æ¨¡å¼ã€‚\n",
    "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ã€‚å®ƒå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ã€‚\n",
    "Pythonæœ‰ä¸°å¯Œçš„åº“ç”Ÿæ€ç³»ç»Ÿã€‚NumPyå’ŒPandasæ˜¯å¸¸ç”¨çš„æ•°æ®å¤„ç†åº“ã€‚\n",
    "\"\"\"\n",
    "\n",
    "chunks = semantic_chunker.split_text(text)\n",
    "\n",
    "print(f\"è¯­ä¹‰åˆ†å—ç»“æœ: {len(chunks)} å—\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nå— {i+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece6f9d-bf9c-469c-ab51-91a014d0aefa",
   "metadata": {},
   "source": [
    "### åˆ†å—æœ€ä½³å®è·µ\n",
    "\n",
    "æµç¨‹ï¼š\n",
    "\n",
    "```python\n",
    "è¾“å…¥æ–‡æ¡£\n",
    "    â†“\n",
    "æŒ‰æ–‡æ¡£ç»“æ„åˆ†å‰²ï¼ˆæ ‡é¢˜ã€æ®µè½ï¼‰\n",
    "    â†“\n",
    "æ£€æŸ¥æ¯ä¸ªç« èŠ‚çš„å¤§å°\n",
    "    â†“\n",
    "å¯¹è¿‡é•¿çš„ç« èŠ‚è¿›è¡ŒäºŒæ¬¡åˆ†å‰²\n",
    "    â†“\n",
    "ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®\n",
    "    â†“\n",
    "è¾“å‡ºç»“æ„åŒ–åˆ†å—ç»“æœ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a974632-39ee-4b41-9d6d-4171509d55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "class SmartChunker:\n",
    "    \"\"\"æ™ºèƒ½åˆ†å—å™¨ - é€‚é…æœ¬åœ°æ¨¡å‹\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        min_chunk_size: int = 100,\n",
    "        max_chunk_size: int = 2000\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "    \n",
    "    def split_by_structure(self, text: str) -> List[str]:\n",
    "        \"\"\"æŒ‰æ–‡æ¡£ç»“æ„åˆ†å—\"\"\"\n",
    "        if not text or len(text) < self.min_chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        # æŒ‰æ ‡é¢˜åˆ†å‰²\n",
    "        sections = text.split('\\n# ')\n",
    "        chunks = []\n",
    "        \n",
    "        for section in sections:\n",
    "            if not section.strip():\n",
    "                continue\n",
    "            \n",
    "            # å¦‚æœç« èŠ‚å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²\n",
    "            if len(section) > self.max_chunk_size:\n",
    "                sub_chunks = self._split_long_section(section)\n",
    "                chunks.extend(sub_chunks)\n",
    "            elif len(section) >= self.min_chunk_size:\n",
    "                chunks.append(section)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_long_section(self, text: str) -> List[str]:\n",
    "        \"\"\"åˆ†å‰²è¿‡é•¿çš„ç« èŠ‚\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_length = len(para)\n",
    "            \n",
    "            if current_length + para_length > self.chunk_size and current_chunk:\n",
    "                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜\n",
    "                chunks.append('\\n\\n'.join(current_chunk))\n",
    "                \n",
    "                # å¤„ç†é‡å \n",
    "                if len(current_chunk) > 1:\n",
    "                    current_chunk = [current_chunk[-1], para]\n",
    "                    current_length = len(current_chunk[-1]) + para_length\n",
    "                else:\n",
    "                    current_chunk = [para]\n",
    "                    current_length = para_length\n",
    "            else:\n",
    "                current_chunk.append(para)\n",
    "                current_length += para_length\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append('\\n\\n'.join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def add_metadata_to_chunks(\n",
    "        self,\n",
    "        chunks: List[str],\n",
    "        doc_metadata: dict\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®\"\"\"\n",
    "        chunk_docs = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_docs.append({\n",
    "                'content': chunk,\n",
    "                'metadata': {\n",
    "                    **doc_metadata,\n",
    "                    'chunk_id': i,\n",
    "                    'chunk_total': len(chunks),\n",
    "                    'chunk_size': len(chunk)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return chunk_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5646d2b9-8a54-4801-add7-b36f2978c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_chunker = SmartChunker(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    min_chunk_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef2892d1-8f51-458c-b729-71c11989d335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å— 1/1\n",
      "å¤§å°: 166 å­—ç¬¦\n",
      "å†…å®¹: \n",
      "# æœºå™¨å­¦ä¹ åŸºç¡€\n",
      "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚\n",
      "\n",
      "# æ·±åº¦å­¦ä¹ \n",
      "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡å¤æ‚æ¨¡å¼ã€‚\n",
      "\n",
      "# Pythonç¼–ç¨‹\n",
      "Python...\n",
      "metadata: {'source': 'python_tutorial.md', 'author': 'å¼ ä¸‰', 'chunk_id': 0, 'chunk_total': 1, 'chunk_size': 166}\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹æ–‡æ¡£\n",
    "document = \"\"\"\n",
    "# æœºå™¨å­¦ä¹ åŸºç¡€\n",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚\n",
    "\n",
    "# æ·±åº¦å­¦ä¹ \n",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸã€‚å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡å¤æ‚æ¨¡å¼ã€‚\n",
    "\n",
    "# Pythonç¼–ç¨‹\n",
    "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ã€‚å®ƒå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ã€‚\n",
    "Pythonæœ‰ä¸°å¯Œçš„åº“ç”Ÿæ€ç³»ç»Ÿã€‚NumPyå’ŒPandasæ˜¯å¸¸ç”¨çš„æ•°æ®å¤„ç†åº“ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# æŒ‰ç»“æ„åˆ†å—\n",
    "chunks = smart_chunker.split_by_structure(document)\n",
    "\n",
    "# æ·»åŠ å…ƒæ•°æ®\n",
    "chunk_docs = smart_chunker.add_metadata_to_chunks(\n",
    "    chunks,\n",
    "    doc_metadata={'source': 'python_tutorial.md', 'author': 'å¼ ä¸‰'}\n",
    ")\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "for doc in chunk_docs:\n",
    "    print(f\"\\nå— {doc['metadata']['chunk_id'] + 1}/{doc['metadata']['chunk_total']}\")\n",
    "    print(f\"å¤§å°: {doc['metadata']['chunk_size']} å­—ç¬¦\")\n",
    "    print(f\"å†…å®¹: {doc['content'][:100]}...\")\n",
    "    print(f\"metadata: {doc['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439c012-5c31-4d5c-99ae-014b5eb202d9",
   "metadata": {},
   "source": [
    "## Part2: å¤šå‘é‡ç´¢å¼• - Multi-Vector Indexing\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šå¤šå‘é‡ç´¢å¼•ä¸ºå•ä¸ªæ–‡æ¡£ç”Ÿæˆå¤šä¸ªå‘é‡ï¼Œæ¯ä¸ªå‘é‡ä»£è¡¨æ–‡æ¡£çš„ä¸åŒæ–¹é¢æˆ–éƒ¨åˆ†ã€‚è¿™æ ·å¯ä»¥æ›´å…¨é¢åœ°è¡¨ç¤ºæ–‡æ¡£çš„è¯­ä¹‰ã€‚\n",
    "\n",
    "ä¸ºä»€ä¹ˆéœ€è¦å¤šå‘é‡ï¼Ÿ\n",
    "\n",
    "```python\n",
    "# åœºæ™¯ï¼šä¸€ç¯‡åŒ…å«å¤šä¸ªä¸»é¢˜çš„æ–‡æ¡£\n",
    "\n",
    "æ–‡æ¡£å†…å®¹:\n",
    "\"\"\"\n",
    "æœ¬æ–‡ä»‹ç»Pythonç¼–ç¨‹åŸºç¡€ã€‚\n",
    "\n",
    "ç¬¬ä¸€éƒ¨åˆ†ï¼šå˜é‡å’Œæ•°æ®ç±»å‹\n",
    "Pythonæ”¯æŒå¤šç§æ•°æ®ç±»å‹...\n",
    "\n",
    "ç¬¬äºŒéƒ¨åˆ†ï¼šå‡½æ•°å’Œæ¨¡å—\n",
    "å‡½æ•°æ˜¯å¯é‡ç”¨çš„ä»£ç å—...\n",
    "\n",
    "ç¬¬ä¸‰éƒ¨åˆ†ï¼šé¢å‘å¯¹è±¡ç¼–ç¨‹\n",
    "ç±»æ˜¯å¯¹è±¡çš„è“å›¾...\n",
    "\"\"\"\n",
    "\n",
    "# é—®é¢˜ï¼š\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"Pythonä¸­çš„ç±»æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "# å•å‘é‡ç´¢å¼•ï¼š\n",
    "â†’ æ•´ä¸ªæ–‡æ¡£è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå‘é‡\n",
    "â†’ å‘é‡æ··åˆäº†æ‰€æœ‰ä¸»é¢˜çš„è¯­ä¹‰\n",
    "â†’ å¯èƒ½æ— æ³•ç²¾ç¡®åŒ¹é…\"ç±»\"çš„ç›¸å…³å†…å®¹\n",
    "\n",
    "# å¤šå‘é‡ç´¢å¼•ï¼šâœ…\n",
    "â†’ ä¸ºæ¯ä¸ªéƒ¨åˆ†ç”Ÿæˆç‹¬ç«‹å‘é‡\n",
    "â†’ \"é¢å‘å¯¹è±¡ç¼–ç¨‹\"éƒ¨åˆ†çš„å‘é‡æ›´åŒ¹é…æŸ¥è¯¢\n",
    "â†’ æ£€ç´¢æ›´ç²¾ç¡®\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6a417-5253-4a0c-9906-b1867c081667",
   "metadata": {},
   "source": [
    "### å®ç°å¤šå‘é‡æ£€ç´¢å™¨\n",
    "\n",
    "æµç¨‹ï¼š\n",
    "\n",
    "```python\n",
    "è¾“å…¥æ–‡æ¡£\n",
    "    â†“\n",
    "ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆå”¯ä¸€ID\n",
    "    â†“\n",
    "å­˜å‚¨å®Œæ•´æ–‡æ¡£åˆ°æ–‡æ¡£å­˜å‚¨\n",
    "    â†“\n",
    "å°†æ–‡æ¡£åˆ†å‰²æˆå°å—\n",
    "    â†“\n",
    "ä¸ºæ¯ä¸ªå°å—æ·»åŠ æ–‡æ¡£IDå…ƒæ•°æ®\n",
    "    â†“\n",
    "å°†å°å—å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“\n",
    "    â†“\n",
    "æ„å»ºå¤šå‘é‡æ£€ç´¢å™¨\n",
    "    â†“\n",
    "å®Œæˆç´¢å¼•æ„å»º\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9772a76e-0101-4665-83cc-4e76b97c9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVectorIndexer:\n",
    "    \"\"\"å¤šå‘é‡ç´¢å¼•å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, persist_directory: str = \"./chroma_db\"):\n",
    "        self.embeddings = embeddings\n",
    "        self.persist_directory = persist_directory\n",
    "        \n",
    "        # å…ˆåˆ é™¤ç°æœ‰é›†åˆ\n",
    "        try:\n",
    "            existing_chroma = Chroma(\n",
    "                collection_name=\"multi_vector\",\n",
    "                persist_directory=persist_directory,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            existing_chroma.delete_collection()\n",
    "            print(\"ğŸ—‘ï¸ å·²åˆ é™¤ç°æœ‰é›†åˆ\")\n",
    "        except:\n",
    "            print(\"â„¹ï¸ æ— éœ€åˆ é™¤é›†åˆï¼Œç»§ç»­åˆå§‹åŒ–\")\n",
    "        \n",
    "        # é‡æ–°åˆ›å»ºå‘é‡å­˜å‚¨\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"multi_vector\",\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # æ–‡æ¡£å­˜å‚¨ï¼šå­˜å‚¨å®Œæ•´æ–‡æ¡£\n",
    "        self.docstore = InMemoryStore()\n",
    "        \n",
    "        # å¤šå‘é‡æ£€ç´¢å™¨ï¼ˆè¿”å›å®Œæ•´æ–‡æ¡£ï¼‰\n",
    "        self.retriever = MultiVectorRetriever(\n",
    "            vectorstore=self.vectorstore,\n",
    "            docstore=self.docstore,\n",
    "            id_key=\"doc_id\"\n",
    "        )\n",
    "    \n",
    "    def index_documents(self, documents: List[str], doc_ids: List[str] = None):\n",
    "        \"\"\"ç´¢å¼•æ–‡æ¡£\"\"\"\n",
    "        if doc_ids is None:\n",
    "            doc_ids = [str(uuid.uuid4()) for _ in documents]\n",
    "        \n",
    "        print(f\"ğŸ“ ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {len(doc_ids)} ä¸ªæ–‡æ¡£ID\")\n",
    "        \n",
    "        # 1. å­˜å‚¨å®Œæ•´æ–‡æ¡£\n",
    "        for doc_id, doc in zip(doc_ids, documents):\n",
    "            self.docstore.mset([(doc_id, doc)])\n",
    "            print(f\"âœ… å­˜å‚¨æ–‡æ¡£: {doc_id} (é•¿åº¦: {len(doc)} å­—ç¬¦)\")\n",
    "        \n",
    "        # 2. å°†æ¯ä¸ªæ–‡æ¡£åˆ†æˆå°å—\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=10\n",
    "        )\n",
    "        \n",
    "        sub_docs = []\n",
    "        for doc_id, doc in zip(doc_ids, documents):\n",
    "            chunks = text_splitter.split_text(doc)\n",
    "            print(f\"ğŸ“Š æ–‡æ¡£ {doc_id} åˆ†å‰²ä¸º {len(chunks)} ä¸ªå°å—\")\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                sub_docs.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\"doc_id\": doc_id}\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # 3. å°†å°å—å‘é‡åŒ–å¹¶å­˜å‚¨\n",
    "        if sub_docs:\n",
    "            self.vectorstore.add_documents(sub_docs)\n",
    "            self.vectorstore.persist()\n",
    "            print(f\"âœ… å­˜å‚¨ {len(sub_docs)} ä¸ªå°å—åˆ°å‘é‡æ•°æ®åº“\")\n",
    "        else:\n",
    "            print(\"âš ï¸ æ²¡æœ‰å°å—å¯å­˜å‚¨\")\n",
    "    \n",
    "    def retrieve_full_documents(self, query: str, k: int = 4):\n",
    "        \"\"\"æ£€ç´¢å®Œæ•´æ–‡æ¡£ï¼ˆè¿”å›å­—ç¬¦ä¸²ï¼‰\"\"\"\n",
    "        docs = self.retriever.get_relevant_documents(query, k=k)\n",
    "        return docs\n",
    "    \n",
    "    def retrieve_chunks(self, query: str, k: int = 4):\n",
    "        \"\"\"æ£€ç´¢å°å—ï¼ˆè¿”å›Documentå¯¹è±¡ï¼‰\"\"\"\n",
    "        docs = self.vectorstore.similarity_search(query, k=k)\n",
    "        return docs\n",
    "    \n",
    "    def debug_info(self):\n",
    "        \"\"\"è°ƒè¯•ä¿¡æ¯\"\"\"\n",
    "        print(\"ğŸ” è°ƒè¯•ä¿¡æ¯:\")\n",
    "        \n",
    "        try:\n",
    "            # æ£€æŸ¥å‘é‡æ•°æ®åº“\n",
    "            all_docs = self.vectorstore.get()\n",
    "            print(f\"ğŸ“Š å‘é‡æ•°æ®åº“: {len(all_docs['documents'])} ä¸ªå°å—\")\n",
    "            \n",
    "            # æ£€æŸ¥æ–‡æ¡£å­˜å‚¨\n",
    "            all_doc_ids = list(set([metadata['doc_id'] for metadata in all_docs['metadatas']]))\n",
    "            print(f\"ğŸ“š æ–‡æ¡£å­˜å‚¨: {len(all_doc_ids)} ä¸ªæ–‡æ¡£ID\")\n",
    "            \n",
    "            for doc_id in all_doc_ids:\n",
    "                doc = self.docstore.mget([doc_id])[0]\n",
    "                if doc:\n",
    "                    print(f\"âœ… æ–‡æ¡£ {doc_id}: å­˜åœ¨ ({len(doc)} å­—ç¬¦)\")\n",
    "                else:\n",
    "                    print(f\"âŒ æ–‡æ¡£ {doc_id}: ä¸å­˜åœ¨\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è°ƒè¯•ä¿¡æ¯è·å–å¤±è´¥: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c51feb22-753d-46e9-9f1d-98d2ecdaa142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸ å·²åˆ é™¤ç°æœ‰é›†åˆ\n"
     ]
    }
   ],
   "source": [
    "#  åˆ›å»ºå¤šå‘é‡ç´¢å¼•å™¨ï¼ŒæŒ‡å®šChromaæ•°æ®åº“è·¯å¾„\n",
    "multi_indexer = MultiVectorIndexer(\n",
    "    embeddings=embeddings,\n",
    "    persist_directory=\"./chroma_db\"  # æœ¬åœ°Chromaæ•°æ®åº“è·¯å¾„\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80913c0d-9267-4e7e-a69e-11a86e778bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç´¢å¼• 2 ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ 2 ä¸ªæ–‡æ¡£ID\n",
      "âœ… å­˜å‚¨æ–‡æ¡£: 36cf53c2-968a-4540-9f75-e2e0e02a9d04 (é•¿åº¦: 178 å­—ç¬¦)\n",
      "âœ… å­˜å‚¨æ–‡æ¡£: 8e2af8dd-e458-4f04-bc14-01d02552e8c9 (é•¿åº¦: 88 å­—ç¬¦)\n",
      "ğŸ“Š æ–‡æ¡£ 36cf53c2-968a-4540-9f75-e2e0e02a9d04 åˆ†å‰²ä¸º 2 ä¸ªå°å—\n",
      "ğŸ“Š æ–‡æ¡£ 8e2af8dd-e458-4f04-bc14-01d02552e8c9 åˆ†å‰²ä¸º 1 ä¸ªå°å—\n",
      "âœ… å­˜å‚¨ 3 ä¸ªå°å—åˆ°å‘é‡æ•°æ®åº“\n",
      "ğŸ” è°ƒè¯•ä¿¡æ¯:\n",
      "ğŸ“Š å‘é‡æ•°æ®åº“: 3 ä¸ªå°å—\n",
      "ğŸ“š æ–‡æ¡£å­˜å‚¨: 2 ä¸ªæ–‡æ¡£ID\n",
      "âœ… æ–‡æ¡£ 36cf53c2-968a-4540-9f75-e2e0e02a9d04: å­˜åœ¨ (178 å­—ç¬¦)\n",
      "âœ… æ–‡æ¡£ 8e2af8dd-e458-4f04-bc14-01d02552e8c9: å­˜åœ¨ (88 å­—ç¬¦)\n"
     ]
    }
   ],
   "source": [
    "#  å‡†å¤‡æ–‡æ¡£\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    Pythonç¼–ç¨‹åŸºç¡€æ•™ç¨‹\n",
    "    \n",
    "    ç¬¬ä¸€ç« ï¼šå˜é‡å’Œæ•°æ®ç±»å‹\n",
    "    Pythonæ˜¯åŠ¨æ€ç±»å‹è¯­è¨€ï¼Œæ”¯æŒæ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ç­‰å¤šç§æ•°æ®ç±»å‹ã€‚\n",
    "    \n",
    "    ç¬¬äºŒç« ï¼šæ§åˆ¶æµ\n",
    "    Pythonä½¿ç”¨ifã€forã€whileç­‰å…³é”®å­—è¿›è¡Œæµç¨‹æ§åˆ¶ã€‚\n",
    "    \n",
    "    ç¬¬ä¸‰ç« ï¼šå‡½æ•°\n",
    "    å‡½æ•°æ˜¯å¯é‡ç”¨çš„ä»£ç å—ï¼Œä½¿ç”¨defå…³é”®å­—å®šä¹‰ã€‚\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    æœºå™¨å­¦ä¹ å…¥é—¨\n",
    "    \n",
    "    ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚\n",
    "    æ— ç›‘ç£å­¦ä¹ ï¼šä»æ— æ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚\n",
    "    å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡å¥–åŠ±æœºåˆ¶å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "#  ç´¢å¼•æ–‡æ¡£\n",
    "multi_indexer.index_documents(documents)\n",
    "\n",
    "#  è°ƒè¯•ä¿¡æ¯\n",
    "multi_indexer.debug_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3f959c7-a3c5-4b6a-89cf-481776c13712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” æ£€ç´¢æµ‹è¯•:\n",
      "ç»“æœ 1: \n",
      "    æœºå™¨å­¦ä¹ å…¥é—¨\n",
      "    \n",
      "    ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚\n",
      "    æ— ç›‘ç£å­¦ä¹ ï¼šä»æ— æ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚\n",
      "    å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡å¥–åŠ±æœºåˆ¶å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚\n",
      "    ...\n",
      "ç»“æœ 2: \n",
      "    Pythonç¼–ç¨‹åŸºç¡€æ•™ç¨‹\n",
      "    \n",
      "    ç¬¬ä¸€ç« ï¼šå˜é‡å’Œæ•°æ®ç±»å‹\n",
      "    Pythonæ˜¯åŠ¨æ€ç±»å‹è¯­è¨€ï¼Œæ”¯æŒæ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ç­‰å¤šç§æ•°æ®ç±»å‹ã€‚\n",
      "    \n",
      "    ç¬¬äºŒç« ï¼šæ§åˆ¶æµ\n",
      "    P...\n"
     ]
    }
   ],
   "source": [
    "#  æ£€ç´¢æµ‹è¯•\n",
    "print(\"\\nğŸ” æ£€ç´¢æµ‹è¯•:\")\n",
    "results = multi_indexer.retrieve_full_documents(\"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\", k=2)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"ç»“æœ {i+1}: {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa7a8a17-45a1-4a59-af6c-c4c253556f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” æ£€ç´¢æµ‹è¯•:\n",
      "ç»“æœ 1: \n",
      "    Pythonç¼–ç¨‹åŸºç¡€æ•™ç¨‹\n",
      "    \n",
      "    ç¬¬ä¸€ç« ï¼šå˜é‡å’Œæ•°æ®ç±»å‹\n",
      "    Pythonæ˜¯åŠ¨æ€ç±»å‹è¯­è¨€ï¼Œæ”¯æŒæ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ç­‰å¤šç§æ•°æ®ç±»å‹ã€‚\n",
      "    \n",
      "    ç¬¬äºŒç« ï¼šæ§åˆ¶æµ\n",
      "    P...\n",
      "ç»“æœ 2: \n",
      "    æœºå™¨å­¦ä¹ å…¥é—¨\n",
      "    \n",
      "    ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚\n",
      "    æ— ç›‘ç£å­¦ä¹ ï¼šä»æ— æ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚\n",
      "    å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡å¥–åŠ±æœºåˆ¶å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "#  æ£€ç´¢æµ‹è¯•\n",
    "print(\"\\nğŸ” æ£€ç´¢æµ‹è¯•:\")\n",
    "results = multi_indexer.retrieve_full_documents(\"ä»€ä¹ˆæ˜¯pythonç¼–ç¨‹ï¼Ÿ\", k=2)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"ç»“æœ {i+1}: {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85cc1566-8297-4428-95fa-6d1f8179a293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” æ£€ç´¢æµ‹è¯•:\n",
      "[Document(page_content='Pythonç¼–ç¨‹åŸºç¡€æ•™ç¨‹\\n    \\n    ç¬¬ä¸€ç« ï¼šå˜é‡å’Œæ•°æ®ç±»å‹\\n    Pythonæ˜¯åŠ¨æ€ç±»å‹è¯­è¨€ï¼Œæ”¯æŒæ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ç­‰å¤šç§æ•°æ®ç±»å‹ã€‚\\n    \\n    ç¬¬äºŒç« ï¼šæ§åˆ¶æµ', metadata={'doc_id': '36cf53c2-968a-4540-9f75-e2e0e02a9d04'}), Document(page_content='Pythonä½¿ç”¨ifã€forã€whileç­‰å…³é”®å­—è¿›è¡Œæµç¨‹æ§åˆ¶ã€‚\\n    \\n    ç¬¬ä¸‰ç« ï¼šå‡½æ•°\\n    å‡½æ•°æ˜¯å¯é‡ç”¨çš„ä»£ç å—ï¼Œä½¿ç”¨defå…³é”®å­—å®šä¹‰ã€‚', metadata={'doc_id': '36cf53c2-968a-4540-9f75-e2e0e02a9d04'})]\n"
     ]
    }
   ],
   "source": [
    "#  æ£€ç´¢æµ‹è¯•\n",
    "print(\"\\nğŸ” æ£€ç´¢æµ‹è¯•:\")\n",
    "results = multi_indexer.retrieve_chunks(\"ä»€ä¹ˆæ˜¯pythonç¼–ç¨‹ï¼Ÿ\", k=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7e8c6-4e38-4725-abf3-53024d39d910",
   "metadata": {},
   "source": [
    "å¤šå‘é‡æ£€ç´¢ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "ç”¨æˆ·æŸ¥è¯¢ \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\"\n",
    "    â†“\n",
    "åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼çš„å°å—\n",
    "    â†“\n",
    "æ‰¾åˆ°ç›¸å…³å°å—ï¼š[å°å—2, å°å—5, å°å—8...]\n",
    "    â†“\n",
    "è·å–å°å—çš„ doc_idï¼š[doc_id_1, doc_id_2, doc_id_1...]\n",
    "    â†“\n",
    "æ ¹æ® doc_id ä»æ–‡æ¡£å­˜å‚¨ä¸­æ£€ç´¢å®Œæ•´æ–‡æ¡£\n",
    "    â†“\n",
    "è¿”å›å®Œæ•´æ–‡æ¡£ï¼š[å®Œæ•´æ–‡æ¡£1, å®Œæ•´æ–‡æ¡£2...]\n",
    "```\n",
    "\n",
    "å¤šå‘é‡æ£€ç´¢ä¼˜ç¼ºç‚¹ï¼š\n",
    "\n",
    "ä¼˜ç‚¹ï¼šæ›´ç²¾ç¡®çš„è¯­ä¹‰è¡¨ç¤ºï¼Œå¯ä»¥æ£€ç´¢åˆ°æ–‡æ¡£çš„ç‰¹å®šéƒ¨åˆ†ï¼Œæé«˜ç›¸å…³æ€§å¾—åˆ†ï¼Œçµæ´»çš„æ£€ç´¢ç­–ç•¥\n",
    "\n",
    "ç¼ºç‚¹ï¼šå­˜å‚¨æˆæœ¬å¢åŠ ï¼Œç´¢å¼•æ—¶é—´æ›´é•¿ï¼Œå®ç°å¤æ‚åº¦é«˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b84ce-878e-4d20-9a84-9232f3909bb6",
   "metadata": {},
   "source": [
    "## Part 3: çˆ¶æ–‡æ¡£æ£€ç´¢å™¨ - Parent Document Retriever\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ:\n",
    "\n",
    "çˆ¶æ–‡æ¡£æ£€ç´¢å™¨çš„ç­–ç•¥æ˜¯ï¼š\n",
    "\n",
    "1. å°†æ–‡æ¡£åˆ†æˆå°å—è¿›è¡Œç´¢å¼•å’Œæ£€ç´¢ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰\n",
    "2. è¿”å›å¤§å—æˆ–å®Œæ•´æ–‡æ¡£ç»™LLMï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰\n",
    "\n",
    "å·¥ä½œåŸç†ï¼š\n",
    "```python\n",
    "ç´¢å¼•é˜¶æ®µ:\n",
    "å¤§æ–‡æ¡£\n",
    "    â†“ åˆ†å—\n",
    "å°å—1ã€å°å—2ã€å°å—3\n",
    "    â†“ å‘é‡åŒ–\n",
    "å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“\n",
    "\n",
    "æ£€ç´¢é˜¶æ®µ:\n",
    "ç”¨æˆ·æŸ¥è¯¢\n",
    "    â†“ æ£€ç´¢\n",
    "åŒ¹é…å°å—2\n",
    "    â†“ æŸ¥æ‰¾çˆ¶æ–‡æ¡£\n",
    "è¿”å›ï¼šåŒ…å«å°å—2çš„å¤§å—/å®Œæ•´æ–‡æ¡£\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2f6a6f-eb79-44bf-b326-c8dbf20a6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "import shutil\n",
    "\n",
    "class ParentDocRetriever:\n",
    "    \"\"\"çˆ¶æ–‡æ¡£æ£€ç´¢å™¨ï¼ˆä¿®å¤å…ƒæ•°æ®é”®é—®é¢˜ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, persist_directory: str = \"./chroma_db\"):\n",
    "        self.embeddings = embeddings\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = \"parent_doc\"\n",
    "        # æ¸…ç†æŒ‡å®šé›†åˆ\n",
    "        self._clean_collection()\n",
    "        # å‘é‡å­˜å‚¨ï¼ˆå­˜å‚¨å­æ–‡æ¡£ï¼‰\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=self.collection_name,\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # æ–‡æ¡£å­˜å‚¨ï¼ˆå­˜å‚¨çˆ¶æ–‡æ¡£ï¼‰\n",
    "        self.docstore = InMemoryStore()\n",
    "        \n",
    "        # å­æ–‡æ¡£åˆ†å‰²å™¨ï¼ˆç”¨äºæ£€ç´¢çš„å°å—ï¼‰\n",
    "        self.child_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        \n",
    "        # çˆ¶æ–‡æ¡£åˆ†å‰²å™¨ï¼ˆç”¨äºè¿”å›çš„å¤§å—ï¼‰\n",
    "        self.parent_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        \n",
    "    def _clean_collection(self):\n",
    "        \"\"\"æ¸…ç†æŒ‡å®šé›†åˆ\"\"\"\n",
    "        try:\n",
    "            # å…ˆå°è¯•è¿æ¥åˆ°ç°æœ‰é›†åˆ\n",
    "            existing_vectorstore = Chroma(\n",
    "                collection_name=self.collection_name,\n",
    "                persist_directory=self.persist_directory,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            \n",
    "            # åˆ é™¤é›†åˆ\n",
    "            existing_vectorstore.delete_collection()\n",
    "            print(f\"ğŸ—‘ï¸ æ¸…ç†é›†åˆ: {self.collection_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # å¦‚æœé›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºç›®å½•\n",
    "            import os\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            print(f\"â„¹ï¸ é›†åˆä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°é›†åˆ: {self.collection_name}\")\n",
    "        \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£ï¼ˆä¿®å¤å…ƒæ•°æ®é”®é—®é¢˜ï¼‰\"\"\"\n",
    "        print(\"ğŸ“ å¼€å§‹ç´¢å¼•æ–‡æ¡£...\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            print(f\"\\nğŸ“„ å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}\")\n",
    "            \n",
    "            # 1. ç”¨çˆ¶æ–‡æ¡£åˆ†å‰²å™¨åˆ†å‰²æˆå¤§å—\n",
    "            parent_chunks = self.parent_splitter.split_documents([doc])\n",
    "            print(f\"   ğŸ”§ çˆ¶æ–‡æ¡£åˆ†å‰²: {len(parent_chunks)} ä¸ªå¤§å—\")\n",
    "            \n",
    "            for j, parent_chunk in enumerate(parent_chunks):\n",
    "                # ä¸ºæ¯ä¸ªçˆ¶æ–‡æ¡£ç”Ÿæˆå”¯ä¸€ID\n",
    "                parent_id = f\"parent_{i}_{j}\"\n",
    "                \n",
    "                # 2. ç”¨å­æ–‡æ¡£åˆ†å‰²å™¨å°†å¤§å—åˆ†å‰²æˆå°å—\n",
    "                child_chunks = self.child_splitter.split_documents([parent_chunk])\n",
    "                print(f\"   ğŸ” å­æ–‡æ¡£åˆ†å‰²: å¤§å— {j+1} â†’ {len(child_chunks)} ä¸ªå°å—\")\n",
    "                \n",
    "                # 3. å­˜å‚¨çˆ¶æ–‡æ¡£åˆ°æ–‡æ¡£å­˜å‚¨\n",
    "                self.docstore.mset([(parent_id, parent_chunk.page_content)])\n",
    "                \n",
    "                # 4. ä¸ºæ¯ä¸ªå­æ–‡æ¡£æ·»åŠ çˆ¶æ–‡æ¡£IDå…ƒæ•°æ®ï¼Œå¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“\n",
    "                child_docs_with_metadata = []\n",
    "                for k, child_chunk in enumerate(child_chunks):\n",
    "                    child_doc = Document(\n",
    "                        page_content=child_chunk.page_content,\n",
    "                        metadata={\"doc_id\": parent_id, \"source\": doc.metadata.get(\"source\", \"unknown\")}\n",
    "                    )\n",
    "                    child_docs_with_metadata.append(child_doc)\n",
    "                \n",
    "                self.vectorstore.add_documents(child_docs_with_metadata)\n",
    "        \n",
    "        # æŒä¹…åŒ–ä¿å­˜\n",
    "        self.vectorstore.persist()\n",
    "        print(f\"âœ… ç´¢å¼•å®Œæˆ: æ‰€æœ‰æ–‡æ¡£å·²å­˜å‚¨\")\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 2):\n",
    "        \"\"\"æ£€ç´¢çˆ¶æ–‡æ¡£ï¼ˆä¿®å¤å…ƒæ•°æ®é”®é—®é¢˜ï¼‰\"\"\"\n",
    "        print(f\"\\nğŸ” å¼€å§‹æ£€ç´¢: '{query}'\")\n",
    "        \n",
    "        # 1. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼å­æ–‡æ¡£\n",
    "        print(\"1. ğŸ” åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼å­æ–‡æ¡£...\")\n",
    "        child_docs = self.vectorstore.similarity_search(query, k=k*3)\n",
    "        print(f\"   æ‰¾åˆ° {len(child_docs)} ä¸ªç›¸å…³å­æ–‡æ¡£\")\n",
    "        \n",
    "        for i, child_doc in enumerate(child_docs):\n",
    "            # ä½¿ç”¨æ­£ç¡®çš„å…ƒæ•°æ®é”® 'doc_id' è€Œä¸æ˜¯ 'parent_id'\n",
    "            parent_id = child_doc.metadata.get(\"doc_id\")\n",
    "            print(f\"     å­æ–‡æ¡£ {i+1}: {child_doc.page_content[:50]}... (çˆ¶æ–‡æ¡£ID: {parent_id})\")\n",
    "        \n",
    "        # 2. æå–çˆ¶æ–‡æ¡£ID\n",
    "        print(\"\\n2. ğŸ†” æå–çˆ¶æ–‡æ¡£ID...\")\n",
    "        parent_ids = [doc.metadata.get(\"doc_id\") for doc in child_docs]\n",
    "        print(f\"   çˆ¶æ–‡æ¡£IDåˆ—è¡¨: {parent_ids}\")\n",
    "        \n",
    "        # 3. ä»æ–‡æ¡£å­˜å‚¨ä¸­æ£€ç´¢å®Œæ•´çˆ¶æ–‡æ¡£\n",
    "        print(\"\\n3. ğŸ“š ä»æ–‡æ¡£å­˜å‚¨ä¸­æ£€ç´¢å®Œæ•´çˆ¶æ–‡æ¡£...\")\n",
    "        parent_docs = self.docstore.mget(parent_ids)\n",
    "        print(f\"   æ£€ç´¢åˆ° {len([doc for doc in parent_docs if doc])} ä¸ªçˆ¶æ–‡æ¡£\")\n",
    "        \n",
    "        # 4. å»é‡å’Œæ’åº\n",
    "        print(\"\\n4. ğŸ”„ å»é‡å’Œæ’åº...\")\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        \n",
    "        for i, parent_doc in enumerate(parent_docs):\n",
    "            if parent_doc and parent_doc not in seen:\n",
    "                seen.add(parent_doc)\n",
    "                unique_docs.append(Document(\n",
    "                    page_content=parent_doc,\n",
    "                    metadata={\"doc_id\": parent_ids[i]}\n",
    "                ))\n",
    "                print(f\"   ä¿ç•™çˆ¶æ–‡æ¡£: {parent_ids[i]} (é•¿åº¦: {len(parent_doc)} å­—ç¬¦)\")\n",
    "        \n",
    "        # 5. è¿”å›å‰kä¸ªç»“æœ\n",
    "        final_results = unique_docs[:k]\n",
    "        print(f\"\\nâœ… æ£€ç´¢å®Œæˆ: è¿”å› {len(final_results)} ä¸ªçˆ¶æ–‡æ¡£\")\n",
    "        \n",
    "        return final_results\n",
    "    \n",
    "    def debug_storage(self):\n",
    "        \"\"\"è°ƒè¯•å­˜å‚¨çŠ¶æ€\"\"\"\n",
    "        print(\"\\nğŸ“Š å­˜å‚¨çŠ¶æ€è°ƒè¯•:\")\n",
    "        \n",
    "        # æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­çš„å­æ–‡æ¡£\n",
    "        all_child_docs = self.vectorstore.get()\n",
    "        print(f\"ğŸ” å‘é‡æ•°æ®åº“: {len(all_child_docs['documents'])} ä¸ªå­æ–‡æ¡£\")\n",
    "        \n",
    "        # æ£€æŸ¥å…ƒæ•°æ®\n",
    "        if all_child_docs['metadatas']:\n",
    "            first_metadata = all_child_docs['metadatas'][0]\n",
    "            print(f\"   ç¬¬ä¸€ä¸ªå­æ–‡æ¡£çš„å…ƒæ•°æ®: {first_metadata}\")\n",
    "            \n",
    "            # è·å–æ‰€æœ‰çˆ¶æ–‡æ¡£IDï¼ˆä½¿ç”¨æ­£ç¡®çš„é”® 'doc_id'ï¼‰\n",
    "            parent_ids = []\n",
    "            for metadata in all_child_docs['metadatas']:\n",
    "                if metadata and 'doc_id' in metadata:\n",
    "                    parent_ids.append(metadata['doc_id'])\n",
    "            \n",
    "            print(f\"ğŸ“š æ–‡æ¡£å­˜å‚¨: {len(set(parent_ids))} ä¸ªçˆ¶æ–‡æ¡£ID\")\n",
    "            print(f\"   çˆ¶æ–‡æ¡£IDåˆ—è¡¨: {list(set(parent_ids))}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å…ƒæ•°æ®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ef771c-3fc2-4dd6-9412-4f8e5d44299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸ æ¸…ç†é›†åˆ: parent_doc\n"
     ]
    }
   ],
   "source": [
    "# 2. åˆ›å»ºçˆ¶æ–‡æ¡£æ£€ç´¢å™¨\n",
    "parent_retriever = ParentDocRetriever(\n",
    "    embeddings=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34688a97-8a85-4f3e-b519-cce3c5464067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ å¼€å§‹ç´¢å¼•æ–‡æ¡£...\n",
      "\n",
      "ğŸ“„ å¤„ç†æ–‡æ¡£ 1/1\n",
      "   ğŸ”§ çˆ¶æ–‡æ¡£åˆ†å‰²: 1 ä¸ªå¤§å—\n",
      "   ğŸ” å­æ–‡æ¡£åˆ†å‰²: å¤§å— 1 â†’ 2 ä¸ªå°å—\n",
      "âœ… ç´¢å¼•å®Œæˆ: æ‰€æœ‰æ–‡æ¡£å·²å­˜å‚¨\n",
      "\n",
      "ğŸ“Š å­˜å‚¨çŠ¶æ€è°ƒè¯•:\n",
      "ğŸ” å‘é‡æ•°æ®åº“: 2 ä¸ªå­æ–‡æ¡£\n",
      "   ç¬¬ä¸€ä¸ªå­æ–‡æ¡£çš„å…ƒæ•°æ®: {'doc_id': 'parent_0_0', 'source': 'python_guide.md'}\n",
      "ğŸ“š æ–‡æ¡£å­˜å‚¨: 1 ä¸ªçˆ¶æ–‡æ¡£ID\n",
      "   çˆ¶æ–‡æ¡£IDåˆ—è¡¨: ['parent_0_0']\n"
     ]
    }
   ],
   "source": [
    "# 3. å‡†å¤‡æ–‡æ¡£\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Pythonç¼–ç¨‹è¯­è¨€å®Œæ•´æŒ‡å—\n",
    "        \n",
    "        ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€è¯­æ³•\n",
    "        Pythonä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ã€‚å˜é‡ä¸éœ€è¦å£°æ˜ç±»å‹ã€‚\n",
    "        æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬æ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€å­—å…¸ç­‰ã€‚\n",
    "        \n",
    "        ç¬¬äºŒéƒ¨åˆ†ï¼šæ§åˆ¶æµ\n",
    "        ifè¯­å¥ç”¨äºæ¡ä»¶åˆ¤æ–­ï¼š\n",
    "        if condition:\n",
    "            do_something()\n",
    "        \n",
    "        forå¾ªç¯ç”¨äºè¿­ä»£ï¼š\n",
    "        for item in items:\n",
    "            process(item)\n",
    "        \n",
    "        whileå¾ªç¯ç”¨äºé‡å¤æ‰§è¡Œï¼š\n",
    "        while condition:\n",
    "            do_something()\n",
    "        \n",
    "        ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‡½æ•°\n",
    "        ä½¿ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°ï¼š\n",
    "        def function_name(parameters):\n",
    "            # function body\n",
    "            return result\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"python_guide.md\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# 4. æ·»åŠ æ–‡æ¡£\n",
    "parent_retriever.add_documents(docs)\n",
    "\n",
    "# 5. è°ƒè¯•å­˜å‚¨çŠ¶æ€\n",
    "parent_retriever.debug_storage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9d45a8-8104-4e9d-ae48-ae46feb9e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ” å¼€å§‹æ£€ç´¢: 'Pythonä¸­forå¾ªç¯æ€ä¹ˆç”¨ï¼Ÿ'\n",
      "1. ğŸ” åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼å­æ–‡æ¡£...\n",
      "   æ‰¾åˆ° 2 ä¸ªç›¸å…³å­æ–‡æ¡£\n",
      "     å­æ–‡æ¡£ 1: Pythonç¼–ç¨‹è¯­è¨€å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€è¯­æ³•\n",
      "        ... (çˆ¶æ–‡æ¡£ID: parent_0_0)\n",
      "     å­æ–‡æ¡£ 2: ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‡½æ•°\n",
      "        ä½¿ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°ï¼š\n",
      "        def function... (çˆ¶æ–‡æ¡£ID: parent_0_0)\n",
      "\n",
      "2. ğŸ†” æå–çˆ¶æ–‡æ¡£ID...\n",
      "   çˆ¶æ–‡æ¡£IDåˆ—è¡¨: ['parent_0_0', 'parent_0_0']\n",
      "\n",
      "3. ğŸ“š ä»æ–‡æ¡£å­˜å‚¨ä¸­æ£€ç´¢å®Œæ•´çˆ¶æ–‡æ¡£...\n",
      "   æ£€ç´¢åˆ° 2 ä¸ªçˆ¶æ–‡æ¡£\n",
      "\n",
      "4. ğŸ”„ å»é‡å’Œæ’åº...\n",
      "   ä¿ç•™çˆ¶æ–‡æ¡£: parent_0_0 (é•¿åº¦: 515 å­—ç¬¦)\n",
      "\n",
      "âœ… æ£€ç´¢å®Œæˆ: è¿”å› 1 ä¸ªçˆ¶æ–‡æ¡£\n",
      "\n",
      "ğŸ¯ æœ€ç»ˆç»“æœ:\n",
      "æ–‡æ¡£ 1:\n",
      "å†…å®¹é¢„è§ˆ: Pythonç¼–ç¨‹è¯­è¨€å®Œæ•´æŒ‡å—\n",
      "        \n",
      "        ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€è¯­æ³•\n",
      "        Pythonä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ã€‚å˜é‡ä¸éœ€è¦å£°æ˜ç±»å‹ã€‚\n",
      "        æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬æ•´æ•°ã€...\n",
      "çˆ¶æ–‡æ¡£ID: parent_0_0\n"
     ]
    }
   ],
   "source": [
    "# 6. æ£€ç´¢æµ‹è¯•\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "results = parent_retriever.retrieve(\"Pythonä¸­forå¾ªç¯æ€ä¹ˆç”¨ï¼Ÿ\", k=1)\n",
    "\n",
    "# 7. æ˜¾ç¤ºç»“æœ\n",
    "print(\"\\nğŸ¯ æœ€ç»ˆç»“æœ:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"æ–‡æ¡£ {i+1}:\")\n",
    "    print(f\"å†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...\")\n",
    "    print(f\"çˆ¶æ–‡æ¡£ID: {doc.metadata['doc_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f1bb9-2248-42ea-8c46-f3ccc35b6d80",
   "metadata": {},
   "source": [
    "**çˆ¶æ–‡æ¡£æ£€ç´¢çš„ä¼˜åŠ¿**\n",
    "\n",
    "æœ€ä½³åœºæ™¯ï¼šéœ€è¦ç²¾ç¡®åŒ¹é… + å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œæ–‡æ¡£æœ‰æ˜ç¡®çš„å±‚æ¬¡ç»“æ„ï¼Œç­”æ¡ˆéœ€è¦å‘¨å›´çš„è§£é‡Šï¼Œé¿å…ä¸Šä¸‹æ–‡ä¸¢å¤±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9796c0-68dd-42d8-b72a-627eef8e62df",
   "metadata": {},
   "source": [
    "## Part 4: ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢ - Contextual Compression\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ:ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢åœ¨æ£€ç´¢åå¯¹æ–‡æ¡£è¿›è¡Œè¿‡æ»¤å’Œå‹ç¼©ï¼Œåªä¿ç•™ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å†…å®¹ã€‚\n",
    "\n",
    "ä¸ºä»€ä¹ˆéœ€è¦å‹ç¼©ï¼Ÿ\n",
    "```python\n",
    "# é—®é¢˜ï¼šæ£€ç´¢å†—ä½™\n",
    "\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"Pythonä¸­çš„åˆ—è¡¨æ¨å¯¼å¼æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "æ£€ç´¢åˆ°çš„æ–‡æ¡£:\n",
    "\"\"\"\n",
    "Pythoné«˜çº§ç‰¹æ€§å®Œæ•´æŒ‡å—\n",
    "\n",
    "1. åˆ—è¡¨æ¨å¯¼å¼\n",
    "åˆ—è¡¨æ¨å¯¼å¼æ˜¯åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼...\n",
    "\n",
    "2. ç”Ÿæˆå™¨è¡¨è¾¾å¼\n",
    "ç”Ÿæˆå™¨ç”¨äºæƒ°æ€§è®¡ç®—...\n",
    "\n",
    "3. è£…é¥°å™¨\n",
    "è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸º...\n",
    "\n",
    "4. ä¸Šä¸‹æ–‡ç®¡ç†å™¨\n",
    "withè¯­å¥ç”¨äºèµ„æºç®¡ç†...\n",
    "\"\"\"\n",
    "\n",
    "# é—®é¢˜ï¼š\n",
    "â†’ åªæœ‰\"åˆ—è¡¨æ¨å¯¼å¼\"éƒ¨åˆ†ç›¸å…³\n",
    "â†’ å…¶ä»–éƒ¨åˆ†æ˜¯å™ªå£°\n",
    "â†’ æµªè´¹LLMçš„ä¸Šä¸‹æ–‡çª—å£\n",
    "â†’ å¯èƒ½å½±å“ç­”æ¡ˆè´¨é‡\n",
    "\n",
    "# è§£å†³æ–¹æ¡ˆï¼šä¸Šä¸‹æ–‡å‹ç¼© âœ…\n",
    "â†’ åªæå–ç›¸å…³éƒ¨åˆ†ï¼š\"åˆ—è¡¨æ¨å¯¼å¼æ˜¯åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼...\"\n",
    "â†’ èŠ‚çœtokens\n",
    "â†’ æé«˜ç­”æ¡ˆç²¾åº¦\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473c29b-676b-47b8-adc3-6277bf18f938",
   "metadata": {},
   "source": [
    "### å®ç°LLMè¿‡æ»¤å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a01e7e5c-34b8-4f89-b626-8730375fc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from vllm import SamplingParams\n",
    "\n",
    "class VLLMCompressedRetriever:\n",
    "    \"\"\"vLLMå‹ç¼©æ£€ç´¢å™¨ï¼ˆChatMLæ ¼å¼ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.base_retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3):\n",
    "        \"\"\"æ£€ç´¢å¹¶å‹ç¼©\"\"\"\n",
    "        # åŸºç¡€æ£€ç´¢\n",
    "        base_docs = self.base_retriever.get_relevant_documents(query, k=k)\n",
    "        # print(f\"base_docs: {base_docs}\")\n",
    "        # å‹ç¼©æ–‡æ¡£\n",
    "        compressed_docs = []\n",
    "        for doc in base_docs:\n",
    "            # ä½¿ç”¨ChatMLæ ¼å¼\n",
    "            prompt = f\"\"\"<|im_start|>system\n",
    "                ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£å‹ç¼©åŠ©æ‰‹ã€‚è¯·ä»ç»™å®šçš„æ–‡æ¡£ä¸­æå–ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼Œå»é™¤æ— å…³å†…å®¹ã€‚\n",
    "                \n",
    "                è¯·åªè¿”å›æå–çš„å…³é”®ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯„è®ºã€‚<|im_end|>\n",
    "                <|im_start|>user\n",
    "                æŸ¥è¯¢ï¼š{query}\n",
    "                \n",
    "                æ–‡æ¡£å†…å®¹ï¼š\n",
    "                {doc.page_content}\n",
    "                \n",
    "                è¯·æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼š<|im_end|>\n",
    "                <|im_start|>assistant\n",
    "                \"\"\"\n",
    "            \n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                max_tokens=300,\n",
    "                stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "            )\n",
    "            \n",
    "            outputs = self.llm.generate([prompt], sampling_params)\n",
    "            if outputs and outputs[0].outputs:\n",
    "                compressed_content = outputs[0].outputs[0].text.strip()\n",
    "                compressed_doc = Document(\n",
    "                    page_content=compressed_content,\n",
    "                    metadata=doc.metadata\n",
    "                )\n",
    "                compressed_docs.append(compressed_doc)\n",
    "        \n",
    "        return compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "299e31e4-e0e5-4fe2-b3d1-6fec223243e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. åˆ›å»ºå‘é‡æ•°æ®åº“\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"docs\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 4. æ·»åŠ æ–‡æ¡£\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Pythoné«˜çº§ç‰¹æ€§\n",
    "        \n",
    "        åˆ—è¡¨æ¨å¯¼å¼ï¼š\n",
    "        åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n",
    "        è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
    "        ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
    "        \n",
    "        ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š\n",
    "        ç”Ÿæˆå™¨ç”¨äºæƒ°æ€§è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ã€‚\n",
    "        è¯­æ³•ï¼š(expression for item in iterable)\n",
    "        \n",
    "        è£…é¥°å™¨ï¼š\n",
    "        è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸ºï¼Œä¸æ”¹å˜åŸå‡½æ•°ä»£ç ã€‚\n",
    "        ä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"python_advanced.md\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "vectorstore.add_documents(docs)\n",
    "\n",
    "# 5. åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨\n",
    "compressed_retriever = VLLMCompressedRetriever(vectorstore, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c56d6c-4a03-41ac-82e0-bd2e5fd10bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ™®é€šæ£€ç´¢:\n",
      "é•¿åº¦: 355 å­—ç¬¦\n",
      "\n",
      "        Pythoné«˜çº§ç‰¹æ€§\n",
      "        \n",
      "        åˆ—è¡¨æ¨å¯¼å¼ï¼š\n",
      "        åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n",
      "        è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "        ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "        \n",
      "        ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š\n",
      "        ç”Ÿæˆå™¨ç”¨äºæƒ°æ€§è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ã€‚\n",
      "        è¯­æ³•ï¼š(expression for item in iterable)\n",
      "        \n",
      "        è£…é¥°å™¨ï¼š\n",
      "        è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸ºï¼Œä¸æ”¹å˜åŸå‡½æ•°ä»£ç ã€‚\n",
      "        ä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” æ™®é€šæ£€ç´¢:\")\n",
    "normal_docs = base_retriever.get_relevant_documents(query)\n",
    "print(f\"é•¿åº¦: {len(normal_docs[0].page_content)} å­—ç¬¦\")\n",
    "print(normal_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640b5f0a-ee7a-4a76-8a7c-506c4ee8e689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ‚ï¸ å‹ç¼©æ£€ç´¢:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.64it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.67it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.67it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é•¿åº¦: 72 å­—ç¬¦\n",
      "åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚è¯­æ³•ï¼š[expression for item in iterable if condition]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nâœ‚ï¸ å‹ç¼©æ£€ç´¢:\")\n",
    "compressed_docs = compressed_retriever.retrieve(query)\n",
    "print(f\"é•¿åº¦: {len(compressed_docs[0].page_content)} å­—ç¬¦\")\n",
    "print(compressed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0742c6-f3f6-493d-8ccf-121366e6ceef",
   "metadata": {},
   "source": [
    "### å®ç°åµŒå…¥è¿‡æ»¤å™¨\n",
    "```python\n",
    "æ“ä½œæµç¨‹ï¼š\n",
    "\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"Pythonåˆ—è¡¨æ¨å¯¼å¼\"\n",
    "    â†“\n",
    "åŸºç¡€æ£€ç´¢å™¨æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "    â†“\n",
    "è·å–å¤šä¸ªå€™é€‰æ–‡æ¡£\n",
    "    â†“\n",
    "è®¡ç®—æŸ¥è¯¢ä¸æ¯ä¸ªæ–‡æ¡£çš„åµŒå…¥ç›¸ä¼¼åº¦\n",
    "    â†“\n",
    "è¿‡æ»¤ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼çš„æ–‡æ¡£\n",
    "    â†“\n",
    "è¿”å›é«˜ç›¸ä¼¼åº¦æ–‡æ¡£\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dceb5f8-c9c0-491a-8ead-3d715b603cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "class EmbeddingFilterRetriever:\n",
    "    \"\"\"åŸºäºåµŒå…¥ç›¸ä¼¼åº¦çš„è¿‡æ»¤å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, base_retriever, embeddings, similarity_threshold: float = 0.75):\n",
    "        # åˆ›å»ºåµŒå…¥è¿‡æ»¤å™¨\n",
    "        compressor = EmbeddingsFilter(\n",
    "            embeddings=embeddings,\n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºå‹ç¼©æ£€ç´¢å™¨\n",
    "        self.retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor,\n",
    "            base_retriever=base_retriever\n",
    "        )\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5):\n",
    "        \"\"\"æ£€ç´¢å¹¶è¿‡æ»¤\"\"\"\n",
    "        docs = self.retriever.get_relevant_documents(query, k=k)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d36224f-3221-42a0-ad89-2f25bcc8ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. åˆ›å»ºåµŒå…¥è¿‡æ»¤å™¨\n",
    "embedding_filter = EmbeddingFilterRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    embeddings=embeddings,\n",
    "    similarity_threshold=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88fd6e91-4a28-44a3-8ef6-ac955e5860fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc_0: \n",
      "        Pythoné«˜çº§ç‰¹æ€§\n",
      "        \n",
      "        åˆ—è¡¨æ¨å¯¼å¼ï¼š\n",
      "        åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n",
      "        è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "        ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "        \n",
      "        ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š\n",
      "        ç”Ÿæˆå™¨ç”¨äºæƒ°æ€§è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ã€‚\n",
      "        è¯­æ³•ï¼š(expression for item in iterable)\n",
      "        \n",
      "        è£…é¥°å™¨ï¼š\n",
      "        è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸ºï¼Œä¸æ”¹å˜åŸå‡½æ•°ä»£ç ã€‚\n",
      "        ä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
      "        \n",
      "Doc_1: \n",
      "        Pythoné«˜çº§ç‰¹æ€§\n",
      "        \n",
      "        åˆ—è¡¨æ¨å¯¼å¼ï¼š\n",
      "        åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n",
      "        è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "        ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "        \n",
      "        ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š\n",
      "        ç”Ÿæˆå™¨ç”¨äºæƒ°æ€§è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ã€‚\n",
      "        è¯­æ³•ï¼š(expression for item in iterable)\n",
      "        \n",
      "        è£…é¥°å™¨ï¼š\n",
      "        è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸ºï¼Œä¸æ”¹å˜åŸå‡½æ•°ä»£ç ã€‚\n",
      "        ä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
      "        \n",
      "Doc_2: \n",
      "        Pythoné«˜çº§ç‰¹æ€§\n",
      "        \n",
      "        åˆ—è¡¨æ¨å¯¼å¼ï¼š\n",
      "        åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n",
      "        è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "        ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "        \n",
      "        ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š\n",
      "        ç”Ÿæˆå™¨ç”¨äºæƒ°æ€§è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ã€‚\n",
      "        è¯­æ³•ï¼š(expression for item in iterable)\n",
      "        \n",
      "        è£…é¥°å™¨ï¼š\n",
      "        è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸ºï¼Œä¸æ”¹å˜åŸå‡½æ•°ä»£ç ã€‚\n",
      "        ä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
      "        \n",
      "Doc_3: \n",
      "        Pythoné«˜çº§ç‰¹æ€§\n",
      "        \n",
      "        åˆ—è¡¨æ¨å¯¼å¼ï¼š\n",
      "        åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n",
      "        è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "        ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "        \n",
      "        ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š\n",
      "        ç”Ÿæˆå™¨ç”¨äºæƒ°æ€§è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ã€‚\n",
      "        è¯­æ³•ï¼š(expression for item in iterable)\n",
      "        \n",
      "        è£…é¥°å™¨ï¼š\n",
      "        è£…é¥°å™¨ç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸ºï¼Œä¸æ”¹å˜åŸå‡½æ•°ä»£ç ã€‚\n",
      "        ä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
      "        \n",
      "è¿‡æ»¤åä¿ç•™ 4 ä¸ªæ–‡æ¡£\n"
     ]
    }
   ],
   "source": [
    "# 4. æ£€ç´¢æµ‹è¯•\n",
    "query = \"Pythonåˆ—è¡¨æ¨å¯¼å¼\"\n",
    "filtered_docs = embedding_filter.retrieve(query, k=5)\n",
    "for i in range(len(filtered_docs)):\n",
    "    print(f\"Doc_{i}: {filtered_docs[i].page_content}\")\n",
    "print(f\"è¿‡æ»¤åä¿ç•™ {len(filtered_docs)} ä¸ªæ–‡æ¡£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aed56-1f9a-4007-9272-38ebda3baa67",
   "metadata": {},
   "source": [
    "### å®ç°æ–‡æ¡£åˆ†å‰²è¿‡æ»¤å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc04e711-8fdf-47e8-9615-c0f9d0a44e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "class SimplePipelineCompressor:\n",
    "    \"\"\"ç®€åŒ–ç‰ˆç®¡é“å‹ç¼©å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, base_retriever, embeddings, llm):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.embeddings = embeddings\n",
    "        self.llm = llm\n",
    "        \n",
    "        # æ–‡æœ¬åˆ†å‰²å™¨\n",
    "        self.splitter = CharacterTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=10,\n",
    "            separator=\". \"\n",
    "        )\n",
    "    \n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"æ‰‹åŠ¨å®ç°ç®¡é“å‹ç¼©\"\"\"\n",
    "        # 1. åŸºç¡€æ£€ç´¢\n",
    "        base_docs = self.base_retriever.get_relevant_documents(query)\n",
    "        # 2. æ–‡æœ¬åˆ†å‰²\n",
    "        split_docs = []\n",
    "        for doc in base_docs:\n",
    "            chunks = self.splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                split_docs.append({\n",
    "                    'content': chunk,\n",
    "                    'metadata': doc.metadata\n",
    "                })\n",
    "        print(f\"length of split_docs: {len(split_docs)}\")\n",
    "        # 3. åµŒå…¥è¿‡æ»¤ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "        filtered_docs = self._embedding_filter(split_docs, query)\n",
    "        \n",
    "        # 4. LLMæå–ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
    "        final_docs = self._llm_extract(filtered_docs, query)\n",
    "        \n",
    "        return final_docs\n",
    "    \n",
    "    def _embedding_filter(self, docs, query):\n",
    "        \"\"\"åµŒå…¥ç›¸ä¼¼åº¦è¿‡æ»¤\"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        import numpy as np\n",
    "        \n",
    "        # è®¡ç®—æŸ¥è¯¢åµŒå…¥\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        filtered_docs = []\n",
    "        for doc in docs:\n",
    "            # è®¡ç®—æ–‡æ¡£åµŒå…¥\n",
    "            doc_embedding = self.embeddings.embed_query(doc['content'])\n",
    "            \n",
    "            # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "            \n",
    "            # åº”ç”¨é˜ˆå€¼\n",
    "            if similarity >= 0.5:\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        return filtered_docs\n",
    "    \n",
    "    def _llm_extract(self, docs, query):\n",
    "        \"\"\"LLMå†…å®¹æå–\"\"\"\n",
    "        from vllm import SamplingParams\n",
    "        \n",
    "        final_docs = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            # æ„å»ºChatMLæ ¼å¼æç¤º\n",
    "            prompt = f\"\"\"<|im_start|>system\n",
    "                ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£å‹ç¼©åŠ©æ‰‹ã€‚è¯·ä»ç»™å®šçš„æ–‡æœ¬ä¸­æå–ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚\n",
    "                \n",
    "                è¯·åªè¿”å›æå–çš„å…³é”®ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯„è®ºã€‚<|im_end|>\n",
    "                <|im_start|>user\n",
    "                æŸ¥è¯¢ï¼š{query}\n",
    "                \n",
    "                æ–‡æœ¬å†…å®¹ï¼š\n",
    "                {doc['content']}\n",
    "                \n",
    "                è¯·æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼š<|im_end|>\n",
    "                <|im_start|>assistant\n",
    "                \"\"\"\n",
    "            \n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                max_tokens=200,\n",
    "                stop=[\"<|im_end|>\"]\n",
    "            )\n",
    "            \n",
    "            outputs = self.llm.generate([prompt], sampling_params)\n",
    "            if outputs and outputs[0].outputs:\n",
    "                extracted_content = outputs[0].outputs[0].text.strip()\n",
    "                \n",
    "                from langchain.schema import Document\n",
    "                final_doc = Document(\n",
    "                    page_content=extracted_content,\n",
    "                    metadata=doc['metadata']\n",
    "                )\n",
    "                final_docs.append(final_doc)\n",
    "        \n",
    "        return final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "568f667c-5a4a-4d8d-aa1e-6928e67a3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"docs\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "base_retriever = vectorstore.as_retriever()\n",
    "\n",
    "# åˆ›å»ºå‹ç¼©å™¨\n",
    "compressor = SimplePipelineCompressor(base_retriever, embeddings, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db21fb55-dba5-475f-a0dd-188cc7d4f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of split_docs: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Pythonåˆ—è¡¨æ¨å¯¼å¼è¯­æ³•ï¼š[expression for item in iterable if condition]\\n                 ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\\n                 ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š(expression for item in iterable)\\n                 è£…é¥°å™¨ï¼šä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚', metadata={'source': 'python_advanced.md'}), Document(page_content='Pythonåˆ—è¡¨æ¨å¯¼å¼è¯­æ³•ï¼š[expression for item in iterable if condition]\\n                 ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\\n                 ç”Ÿæˆå™¨è¡¨è¾¾å¼è¯­æ³•ï¼š(expression for item in iterable)\\n                 è£…é¥°å™¨è¯­æ³•ï¼š@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚', metadata={'source': 'python_advanced.md'}), Document(page_content='Pythonåˆ—è¡¨æ¨å¯¼å¼è¯­æ³•ï¼š[expression for item in iterable if condition]\\n                 ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\\n                 ç”Ÿæˆå™¨è¡¨è¾¾å¼è¯­æ³•ï¼š(expression for item in iterable)\\n                 è£…é¥°å™¨ä½¿ç”¨@ç¬¦å·åº”ç”¨', metadata={'source': 'python_advanced.md'}), Document(page_content='åˆ—è¡¨æ¨å¯¼å¼ï¼š[expression for item in iterable if condition]\\n                 ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š(expression for item in iterable)\\n                 è£…é¥°å™¨ï¼š@ç¬¦å·åº”ç”¨è£…é¥°å™¨', metadata={'source': 'python_advanced.md'})]\n",
      "ç»“æœ 1: Pythonåˆ—è¡¨æ¨å¯¼å¼è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "                 ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "                 ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š(expression for item in iterable)\n",
      "                 è£…é¥°å™¨ï¼šä½¿ç”¨@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
      "ç»“æœ 2: Pythonåˆ—è¡¨æ¨å¯¼å¼è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "                 ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "                 ç”Ÿæˆå™¨è¡¨è¾¾å¼è¯­æ³•ï¼š(expression for item in iterable)\n",
      "                 è£…é¥°å™¨è¯­æ³•ï¼š@ç¬¦å·åº”ç”¨è£…é¥°å™¨ã€‚\n",
      "ç»“æœ 3: Pythonåˆ—è¡¨æ¨å¯¼å¼è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
      "                 ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
      "                 ç”Ÿæˆå™¨è¡¨è¾¾å¼è¯­æ³•ï¼š(expression for item in iterable)\n",
      "                 è£…é¥°å™¨ä½¿ç”¨@ç¬¦å·åº”ç”¨\n",
      "ç»“æœ 4: åˆ—è¡¨æ¨å¯¼å¼ï¼š[expression for item in iterable if condition]\n",
      "                 ç”Ÿæˆå™¨è¡¨è¾¾å¼ï¼š(expression for item in iterable)\n",
      "                 è£…é¥°å™¨ï¼š@ç¬¦å·åº”ç”¨è£…é¥°å™¨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# æ£€ç´¢\n",
    "results = compressor.retrieve(\"Pythonåˆ—è¡¨æ¨å¯¼å¼\")\n",
    "print(results)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"ç»“æœ {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e3510-6f70-4bbb-b9ee-4bf35bc768eb",
   "metadata": {},
   "source": [
    "### å‹ç¼©ç­–ç•¥å¯¹æ¯”\n",
    "\n",
    "| å‹ç¼©å™¨ | æ–¹æ³• | é€Ÿåº¦ | è´¨é‡ | æˆæœ¬ |\n",
    "|--------|------|------|------|------|\n",
    "| LLMæå–å™¨ | LLMæå–ç›¸å…³å†…å®¹ | æ…¢ | é«˜ | é«˜ |\n",
    "| åµŒå…¥è¿‡æ»¤å™¨ | ç›¸ä¼¼åº¦è¿‡æ»¤ | å¿« | ä¸­ | ä½ |\n",
    "| ç®¡é“å‹ç¼© | ç»„åˆå¤šç§æ–¹æ³• | ä¸­ | é«˜ | ä¸­ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa24de-42cb-4e8c-b322-af9e0e017cc8",
   "metadata": {},
   "source": [
    "## Part 5: æ—¶é—´è¡°å‡æ£€ç´¢ - Time-Weighted Retrieval\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šæ—¶é—´è¡°å‡æ£€ç´¢è€ƒè™‘æ–‡æ¡£çš„æ–°é²œåº¦ï¼Œç»™äºˆæ–°æ–‡æ¡£æ›´é«˜çš„æƒé‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0a5ba-7362-4f82-a26a-78dfc618e5b3",
   "metadata": {},
   "source": [
    "### å®ç°æ—¶é—´åŠ æƒæ£€ç´¢å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "57985956-2401-4be6-8ac9-5aa515bd6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "import datetime\n",
    "\n",
    "class TimeSensitiveRetriever:\n",
    "    \"\"\"æ—¶é—´æ•æ„Ÿæ£€ç´¢å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, decay_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decay_rate: è¡°å‡ç‡ï¼Œè¶Šå¤§åˆ™æ—¶é—´å½±å“è¶Šå¤§\n",
    "        \"\"\"\n",
    "        self.retriever = TimeWeightedVectorStoreRetriever(\n",
    "            vectorstore=vectorstore,\n",
    "            decay_rate=decay_rate,\n",
    "            k=2\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£ï¼ˆä¼šè‡ªåŠ¨è®°å½•æ—¶é—´ï¼‰\"\"\"\n",
    "        self.retriever.add_documents(documents)\n",
    "    \n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"æ£€ç´¢ï¼ˆè€ƒè™‘æ—¶é—´å› ç´ ï¼‰\"\"\"\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        print(f\"docs: {docs}\")\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c67726a9-4f59-422f-a9d6-5ecd85b40b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"news\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "time_retriever = TimeSensitiveRetriever(vectorstore, decay_rate=0.01)\n",
    "\n",
    "old_doc = Document(\n",
    "    page_content=\"2023å¹´1æœˆï¼šPython 3.11å‘å¸ƒ\",\n",
    "    metadata={\"date\": \"2023-01-01\"}\n",
    ")\n",
    "\n",
    "recent_doc = Document(\n",
    "    page_content=\"2024å¹´10æœˆï¼šPython 3.13å‘å¸ƒï¼Œæ€§èƒ½æå‡æ˜¾è‘—\",\n",
    "    metadata={\"date\": \"2024-10-01\"}\n",
    ")\n",
    "\n",
    "# vectorstore.add_documents([old_doc])\n",
    "# vectorstore.add_documents([recent_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "135e512e-9989-4069-972f-79fadbc9f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs: []\n",
      "\n",
      "æ£€ç´¢ç»“æœï¼ˆæŒ‰æ—¶é—´åŠ æƒï¼‰:\n"
     ]
    }
   ],
   "source": [
    "# æ£€ç´¢ï¼šæ–°æ–‡æ¡£ä¼šè·å¾—æ›´é«˜æƒé‡\n",
    "results = time_retriever.retrieve(\"Pythonæœ€æ–°ç‰ˆæœ¬\")\n",
    "\n",
    "print(\"\\næ£€ç´¢ç»“æœï¼ˆæŒ‰æ—¶é—´åŠ æƒï¼‰:\")\n",
    "for doc in results:\n",
    "    print(f\"\\n{doc.metadata['date']}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d9ef0-b0d4-486d-a4e0-4d18a657be36",
   "metadata": {},
   "source": [
    "## ç»¼åˆå®æˆ˜ï¼šæ„å»ºé«˜çº§RAGç³»ç»Ÿ\n",
    "\n",
    "å°†å‰é¢çš„æ£€ç´¢å™¨éƒ½é›†æˆåˆ°ä¸€ä¸ªç³»ç»Ÿä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b3dcae7e-21fb-4c40-86de-a355415902eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from typing import Dict, List\n",
    "\n",
    "class AdvancedRAGSystem:\n",
    "    \"\"\"é«˜çº§RAGç³»ç»Ÿï¼ˆç»¼åˆæ£€ç´¢å™¨ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, llm):\n",
    "        self.embeddings = embeddings\n",
    "        self.llm = llm\n",
    "        \n",
    "        # å‘é‡å­˜å‚¨\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"advanced_rag\",\n",
    "            persist_directory=\"./chroma_db\",\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # åŸºç¡€æ£€ç´¢å™¨\n",
    "        self.base_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    def index_document(self, content: str, metadata: Dict = None):\n",
    "        \"\"\"ç´¢å¼•æ–‡æ¡£\"\"\"\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        self.vectorstore.add_documents([doc])\n",
    "        print(\"âœ… æ–‡æ¡£å·²ç´¢å¼•\")\n",
    "    \n",
    "    def query_with_compression(self, question: str, k: int = 3):\n",
    "        \"\"\"ä½¿ç”¨å‹ç¼©æ£€ç´¢å™¨\"\"\"\n",
    "        from vllm import SamplingParams\n",
    "        \n",
    "        # åŸºç¡€æ£€ç´¢\n",
    "        base_docs = self.base_retriever.get_relevant_documents(question, k=k*2)\n",
    "        \n",
    "        # å‹ç¼©æ–‡æ¡£\n",
    "        compressed_docs = []\n",
    "        for doc in base_docs:\n",
    "            prompt = f\"\"\"<|im_start|>system\n",
    "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£å‹ç¼©åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚\n",
    "\n",
    "è¯·åªè¿”å›æå–çš„å…³é”®ä¿¡æ¯ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯„è®ºã€‚<|im_end|>\n",
    "<|im_start|>user\n",
    "æŸ¥è¯¢ï¼š{question}\n",
    "\n",
    "æ–‡æœ¬å†…å®¹ï¼š\n",
    "{doc.page_content}\n",
    "\n",
    "è¯·æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼š<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "            \n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                max_tokens=300,\n",
    "                stop=[\"<|im_end|>\"]\n",
    "            )\n",
    "            \n",
    "            outputs = self.llm.generate([prompt], sampling_params)\n",
    "            if outputs and outputs[0].outputs:\n",
    "                compressed_content = outputs[0].outputs[0].text.strip()\n",
    "                compressed_doc = Document(\n",
    "                    page_content=compressed_content,\n",
    "                    metadata=doc.metadata\n",
    "                )\n",
    "                compressed_docs.append(compressed_doc)\n",
    "        \n",
    "        return compressed_docs[:k]\n",
    "    \n",
    "    def query_with_embedding_filter(self, question: str, k: int = 3, similarity_threshold: float = 0.7):\n",
    "        \"\"\"ä½¿ç”¨åµŒå…¥è¿‡æ»¤å™¨\"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        import numpy as np\n",
    "        \n",
    "        # åŸºç¡€æ£€ç´¢\n",
    "        base_docs = self.base_retriever.get_relevant_documents(question, k=k*3)\n",
    "        \n",
    "        # è®¡ç®—æŸ¥è¯¢åµŒå…¥\n",
    "        query_embedding = self.embeddings.embed_query(question)\n",
    "        \n",
    "        # è¿‡æ»¤ç›¸ä¼¼åº¦ä½çš„æ–‡æ¡£\n",
    "        filtered_docs = []\n",
    "        for doc in base_docs:\n",
    "            doc_embedding = self.embeddings.embed_query(doc.page_content)\n",
    "            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        return filtered_docs[:k]\n",
    "    \n",
    "    def query_with_pipeline(self, question: str, k: int = 3):\n",
    "        \"\"\"ä½¿ç”¨ç®¡é“å‹ç¼©ï¼ˆå‹ç¼©+è¿‡æ»¤ï¼‰\"\"\"\n",
    "        # å…ˆè¿‡æ»¤\n",
    "        filtered_docs = self.query_with_embedding_filter(question, k=k*2, similarity_threshold=0.6)\n",
    "        \n",
    "        # å†å‹ç¼©\n",
    "        compressed_docs = []\n",
    "        for doc in filtered_docs:\n",
    "            prompt = f\"ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä¸'{question}'ç›¸å…³çš„å…³é”®ä¿¡æ¯ï¼š\\n\\n{doc.page_content}\\n\\nå…³é”®ä¿¡æ¯ï¼š\"\n",
    "            \n",
    "            from vllm import SamplingParams\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                max_tokens=200,\n",
    "                stop=[\"<|im_end|>\"]\n",
    "            )\n",
    "            \n",
    "            outputs = self.llm.generate([prompt], sampling_params)\n",
    "            if outputs and outputs[0].outputs:\n",
    "                compressed_content = outputs[0].outputs[0].text.strip()\n",
    "                compressed_doc = Document(\n",
    "                    page_content=compressed_content,\n",
    "                    metadata=doc.metadata\n",
    "                )\n",
    "                compressed_docs.append(compressed_doc)\n",
    "        \n",
    "        return compressed_docs[:k]\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        method: str = \"compression\",  # \"compression\", \"filter\", \"pipeline\", \"basic\"\n",
    "        k: int = 3\n",
    "    ):\n",
    "        \"\"\"ç»¼åˆæ£€ç´¢æ–¹æ³•\"\"\"\n",
    "        print(f\"â“ æŸ¥è¯¢: {question}\")\n",
    "        print(f\"ğŸ”§ æ–¹æ³•: {method}\")\n",
    "        \n",
    "        # é€‰æ‹©æ£€ç´¢æ–¹æ³•\n",
    "        if method == \"compression\":\n",
    "            print(\"âœ‚ï¸ ä½¿ç”¨å‹ç¼©æ£€ç´¢\")\n",
    "            docs = self.query_with_compression(question, k)\n",
    "        elif method == \"filter\":\n",
    "            print(\"ğŸ¯ ä½¿ç”¨åµŒå…¥è¿‡æ»¤\")\n",
    "            docs = self.query_with_embedding_filter(question, k)\n",
    "        elif method == \"pipeline\":\n",
    "            print(\"âš¡ ä½¿ç”¨ç®¡é“å‹ç¼©\")\n",
    "            docs = self.query_with_pipeline(question, k)\n",
    "        else:\n",
    "            print(\"ğŸ” ä½¿ç”¨åŸºç¡€æ£€ç´¢\")\n",
    "            docs = self.base_retriever.get_relevant_documents(question, k=k)\n",
    "        \n",
    "        print(f\"ğŸ“„ æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        # ç”Ÿæˆç­”æ¡ˆ\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚\n",
    "            \n",
    "            æ–‡æ¡£å†…å®¹ï¼š\n",
    "            {context}<|im_end|>\n",
    "            <|im_start|>user\n",
    "            é—®é¢˜ï¼š{question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        from vllm import SamplingParams\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=500,\n",
    "            stop=[\"<|im_end|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        answer = outputs[0].outputs[0].text.strip() if outputs and outputs[0].outputs else \"æœªèƒ½ç”Ÿæˆç­”æ¡ˆ\"\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"method\": method,\n",
    "            \"documents\": docs,\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c076d979-550d-4946-8193-14d99b16289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ–‡æ¡£å·²ç´¢å¼•\n"
     ]
    }
   ],
   "source": [
    "# 3. åˆ›å»ºé«˜çº§RAGç³»ç»Ÿ\n",
    "advanced_rag = AdvancedRAGSystem(embeddings, llm)\n",
    "\n",
    "# 4. ç´¢å¼•æ–‡æ¡£\n",
    "python_content = \"\"\"\n",
    "Pythonç¼–ç¨‹è¯­è¨€åŸºç¡€\n",
    "\n",
    "å‡½æ•°å®šä¹‰ï¼š\n",
    "ä½¿ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°ï¼š\n",
    "def function_name(parameters):\n",
    "    # å‡½æ•°ä½“\n",
    "    return result\n",
    "\n",
    "å‡½æ•°å¯ä»¥æ¥å—å‚æ•°ï¼Œä¹Ÿå¯ä»¥è¿”å›å€¼ã€‚\n",
    "å‚æ•°å¯ä»¥æœ‰é»˜è®¤å€¼ï¼Œä½¿ç”¨parameter=default_valueè¯­æ³•ã€‚\n",
    "\n",
    "ç¤ºä¾‹ï¼š\n",
    "def greet(name=\"World\"):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "è°ƒç”¨å‡½æ•°ï¼š\n",
    "result = greet(\"Alice\")\n",
    "print(result)  # è¾“å‡º: Hello, Alice!\n",
    "\n",
    "åˆ—è¡¨æ¨å¯¼å¼ï¼š\n",
    "åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚\n",
    "è¯­æ³•ï¼š[expression for item in iterable if condition]\n",
    "ç¤ºä¾‹ï¼šsquares = [x**2 for x in range(10)]\n",
    "\"\"\"\n",
    "\n",
    "advanced_rag.index_document(\n",
    "    content=python_content,\n",
    "    metadata={\"source\": \"python_tutorial.md\", \"type\": \"tutorial\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84ee2b13-bf02-43b3-84e7-7216f81a1d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "â“ æŸ¥è¯¢: Pythonä¸­çš„å‡½æ•°å¦‚ä½•å®šä¹‰ï¼Ÿ\n",
      "ğŸ”§ æ–¹æ³•: basic\n",
      "ğŸ” ä½¿ç”¨åŸºç¡€æ£€ç´¢\n",
      "ğŸ“„ æ£€ç´¢åˆ° 1 ä¸ªæ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ åŸºç¡€æ£€ç´¢ç­”æ¡ˆ: ä½¿ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°ï¼Œå‡½æ•°å¯ä»¥æ¥å—å‚æ•°ï¼Œä¹Ÿå¯ä»¥è¿”å›å€¼ã€‚å‚æ•°å¯ä»¥æœ‰é»˜è®¤å€¼ï¼Œä½¿ç”¨parameter=default_valueè¯­æ³•ã€‚...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. æµ‹è¯•ä¸åŒæ£€ç´¢æ–¹æ³•\n",
    "question = \"Pythonä¸­çš„å‡½æ•°å¦‚ä½•å®šä¹‰ï¼Ÿ\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "result1 = advanced_rag.query(question, method=\"basic\", k=2)\n",
    "print(f\"\\nğŸ’¡ åŸºç¡€æ£€ç´¢ç­”æ¡ˆ: {result1['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9203fe70-d1fd-4c18-bfe2-eff93bc0d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "â“ æŸ¥è¯¢: Pythonä¸­çš„å‡½æ•°å¦‚ä½•å®šä¹‰ï¼Ÿ\n",
      "ğŸ”§ æ–¹æ³•: filter\n",
      "ğŸ¯ ä½¿ç”¨åµŒå…¥è¿‡æ»¤\n",
      "ğŸ“„ æ£€ç´¢åˆ° 1 ä¸ªæ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ è¿‡æ»¤æ£€ç´¢ç­”æ¡ˆ: ä½¿ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°ï¼Œå‡½æ•°å¯ä»¥æ¥å—å‚æ•°ï¼Œä¹Ÿå¯ä»¥è¿”å›å€¼ã€‚å‚æ•°å¯ä»¥æœ‰é»˜è®¤å€¼ï¼Œä½¿ç”¨parameter=default_valueè¯­æ³•ã€‚...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "result2 = advanced_rag.query(question, method=\"filter\", k=2)\n",
    "print(f\"\\nğŸ’¡ è¿‡æ»¤æ£€ç´¢ç­”æ¡ˆ: {result2['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "507be4a3-252d-42fc-a7f8-855c4b962001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "â“ æŸ¥è¯¢: Pythonä¸­çš„å‡½æ•°å¦‚ä½•å®šä¹‰ï¼Ÿ\n",
      "ğŸ”§ æ–¹æ³•: compression\n",
      "âœ‚ï¸ ä½¿ç”¨å‹ç¼©æ£€ç´¢\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æ£€ç´¢åˆ° 1 ä¸ªæ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ å‹ç¼©æ£€ç´¢ç­”æ¡ˆ: Pythonå‡½æ•°å®šä¹‰ä½¿ç”¨defå…³é”®å­—ï¼Œå¯ä»¥æ¥å—å‚æ•°ï¼Œä¹Ÿå¯ä»¥è¿”å›å€¼ã€‚å‚æ•°å¯ä»¥æœ‰é»˜è®¤å€¼ï¼Œä½¿ç”¨parameter=default_valueè¯­æ³•ã€‚...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "result3 = advanced_rag.query(question, method=\"compression\", k=2)\n",
    "print(f\"\\nğŸ’¡ å‹ç¼©æ£€ç´¢ç­”æ¡ˆ: {result3['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40453526-7a2f-48c7-b300-90b7380f0387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "â“ æŸ¥è¯¢: Pythonä¸­çš„å‡½æ•°å¦‚ä½•å®šä¹‰ï¼Ÿ\n",
      "ğŸ”§ æ–¹æ³•: pipeline\n",
      "âš¡ ä½¿ç”¨ç®¡é“å‹ç¼©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æ£€ç´¢åˆ° 1 ä¸ªæ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ ç®¡é“æ£€ç´¢ç­”æ¡ˆ: Pythonä¸­çš„å‡½æ•°å®šä¹‰ä½¿ç”¨defå…³é”®å­—ï¼Œå¯ä»¥æ¥å—å‚æ•°ï¼Œä¹Ÿå¯ä»¥è¿”å›å€¼ã€‚å‡½æ•°å¯ä»¥æœ‰é»˜è®¤å€¼ï¼Œå‚æ•°å¯ä»¥æœ‰é»˜è®¤å€¼ã€‚åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚å‡½æ•°å®šä¹‰çš„è¯­æ³•æ˜¯ï¼šdef function_na...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "result4 = advanced_rag.query(question, method=\"pipeline\", k=2)\n",
    "print(f\"\\nğŸ’¡ ç®¡é“æ£€ç´¢ç­”æ¡ˆ: {result4['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247a0c9-0655-4639-a3bf-8d994faa3347",
   "metadata": {},
   "source": [
    "## æ€§èƒ½ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95638ff-d678-42f4-b11b-2f94954f752b",
   "metadata": {},
   "source": [
    "### 1.ç´¢å¼•ä¼˜åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61122fdd-8039-4781-9180-039b9d4648cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹é‡ç´¢å¼•\n",
    "def batch_index(documents: List[str], batch_size: int = 100):\n",
    "    \"\"\"æ‰¹é‡ç´¢å¼•æ–‡æ¡£\"\"\"\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "        print(f\"âœ… å·²ç´¢å¼• {i+batch_size}/{len(documents)}\")\n",
    "\n",
    "# å¼‚æ­¥ç´¢å¼•\n",
    "import asyncio\n",
    "\n",
    "async def async_index(documents: List[str]):\n",
    "    \"\"\"å¼‚æ­¥ç´¢å¼•\"\"\"\n",
    "    tasks = [\n",
    "        vectorstore.aadd_documents([doc])\n",
    "        for doc in documents\n",
    "    ]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c45099-4ddd-4f14-8a15-a9b60d9acec1",
   "metadata": {},
   "source": [
    "### 2. å—å¤§å°é€‰æ‹©æŒ‡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9586c2e0-5d50-49e8-9508-47968e5e2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©å—å¤§å°\n",
    "\n",
    "CHUNK_SIZES = {\n",
    "    \"code\": {\n",
    "        \"chunk_size\": 300,      # ä»£ç ç‰‡æ®µé€šå¸¸è¾ƒçŸ­\n",
    "        \"chunk_overlap\": 50\n",
    "    },\n",
    "    \"article\": {\n",
    "        \"chunk_size\": 1000,     # æ–‡ç« æ®µè½è¾ƒé•¿\n",
    "        \"chunk_overlap\": 200\n",
    "    },\n",
    "    \"qa\": {\n",
    "        \"chunk_size\": 500,      # QAå¯¹ä¸­ç­‰é•¿åº¦\n",
    "        \"chunk_overlap\": 100\n",
    "    },\n",
    "    \"academic\": {\n",
    "        \"chunk_size\": 1500,     # å­¦æœ¯è®ºæ–‡éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡\n",
    "        \"chunk_overlap\": 300\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_optimal_chunker(doc_type: str):\n",
    "    \"\"\"è·å–æœ€ä¼˜åˆ†å—å™¨\"\"\"\n",
    "    config = CHUNK_SIZES.get(doc_type, CHUNK_SIZES[\"article\"])\n",
    "    \n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config[\"chunk_size\"],\n",
    "        chunk_overlap=config[\"chunk_overlap\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49019b2d-b916-4c2d-bbc0-9cef4190cb39",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "1. æ™ºèƒ½åˆ†å—: æ–‡æ¡£è´¨é‡çš„åŸºç¡€\n",
    "2. å¤šå‘é‡ç´¢å¼•: æå‡è¯­ä¹‰è¡¨ç¤º\n",
    "3. çˆ¶æ–‡æ¡£æ£€ç´¢: å¹³è¡¡ç²¾åº¦å’Œä¸Šä¸‹æ–‡\n",
    "4. ä¸Šä¸‹æ–‡å‹ç¼©: èŠ‚çœtokensï¼Œæé«˜è´¨é‡\n",
    "\n",
    "## æŠ€æœ¯é€‰æ‹©\n",
    "\n",
    "```python\n",
    "æ–‡æ¡£ç´¢å¼•ç­–ç•¥:\n",
    "â”œâ”€ çŸ­æ–‡æ¡£ï¼ˆ<1000å­—ï¼‰\n",
    "â”‚  â†’ ç›´æ¥ç´¢å¼•å®Œæ•´æ–‡æ¡£\n",
    "â”œâ”€ ä¸­ç­‰æ–‡æ¡£ï¼ˆ1000-5000å­—ï¼‰\n",
    "â”‚  â†’ æ™ºèƒ½åˆ†å— + çˆ¶æ–‡æ¡£æ£€ç´¢\n",
    "â””â”€ é•¿æ–‡æ¡£ï¼ˆ>5000å­—ï¼‰\n",
    "   â†’ å¤šå‘é‡ç´¢å¼• + å‹ç¼©æ£€ç´¢\n",
    "\n",
    "æ£€ç´¢ä¼˜åŒ–:\n",
    "â”œâ”€ éœ€è¦ç²¾ç¡®åŒ¹é…\n",
    "â”‚  â†’ å°å—æ£€ç´¢ + çˆ¶æ–‡æ¡£è¿”å›\n",
    "â”œâ”€ éœ€è¦èŠ‚çœtokens\n",
    "â”‚  â†’ ä¸Šä¸‹æ–‡å‹ç¼©\n",
    "â””â”€ æœ‰æ—¶æ•ˆæ€§è¦æ±‚\n",
    "   â†’ æ—¶é—´è¡°å‡æ£€ç´¢\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315ece8-44e1-4993-ba24-d8fb2042762d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
