{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000e5a88-123f-41c9-9c59-a88f9a96b950",
   "metadata": {},
   "source": [
    "# RAGæŸ¥è¯¢ä¼˜åŒ–ï¼šå¤šæŸ¥è¯¢ä¸æŸ¥è¯¢è½¬æ¢æŠ€æœ¯\n",
    "\n",
    "åœ¨åŸºç¡€RAGç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å•ä¸€æŸ¥è¯¢è¿›è¡Œæ£€ç´¢ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç”¨æˆ·çš„æŸ¥è¯¢å¾€å¾€å­˜åœ¨è¡¨è¾¾æ¨¡ç³Šã€è§’åº¦å•ä¸€æˆ–è¿‡äºç¬¼ç»Ÿçš„é—®é¢˜ã€‚æœ¬ç« å°†ä»‹ç»å¤šç§æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯ï¼Œè®©ä½ çš„RAGç³»ç»Ÿèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·æ„å›¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914a684-6815-490c-ad96-58544c5e3a95",
   "metadata": {},
   "source": [
    "## æŸ¥è¯¢ä¼˜åŒ–çš„å¿…è¦æ€§\n",
    "\n",
    "### å•ä¸€æŸ¥è¯¢çš„å±€é™æ€§\n",
    "\n",
    "```python\n",
    "\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "# å¯èƒ½é”™è¿‡çš„ç›¸å…³æ–‡æ¡£ï¼š\n",
    "- \"æ·±åº¦å­¦ä¹ å…¥é—¨\" (ä½¿ç”¨äº†ä¸åŒä½†ç›¸å…³çš„æœ¯è¯­)\n",
    "- \"AIç®—æ³•åŸºç¡€\" (æ›´å¹¿æ³›çš„ä¸»é¢˜)\n",
    "- \"ç¥ç»ç½‘ç»œåŸç†\" (å…·ä½“æŠ€æœ¯)\n",
    "- \"ç›‘ç£å­¦ä¹ vsæ— ç›‘ç£å­¦ä¹ \" (ç»†åˆ†è¯é¢˜)\n",
    "```\n",
    "\n",
    "### æŸ¥è¯¢ä¼˜åŒ–å¦‚ä½•å¸®åŠ©ï¼Ÿ\n",
    "\n",
    "æŸ¥è¯¢ä¼˜åŒ–çš„æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡ç”Ÿæˆå¤šä¸ªè§’åº¦çš„æŸ¥è¯¢ã€é‡å†™æŸ¥è¯¢æˆ–åˆ†è§£å¤æ‚æŸ¥è¯¢ï¼Œå¢åŠ æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "```python\n",
    "# âœ… æŸ¥è¯¢ä¼˜åŒ–å\n",
    "\n",
    "åŸå§‹æŸ¥è¯¢: \"æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "ç”Ÿæˆçš„å˜ä½“:\n",
    "1. \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ\"\n",
    "2. \"æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’ŒåŸç†\"\n",
    "3. \"AIä¸­çš„æœºå™¨å­¦ä¹ æŠ€æœ¯\"\n",
    "4. \"æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯\"\n",
    "\n",
    "â†’ æ£€ç´¢ â†’ åˆå¹¶ç»“æœ â†’ å»é‡ â†’ ç”Ÿæˆç­”æ¡ˆ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44b705-7258-43b9-9160-1d834b96341e",
   "metadata": {},
   "source": [
    "## æŠ€æœ¯æ¦‚è§ˆ\n",
    "\n",
    "æœ¬ç« å°†ä»‹ç»5ç§ä¸»è¦çš„æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯ï¼š\n",
    "\n",
    "| æŠ€æœ¯ | æ ¸å¿ƒæ€æƒ³ | é€‚ç”¨åœºæ™¯ | å¤æ‚åº¦ |\n",
    "|:-----|:---------|:---------|:-------|\n",
    "| Multi-Query | ç”ŸæˆæŸ¥è¯¢çš„å¤šä¸ªå˜ä½“ | ç”¨æˆ·æŸ¥è¯¢è¡¨è¾¾ä¸æ¸… | â­ |\n",
    "| RAG-Fusion | å¤šæŸ¥è¯¢+é‡æ’åºèåˆ | éœ€è¦é«˜è´¨é‡ç»“æœ | â­â­ |\n",
    "| Decomposition | åˆ†è§£å¤æ‚æŸ¥è¯¢ | å¤šæ­¥éª¤é—®é¢˜ | â­â­â­ |\n",
    "| Step Back | å…ˆé—®æ¦‚æ‹¬æ€§é—®é¢˜ | éœ€è¦èƒŒæ™¯çŸ¥è¯† | â­â­ |\n",
    "| HyDE | ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£ | è¯­ä¹‰æœç´¢å¢å¼º | â­â­â­ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48be711d-b768-4956-a7e7-10734b3b361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/shared-nvme/conda-envs/py310/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 09:52:04 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 11-11 09:52:04 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', speculative_config=None, tokenizer='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8)\n",
      "WARNING 11-11 09:52:05 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-11 09:52:05 utils.py:660] Found nccl from library /usr/lib/x86_64-linux-gnu/libnccl.so.2\n",
      "INFO 11-11 09:52:05 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 11-11 09:52:05 selector.py:32] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 09:52:15,933 - modelscope - INFO - PyTorch version 2.3.0+cu118 Found.\n",
      "2025-11-11 09:52:15,935 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-11-11 09:52:16,000 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 298ceecce207285dd10b135af16e71cc and a total number of 964 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-11 09:52:29 model_runner.py:175] Loading model weights took 8.4983 GB\n",
      "INFO 11-11 09:52:32 gpu_executor.py:114] # GPU blocks: 1404, # CPU blocks: 512\n",
      "INFO 11-11 09:52:36 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-11 09:52:36 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-11 09:52:46 model_runner.py:1017] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹å‡†å¤‡å·¥ä½œ\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from vllm import LLM\n",
    "\n",
    "# 1. åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹\n",
    "local_model_path = \"./Models/maidalun/bce-embedding-base_v1\" \n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=local_model_path,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 2. åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“\n",
    "vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# 3. åŠ è½½æœ¬åœ°å¤§æ¨¡å‹\n",
    "model_dir=\"../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8\"\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "llm = LLM(\n",
    "    model=model_dir,\n",
    "    tokenizer=model_dir,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303933ea-c91f-46bc-8c51-b014d2e22bb0",
   "metadata": {},
   "source": [
    "## Part 1ï¼šMulti-Query - å¤šè§’åº¦æŸ¥è¯¢\n",
    "\n",
    "æ ¸å¿ƒæ€æƒ³ï¼šMulti-QueryæŠ€æœ¯é€šè¿‡LLMç”ŸæˆåŸå§‹æŸ¥è¯¢çš„å¤šä¸ªå˜ä½“ï¼Œä»ä¸åŒè§’åº¦æ£€ç´¢æ–‡æ¡£ï¼Œæé«˜æ£€ç´¢çš„å¬å›ç‡ã€‚\n",
    "\n",
    "å·¥ä½œæµç¨‹ï¼š\n",
    "\n",
    "```python\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"ä»€ä¹ˆæ˜¯Agentï¼Ÿ\"\n",
    "    â†“\n",
    "ä½¿ç”¨LLMç”Ÿæˆå˜ä½“:\n",
    "    â”œâ”€ \"Agentç³»ç»Ÿçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "    â”œâ”€ \"AI Agentçš„æ ¸å¿ƒæ¦‚å¿µ\"\n",
    "    â””â”€ \"ä»€ä¹ˆæ˜¯è‡ªä¸»æ™ºèƒ½ä½“ï¼Ÿ\"\n",
    "    â†“\n",
    "å¹¶è¡Œæ£€ç´¢æ¯ä¸ªå˜ä½“\n",
    "    â†“\n",
    "åˆå¹¶å¹¶å»é‡ç»“æœ\n",
    "    â†“\n",
    "ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc0b18b-4936-44e1-b37d-01699db0afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ChatMLæ ¼å¼çš„Multi-Query RAG\n",
      "============================================================\n",
      "âœ… Multi-Query RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n",
      "ğŸ¯ åŸå§‹é—®é¢˜: ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆäº† 4 ä¸ªæŸ¥è¯¢:\n",
      "   1. ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ\n",
      "   2. æœºæ¢°è‡‚æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "   3. æœºæ¢°è‡‚æœ‰å“ªäº›åŠŸèƒ½ï¼Ÿ\n",
      "   4. æœºæ¢°è‡‚çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "   ğŸ” 'ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ': æ‰¾åˆ° 3 ä¸ªæ–‡æ¡£\n",
      "   ğŸ” 'æœºæ¢°è‡‚æ˜¯ä»€ä¹ˆï¼Ÿ': æ‰¾åˆ° 3 ä¸ªæ–‡æ¡£\n",
      "   ğŸ” 'æœºæ¢°è‡‚æœ‰å“ªäº›åŠŸèƒ½ï¼Ÿ': æ‰¾åˆ° 3 ä¸ªæ–‡æ¡£\n",
      "   ğŸ” 'æœºæ¢°è‡‚çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ': æ‰¾åˆ° 3 ä¸ªæ–‡æ¡£\n",
      "ğŸ“š æ£€ç´¢åˆ° 12 ä¸ªæ–‡æ¡£ï¼ˆå«é‡å¤ï¼‰\n",
      "âœ¨ å»é‡åå‰©ä½™ 6 ä¸ªæ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ’¡ ç­”æ¡ˆ: æœºæ¢°è‡‚æ˜¯ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨æ‰§è¡Œä»»åŠ¡çš„æœºå™¨äººæ‰‹è‡‚ï¼Œå®ƒç”±å¤šä¸ªå…³èŠ‚ç»„æˆï¼Œå¯ä»¥æŒ‰ç…§é¢„è®¾çš„ç¨‹åºæˆ–æŒ‡ä»¤è¿›è¡Œç²¾ç¡®çš„è¿åŠ¨ã€‚æœºæ¢°è‡‚å…·æœ‰å¼ºå¤§çš„åŠ›é‡å’Œç²¾ç¡®åº¦ï¼Œå¯ä»¥å®Œæˆå„ç§å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æŠ“å–ã€æ¬è¿ã€è£…é…ç­‰ã€‚\n",
      "ğŸ“ ä½¿ç”¨çš„æŸ¥è¯¢: ['ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ', 'æœºæ¢°è‡‚æ˜¯ä»€ä¹ˆï¼Ÿ', 'æœºæ¢°è‡‚æœ‰å“ªäº›åŠŸèƒ½ï¼Ÿ', 'æœºæ¢°è‡‚çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ']\n",
      "ğŸ“š å‚è€ƒæ–‡æ¡£æ•°: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from vllm import LLM, SamplingParams\n",
    "from modelscope import snapshot_download\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def get_unique_documents(documents: List[List]) -> List:\n",
    "    \"\"\"å»é‡æ–‡æ¡£\"\"\"\n",
    "    unique_docs = {}\n",
    "    for doc_list in documents:\n",
    "        for doc in doc_list:\n",
    "            content = doc.page_content\n",
    "            if content not in unique_docs:\n",
    "                unique_docs[content] = doc\n",
    "    return list(unique_docs.values())\n",
    "\n",
    "class MultiQueryRAG:\n",
    "    \"\"\"å®Œå…¨ä½¿ç”¨ChatMLæ ¼å¼çš„Multi-Query RAGç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–Multi-Query RAG\n",
    "        \n",
    "        Args:\n",
    "            vectorstore: å‘é‡æ•°æ®åº“\n",
    "            llm: è¯­è¨€æ¨¡å‹\n",
    "        \"\"\"\n",
    "        self.vectorstore = vectorstore\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        self.llm = llm\n",
    "        print(\"âœ… Multi-Query RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def _generate_query_variants(self, question: str) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨ChatMLæ ¼å¼ç”ŸæˆæŸ¥è¯¢å˜ä½“\"\"\"\n",
    "        query_prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿å°†ç”¨æˆ·çš„é—®é¢˜æ”¹å†™æˆå¤šä¸ªè¯­ä¹‰ç›¸åŒä½†è¡¨è¾¾ä¸åŒçš„æœç´¢æŸ¥è¯¢ã€‚æ¯ä¸ªæŸ¥è¯¢åº”ç®€æ´ã€ç‹¬ç«‹ï¼Œé€‚åˆç”¨äºå‘é‡æ£€ç´¢ã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            åŸå§‹é—®é¢˜ï¼š{question}\n",
    "            \n",
    "            è¯·ç”Ÿæˆ3ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Šï¼š<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=500,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([query_prompt], sampling_params)\n",
    "        response = outputs[0].outputs[0].text.strip()\n",
    "        \n",
    "        # è§£æç”Ÿæˆçš„æŸ¥è¯¢\n",
    "        queries = []\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 5:\n",
    "                # ç§»é™¤å¯èƒ½çš„ç¼–å·æ ‡è®°\n",
    "                if line[0].isdigit() and (line[1] == '.' or line[1] == 'ã€'):\n",
    "                    query = line[2:].strip()\n",
    "                else:\n",
    "                    query = line\n",
    "                queries.append(query)\n",
    "        \n",
    "        # ç¡®ä¿åŒ…å«åŸå§‹é—®é¢˜å¹¶å»é‡\n",
    "        all_queries = [question] + queries\n",
    "        unique_queries = []\n",
    "        seen = set()\n",
    "        for q in all_queries:\n",
    "            if q not in seen:\n",
    "                seen.add(q)\n",
    "                unique_queries.append(q)\n",
    "        \n",
    "        return unique_queries[:4]  # æœ€å¤šè¿”å›4ä¸ªæŸ¥è¯¢\n",
    "    \n",
    "    def _retrieve_documents(self, queries: List[str]) -> List:\n",
    "        \"\"\"æ£€ç´¢æ‰€æœ‰æŸ¥è¯¢çš„æ–‡æ¡£\"\"\"\n",
    "        all_docs = []\n",
    "        for query in queries:\n",
    "            try:\n",
    "                docs = self.retriever.get_relevant_documents(query)\n",
    "                all_docs.append(docs)\n",
    "                print(f\"   ğŸ” '{query}': æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ æ£€ç´¢å¤±è´¥ '{query}': {e}\")\n",
    "                all_docs.append([])\n",
    "        return all_docs\n",
    "    \n",
    "    def _process_documents(self, all_docs: List[List]) -> List:\n",
    "        \"\"\"å¤„ç†å¹¶å»é‡æ–‡æ¡£\"\"\"\n",
    "        unique_docs = get_unique_documents(all_docs)\n",
    "        return unique_docs[:10]  # è¿”å›top-10\n",
    "    \n",
    "    def _clean_context(self, text: str) -> str:\n",
    "        \"\"\"æ¸…ç†ä¸Šä¸‹æ–‡å†…å®¹\"\"\"\n",
    "        import re\n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "        \n",
    "        # æ¸…ç†æ ¼å¼\n",
    "        cleaned = re.sub(r'(?<=[^\\s])\\n(?=[^\\s])', '', text)  # åˆå¹¶é”™è¯¯åˆ†å‰²çš„æ–‡å­—\n",
    "        cleaned = re.sub(r'\\n\\s+\\n', '\\n\\n', cleaned)  # å‹ç¼©ç©ºè¡Œ\n",
    "        cleaned = re.sub(r'\\n\\s+', '\\n', cleaned)  # æ¸…ç†è¡Œå†…ç©ºæ ¼\n",
    "        cleaned = re.sub(r'[ \\t]{2,}', ' ', cleaned)  # æ¸…ç†è¿ç»­ç©ºæ ¼\n",
    "        cleaned = cleaned.strip()  # æ¸…ç†é¦–å°¾ç©ºæ ¼\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def _build_answer_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"æ„å»ºChatMLæ ¼å¼çš„ç­”æ¡ˆç”Ÿæˆæç¤ºè¯\"\"\"\n",
    "        cleaned_context = self._clean_context(context)\n",
    "        \n",
    "        return f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n",
    "            \n",
    "            {cleaned_context}\n",
    "            \n",
    "            è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n",
    "            1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å›ç­”\n",
    "            2. å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”\"æˆ‘ä¸çŸ¥é“\"\n",
    "            3. ä¿æŒå›ç­”å‡†ç¡®ã€ç®€æ´\n",
    "            4. ä¸è¦ç¼–é€ ä¿¡æ¯<|im_end|>\n",
    "            <|im_start|>user\n",
    "            {question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "    \n",
    "    def _generate_answer(self, prompt: str, max_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "        \"\"\"ç”Ÿæˆç­”æ¡ˆ\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        return outputs[0].outputs[0].text.strip()\n",
    "    \n",
    "    def query(self, question: str, max_tokens: int = 512, temperature: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰§è¡ŒMulti-Query RAGæŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¯ åŸå§‹é—®é¢˜: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Step 1: ç”ŸæˆæŸ¥è¯¢å˜ä½“\n",
    "        queries = self._generate_query_variants(question)\n",
    "        print(f\"ğŸ“ ç”Ÿæˆäº† {len(queries)} ä¸ªæŸ¥è¯¢:\")\n",
    "        for i, q in enumerate(queries, 1):\n",
    "            print(f\"   {i}. {q}\")\n",
    "        \n",
    "        # Step 2: æ£€ç´¢æ–‡æ¡£\n",
    "        all_docs = self._retrieve_documents(queries)\n",
    "        total_docs_before_dedup = sum(len(docs) for docs in all_docs)\n",
    "        print(f\"ğŸ“š æ£€ç´¢åˆ° {total_docs_before_dedup} ä¸ªæ–‡æ¡£ï¼ˆå«é‡å¤ï¼‰\")\n",
    "        \n",
    "        # Step 3: å»é‡\n",
    "        unique_docs = self._process_documents(all_docs)\n",
    "        print(f\"âœ¨ å»é‡åå‰©ä½™ {len(unique_docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        # Step 4: æ„å»ºä¸Šä¸‹æ–‡\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in unique_docs])\n",
    "        \n",
    "        # Step 5: ç”Ÿæˆç­”æ¡ˆ\n",
    "        prompt = self._build_answer_prompt(question, context)\n",
    "        answer = self._generate_answer(prompt, max_tokens, temperature)\n",
    "        \n",
    "        # è¿”å›ç»“æœ\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"queries\": queries,\n",
    "            \"num_docs\": len(unique_docs),\n",
    "            \"total_docs_before_dedup\": total_docs_before_dedup,\n",
    "            \"context_preview\": context[:200] + \"...\" if len(context) > 200 else context\n",
    "        }\n",
    "    \n",
    "    def batch_query(self, questions: List[str], **kwargs) -> List[Dict]:\n",
    "        \"\"\"æ‰¹é‡æŸ¥è¯¢\"\"\"\n",
    "        results = []\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\nğŸ” å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜...\")\n",
    "            result = self.query(question, **kwargs)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "# === ä½¿ç”¨ç¤ºä¾‹ ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ ChatMLæ ¼å¼çš„Multi-Query RAG\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. åˆ›å»ºå®ä¾‹\n",
    "    multi_rag = MultiQueryRAG(vectorstore, llm)\n",
    "    \n",
    "    # 2. æ‰§è¡ŒæŸ¥è¯¢\n",
    "    result = multi_rag.query(\"ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ\")\n",
    "    \n",
    "    # 3. æŸ¥çœ‹ç»“æœ\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ’¡ ç­”æ¡ˆ:\", result[\"answer\"])\n",
    "    print(\"ğŸ“ ä½¿ç”¨çš„æŸ¥è¯¢:\", result[\"queries\"])\n",
    "    print(\"ğŸ“š å‚è€ƒæ–‡æ¡£æ•°:\", result[\"num_docs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383cb12-7219-4592-a315-c6ba675a1c51",
   "metadata": {},
   "source": [
    "Multi-Queryçš„ä¼˜ç¼ºç‚¹:\n",
    "\n",
    "ä¼˜ç‚¹ï¼šæé«˜å¬å›ç‡ï¼ˆæ‰¾åˆ°æ›´å¤šç›¸å…³æ–‡æ¡£ï¼‰ï¼›è¦†ç›–ä¸åŒè§’åº¦å’Œè¡¨è¾¾æ–¹å¼ï¼›å¯¹ç”¨æˆ·è¡¨è¾¾ä¸æ¸…çš„æŸ¥è¯¢ç‰¹åˆ«æœ‰æ•ˆï¼›å®ç°ç›¸å¯¹ç®€å•\n",
    "\n",
    "ç¼ºç‚¹ï¼šå¢åŠ æ£€ç´¢æˆæœ¬ï¼ˆå¤šæ¬¡æŸ¥è¯¢ï¼‰ï¼›å¯èƒ½å¼•å…¥å™ªå£°ï¼ˆä¸ç›¸å…³çš„å˜ä½“ï¼‰ï¼›éœ€è¦é¢å¤–çš„LLMè°ƒç”¨ï¼›å»é‡é€»è¾‘å¯èƒ½è¿‡æ»¤æ‰æœ‰ä»·å€¼çš„æ–‡æ¡£\n",
    "\n",
    "ä¼˜åŒ–æŠ€å·§ï¼š\n",
    "\n",
    "1. é™åˆ¶æŸ¥è¯¢æ•°é‡\n",
    "\n",
    "```python\n",
    "return unique_queries[:4]  # æœ€å¤šè¿”å›4ä¸ªæŸ¥è¯¢\n",
    "```\n",
    "\n",
    "2. ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤ç”Ÿæˆ\n",
    "\n",
    "```python\n",
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=100)\n",
    "def cached_query_generation(question: str) -> tuple:\n",
    "    queries = query_generator.invoke({\"question\": question})\n",
    "    return tuple(queries)  # è¿”å›å…ƒç»„ä»¥æ”¯æŒç¼“å­˜\n",
    "```\n",
    "\n",
    "3. å¼‚æ­¥å¹¶è¡Œæ£€ç´¢\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "async def async_retrieve_all(queries: List[str], retriever):\n",
    "    \"\"\"å¼‚æ­¥å¹¶è¡Œæ£€ç´¢\"\"\"\n",
    "    tasks = [retriever.aget_relevant_documents(q) for q in queries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a9a9a-323f-4ad0-a9d9-f4770260853d",
   "metadata": {},
   "source": [
    "## Part 2: RAG-Fusion - èåˆå¼æ£€ç´¢\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šRAG-Fusionç»“åˆäº†Multi-Queryå’Œå€’æ•°æ’åºèåˆï¼ˆReciprocal Rank Fusion, RRFï¼‰ï¼Œä¸ä»…ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢ï¼Œè¿˜æ™ºèƒ½åœ°åˆå¹¶å’Œé‡æ’åºç»“æœã€‚\n",
    "\n",
    "ä»€ä¹ˆæ˜¯å€’æ•°æ’åºèåˆï¼Ÿ\n",
    "\n",
    "RRFæ˜¯ä¸€ç§æ’åºèåˆç®—æ³•ï¼Œç»™äºˆæ’åé å‰çš„æ–‡æ¡£æ›´é«˜çš„åˆ†æ•°ï¼š\n",
    "\n",
    "```python\n",
    "å…¬å¼: RRF_score(doc) = Î£ 1 / (k + rank(doc))\n",
    "\n",
    "å…¶ä¸­:\n",
    "- k = å¸¸æ•°ï¼ˆé€šå¸¸ä¸º60ï¼‰\n",
    "- rank(doc) = æ–‡æ¡£åœ¨æŸä¸ªæŸ¥è¯¢ç»“æœä¸­çš„æ’å\n",
    "- Î£ = å¯¹æ‰€æœ‰æŸ¥è¯¢ç»“æœæ±‚å’Œ\n",
    "```\n",
    "\n",
    "ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "# æŸ¥è¯¢1çš„ç»“æœæ’åº:\n",
    "æŸ¥è¯¢1: \"ä»€ä¹ˆæ˜¯Agent?\"\n",
    "  1. æ–‡æ¡£A (æ’å1)\n",
    "  2. æ–‡æ¡£B (æ’å2)\n",
    "  3. æ–‡æ¡£C (æ’å3)\n",
    "\n",
    "# æŸ¥è¯¢2çš„ç»“æœæ’åº:\n",
    "æŸ¥è¯¢2: \"AI Agentçš„å®šä¹‰\"\n",
    "  1. æ–‡æ¡£B (æ’å1)  â† åœ¨è¿™ä¸ªæŸ¥è¯¢ä¸­æ’åæ›´é«˜\n",
    "  2. æ–‡æ¡£D (æ’å2)\n",
    "  3. æ–‡æ¡£A (æ’å3)\n",
    "\n",
    "# RRFèåˆè®¡ç®— (k=60):\n",
    "æ–‡æ¡£A: 1/(60+1) + 1/(60+3) = 0.0164 + 0.0159 = 0.0323\n",
    "æ–‡æ¡£B: 1/(60+2) + 1/(60+1) = 0.0161 + 0.0164 = 0.0325 â† æœ€é«˜åˆ†\n",
    "æ–‡æ¡£C: 1/(60+3) + 0         = 0.0159\n",
    "æ–‡æ¡£D: 0         + 1/(60+2) = 0.0161\n",
    "\n",
    "æœ€ç»ˆæ’åº: B > A > D > C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a5370a-e45d-4f69-82c7-91161c72c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from vllm import SamplingParams\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    results: List[List],\n",
    "    k: int = 60\n",
    ") -> List[Tuple[any, float]]:\n",
    "    \"\"\"\n",
    "    å®ç°å€’æ•°æ’åºèåˆ\n",
    "    \n",
    "    Args:\n",
    "        results: å¤šä¸ªæŸ¥è¯¢çš„ç»“æœåˆ—è¡¨\n",
    "        k: RRFå¸¸æ•°\n",
    "    \n",
    "    Returns:\n",
    "        æ’åºåçš„(æ–‡æ¡£, åˆ†æ•°)åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # å­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„èåˆåˆ†æ•°\n",
    "    fusion_scores = {}\n",
    "    \n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # ä½¿ç”¨æ–‡æ¡£å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†\n",
    "            doc_id = doc.page_content\n",
    "            \n",
    "            # è®¡ç®—RRFåˆ†æ•°\n",
    "            if doc_id not in fusion_scores:\n",
    "                fusion_scores[doc_id] = {\n",
    "                    'doc': doc,\n",
    "                    'score': 0\n",
    "                }\n",
    "            \n",
    "            # ç´¯åŠ åˆ†æ•°\n",
    "            fusion_scores[doc_id]['score'] += 1 / (k + rank + 1)\n",
    "    \n",
    "    # æŒ‰åˆ†æ•°æ’åº\n",
    "    sorted_docs = sorted(\n",
    "        fusion_scores.values(),\n",
    "        key=lambda x: x['score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return [(item['doc'], item['score']) for item in sorted_docs]\n",
    "\n",
    "\n",
    "class RAGFusion:\n",
    "    \"\"\"ä½¿ç”¨ChatMLæ ¼å¼çš„RAG-Fusionç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, k: int = 60):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        self.llm = llm\n",
    "        self.k = k\n",
    "        \n",
    "    def generate_queries(self, question: str, n: int = 3) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨ChatMLæ ¼å¼ç”ŸæˆæŸ¥è¯¢å˜ä½“\"\"\"\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿å°†ç”¨æˆ·çš„é—®é¢˜æ”¹å†™æˆå¤šä¸ªè¯­ä¹‰ç›¸åŒä½†è¡¨è¾¾ä¸åŒçš„æœç´¢æŸ¥è¯¢ã€‚\n",
    "            è¯·ç”Ÿæˆ{n}ä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢å˜ä½“ã€‚\n",
    "            \n",
    "            è¦æ±‚ï¼š\n",
    "            1. è¯­ä¹‰ç›¸å…³ä½†è¡¨è¾¾ä¸åŒ\n",
    "            2. è¦†ç›–ä¸åŒè§’åº¦\n",
    "            3. æ¯è¡Œä¸€ä¸ªæŸ¥è¯¢<|im_end|>\n",
    "            <|im_start|>user\n",
    "            åŸå§‹é—®é¢˜ï¼š{question}\n",
    "            \n",
    "            è¯·ç”Ÿæˆ{n}ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼š<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        response = outputs[0].outputs[0].text.strip()\n",
    "        \n",
    "        # è§£æç”Ÿæˆçš„æŸ¥è¯¢\n",
    "        queries = []\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æŸ¥è¯¢\n",
    "                # ç§»é™¤å¯èƒ½çš„ç¼–å·æ ‡è®°\n",
    "                if line[0].isdigit() and (line[1] == '.' or line[1] == 'ã€'):\n",
    "                    query = line[2:].strip()\n",
    "                else:\n",
    "                    query = line\n",
    "                queries.append(query)\n",
    "        \n",
    "        # ç¡®ä¿åŒ…å«åŸå§‹é—®é¢˜å¹¶å»é‡\n",
    "        all_queries = [question] + queries\n",
    "        unique_queries = []\n",
    "        seen = set()\n",
    "        for q in all_queries:\n",
    "            if q not in seen:\n",
    "                seen.add(q)\n",
    "                unique_queries.append(q)\n",
    "        \n",
    "        return unique_queries[:n+1]  # åŒ…å«åŸå§‹é—®é¢˜\n",
    "    \n",
    "    def retrieve_and_fuse(self, queries: List[str]) -> List[Tuple[any, float]]:\n",
    "        \"\"\"æ£€ç´¢å¹¶èåˆç»“æœ\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"ğŸ” æ‰§è¡Œ {len(queries)} ä¸ªæŸ¥è¯¢...\")\n",
    "        for i, query in enumerate(queries, 1):\n",
    "            docs = self.retriever.get_relevant_documents(query)\n",
    "            all_results.append(docs)\n",
    "            print(f\"   æŸ¥è¯¢{i}: '{query}' -> æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        # RRFèåˆ\n",
    "        fused_results = reciprocal_rank_fusion(all_results, k=self.k)\n",
    "        print(f\"âœ¨ èåˆåå…± {len(fused_results)} ä¸ªå”¯ä¸€æ–‡æ¡£\")\n",
    "        \n",
    "        return fused_results\n",
    "    \n",
    "    def _build_answer_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"æ„å»ºChatMLæ ¼å¼çš„ç­”æ¡ˆç”Ÿæˆæç¤ºè¯\"\"\"\n",
    "        return f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æŒ‰ç›¸å…³æ€§æ’åºçš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚\n",
    "            \n",
    "            {context}\n",
    "            \n",
    "            è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n",
    "            1. åªä½¿ç”¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯å›ç­”\n",
    "            2. å¦‚æœæ–‡æ¡£ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”\"æˆ‘ä¸çŸ¥é“\"\n",
    "            3. ä¿æŒå›ç­”å‡†ç¡®ã€ç®€æ´<|im_end|>\n",
    "            <|im_start|>user\n",
    "            {question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "    \n",
    "    def _generate_answer(self, prompt: str, max_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "        \"\"\"ç”Ÿæˆç­”æ¡ˆ\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        return outputs[0].outputs[0].text.strip()\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5, max_tokens: int = 512, temperature: float = 0.3) -> dict:\n",
    "        \"\"\"æ‰§è¡ŒRAG-FusionæŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¯ åŸå§‹é—®é¢˜: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 1. ç”ŸæˆæŸ¥è¯¢\n",
    "        queries = self.generate_queries(question)\n",
    "        print(f\"ğŸ“ ç”Ÿæˆäº† {len(queries)} ä¸ªæŸ¥è¯¢:\")\n",
    "        for i, q in enumerate(queries, 1):\n",
    "            print(f\"   {i}. {q}\")\n",
    "        \n",
    "        # 2. æ£€ç´¢å¹¶èåˆ\n",
    "        fused_docs = self.retrieve_and_fuse(queries)\n",
    "        \n",
    "        # 3. é€‰æ‹©top-k\n",
    "        top_docs = fused_docs[:top_k]\n",
    "        print(f\"ğŸ† é€‰æ‹©Top-{len(top_docs)} æ–‡æ¡£:\")\n",
    "        for i, (doc, score) in enumerate(top_docs, 1):\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"   {i}. [åˆ†æ•°: {score:.4f}] {preview}\")\n",
    "        \n",
    "        # 4. ç”Ÿæˆç­”æ¡ˆ\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[æ–‡æ¡£{i+1} åˆ†æ•°: {score:.4f}]\\n{doc.page_content}\" \n",
    "            for i, (doc, score) in enumerate(top_docs)\n",
    "        ])\n",
    "        \n",
    "        prompt = self._build_answer_prompt(question, context)\n",
    "        answer = self._generate_answer(prompt, max_tokens, temperature)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"generated_queries\": queries,\n",
    "            \"num_docs\": len(top_docs),\n",
    "            \"top_scores\": [score for _, score in top_docs],\n",
    "            \"answer\": answer,\n",
    "            \"context_preview\": context[:200] + \"...\" if len(context) > 200 else context\n",
    "        }\n",
    "    \n",
    "    def batch_query(self, questions: List[str], **kwargs) -> List[dict]:\n",
    "        \"\"\"æ‰¹é‡æŸ¥è¯¢\"\"\"\n",
    "        results = []\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\nğŸ” å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜...\")\n",
    "            result = self.query(question, **kwargs)\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e505955-2b80-4cb2-9689-a7ee7e33de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ RAG-Fusionç³»ç»Ÿ\n",
      "============================================================\n",
      "ğŸ¯ åŸå§‹é—®é¢˜: ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç”Ÿæˆäº† 4 ä¸ªæŸ¥è¯¢:\n",
      "   1. ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ\n",
      "   2. æœºæ¢°è‡‚æ˜¯ä»€ä¹ˆï¼Ÿå®ƒçš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "   3. æœºæ¢°è‡‚æœ‰å“ªäº›é‡è¦çš„ç‰¹ç‚¹å’ŒåŠŸèƒ½ï¼Ÿ\n",
      "   4. æœºæ¢°è‡‚çš„è¿ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ\n",
      "ğŸ” æ‰§è¡Œ 4 ä¸ªæŸ¥è¯¢...\n",
      "   æŸ¥è¯¢1: 'ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ' -> æ£€ç´¢åˆ° 5 ä¸ªæ–‡æ¡£\n",
      "   æŸ¥è¯¢2: 'æœºæ¢°è‡‚æ˜¯ä»€ä¹ˆï¼Ÿå®ƒçš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ' -> æ£€ç´¢åˆ° 5 ä¸ªæ–‡æ¡£\n",
      "   æŸ¥è¯¢3: 'æœºæ¢°è‡‚æœ‰å“ªäº›é‡è¦çš„ç‰¹ç‚¹å’ŒåŠŸèƒ½ï¼Ÿ' -> æ£€ç´¢åˆ° 5 ä¸ªæ–‡æ¡£\n",
      "   æŸ¥è¯¢4: 'æœºæ¢°è‡‚çš„è¿ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ' -> æ£€ç´¢åˆ° 5 ä¸ªæ–‡æ¡£\n",
      "âœ¨ èåˆåå…± 13 ä¸ªå”¯ä¸€æ–‡æ¡£\n",
      "ğŸ† é€‰æ‹©Top-5 æ–‡æ¡£:\n",
      "   1. [åˆ†æ•°: 0.0640] å°½\n",
      "ç®¡ï¼Œ\n",
      "æœº\n",
      "æ¢°î—’î—\n",
      "è‡‚çš„\n",
      "è¿\n",
      "åŠ¨\n",
      "é€Ÿ\n",
      "åº¦\n",
      "å¯\n",
      "ä»¥è®¾å®šä¸º\n",
      "åŒ€\n",
      "é€Ÿ\n",
      "è¿\n",
      "åŠ¨ï¼Œ é¿\n",
      "å…\n",
      "åŠ \n",
      "é€Ÿ\n",
      "åº¦å˜\n",
      "åŒ–\n",
      "è€Œ\n",
      "é€ \n",
      "æˆ\n",
      "å¯¹ç›®\n",
      "æ ‡\n",
      "å’Œ\n",
      "å¤¹\n",
      "çˆªä¹‹é—´åŠ¨î—’î—\n",
      "åŠ›\n",
      "å­¦\n",
      "å¹³\n",
      "è¡¡\n",
      "å…³ç³»çš„å½±å“\n",
      "ã€‚\n",
      "ä½†\n",
      "æ˜¯\n",
      "ï¼Œ æœºæ¢°è‡‚çš„\n",
      "   2. [åˆ†æ•°: 0.0489] åŠŸèƒ½ï¼Œå…¶ä½œç”¨åŠ›çš„èŒƒå›´ä¸º 50Nï¼Œç²¾ç¡®åº¦ä¸º 2.5Nï¼Œå‡†ç¡®åº¦ä¸º 4Nï¼Œæ‰­çŸ©èŒƒå›´ä¸º 10NÂ·mï¼Œ\n",
      "ç²¾ç¡®åº¦ä¸º 0.04NÂ·mï¼Œå‡†ç¡®åº¦ä¸º 0.3Nã€‚æœºæ¢°è‡‚æœ¬ä½“è‡ªé‡ 20.6kgï¼Œé‡‡ç”¨ 220Väº¤æµä¾›ç”µï¼Œ\n",
      "   3. [åˆ†æ•°: 0.0318] æœºæ¢°è‡‚ \n",
      "å¯é‡å¤æ€§  +-0.03mm  \n",
      "æœ‰æ•ˆè´Ÿè½½  5 åƒå…‹/11 ç£… \n",
      "å·¥ä½œåŠå¾„  850mm/33.5 è‹±å¯¸ \n",
      "è‡ªç”±åº¦ 6 ä¸ªæ—‹è½¬å…³èŠ‚\n",
      "   4. [åˆ†æ•°: 0.0315] Â±180Â°/sï¼Œæœºæ¢°è‡‚æœ«ç«¯é€Ÿåº¦è§†å„å…³èŠ‚è½½è·ä¸å®é™…é€Ÿåº¦è€Œå®šã€‚æœºæ¢°è‡‚æœ¬ä½“çš„ä½å§¿å¯é‡å¤æ€§\n",
      "ç²¾åº¦ä¸º Â±0.03mmã€‚ä¸å…¶å®ƒå‹å·æœºæ¢°è‡‚ä¸åŒçš„æ˜¯ï¼ŒUR5eæœºæ¢°è‡‚çš„å·¥å…·æ³•å…°å¸¦æœ‰åŠ›æ„Ÿåº”\n",
      "   5. [åˆ†æ•°: 0.0161] æœºè®°å½•è¯¥æµ®é›•é£æ ¼çš„æ¥è§¦è¡¨é¢å›¾åƒï¼Œå¹¶ä½¿ç”¨å…‰åº¦ç«‹ä½“ç®—æ³•\n",
      "[55] é‡å»ºæ¥è§¦è¡¨é¢çš„æ·±åº¦å›¾ã€‚\n",
      "è¯¥ä¼ æ„Ÿå™¨é€šè¿‡å‡èƒ¶(gel)å½¢å˜å›¾åƒè·å–è§¦è§‰ä¿¡æ¯ï¼Œå› æ­¤è¢«ç§°ä¸ºGelsightã€‚ä¸ºæœºå™¨äººæ‰‹æŒ‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ç”Ÿæˆçš„æŸ¥è¯¢:\n",
      "  - ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ\n",
      "  - æœºæ¢°è‡‚æ˜¯ä»€ä¹ˆï¼Ÿå®ƒçš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "  - æœºæ¢°è‡‚æœ‰å“ªäº›é‡è¦çš„ç‰¹ç‚¹å’ŒåŠŸèƒ½ï¼Ÿ\n",
      "  - æœºæ¢°è‡‚çš„è¿ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ\n",
      "\n",
      "Top-5 æ–‡æ¡£åˆ†æ•°:\n",
      "  1. 0.0640\n",
      "  2. 0.0489\n",
      "  3. 0.0318\n",
      "  4. 0.0315\n",
      "  5. 0.0161\n",
      "\n",
      "ç­”æ¡ˆ:\n",
      "æœºæ¢°è‡‚æ˜¯ä¸€ç§èƒ½å¤ŸæŒ‰ç…§é¢„å®šç¨‹åºè‡ªåŠ¨å®Œæˆç‰¹å®šä»»åŠ¡çš„æœºå™¨äººæ‰‹è‡‚ã€‚å®ƒé€šå¸¸ç”±å¤šä¸ªå…³èŠ‚å’Œæ‰§è¡Œå™¨ç»„æˆï¼Œå¯ä»¥å®ç°ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶å’ŒæŠ“å–æ“ä½œã€‚æœºæ¢°è‡‚çš„å…³é”®èƒ½åŠ›åŒ…æ‹¬ï¼š\n",
      "             1. è¿åŠ¨æ§åˆ¶ï¼šæœºæ¢°è‡‚å¯ä»¥å®ç°ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ï¼ŒåŒ…æ‹¬é€Ÿåº¦ã€ä½ç½®å’Œæ–¹å‘æ§åˆ¶ã€‚\n",
      "             2. æŠ“å–æ“ä½œï¼šæœºæ¢°è‡‚å¯ä»¥å®ç°ç²¾ç¡®çš„æŠ“å–æ“ä½œï¼ŒåŒ…æ‹¬æŠ“å–ç‰©ä½“å’Œé‡Šæ”¾ç‰©ä½“ã€‚\n",
      "             3. è‡ªåŠ¨åŒ–æ“ä½œï¼šæœºæ¢°è‡‚å¯ä»¥å®ç°è‡ªåŠ¨åŒ–æ“ä½œï¼ŒåŒ…æ‹¬è‡ªåŠ¨è£…é…ã€æ¬è¿å’Œæ‹†å¸ç­‰ã€‚\n",
      "             4. ç²¾ç¡®åº¦ï¼šæœºæ¢°è‡‚å¯ä»¥å®ç°é«˜ç²¾åº¦çš„æ“ä½œï¼ŒåŒ…æ‹¬ç²¾ç¡®çš„å®šä½å’Œç²¾ç¡®çš„æŠ“å–ã€‚\n",
      "             5. åŠŸèƒ½ï¼šæœºæ¢°è‡‚å¯ä»¥å®ç°å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ¬è¿ã€è£…é…ã€æ‹†å¸ã€åˆ‡å‰²ã€ç„Šæ¥ã€å–·æ¶‚ç­‰ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ RAG-Fusionç³»ç»Ÿ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. åˆ›å»ºå®ä¾‹\n",
    "rag_fusion = RAGFusion(vectorstore, llm, k=60)\n",
    "\n",
    "# 2. æ‰§è¡ŒæŸ¥è¯¢\n",
    "result = rag_fusion.query(\"ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰å“ªäº›å…³é”®èƒ½åŠ›ï¼Ÿ\")\n",
    "\n",
    "# 3. æŸ¥çœ‹ç»“æœ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ç”Ÿæˆçš„æŸ¥è¯¢:\")\n",
    "for q in result[\"generated_queries\"]:\n",
    "    print(f\"  - {q}\")\n",
    "\n",
    "print(f\"\\nTop-{result['num_docs']} æ–‡æ¡£åˆ†æ•°:\")\n",
    "for i, score in enumerate(result[\"top_scores\"], 1):\n",
    "    print(f\"  {i}. {score:.4f}\")\n",
    "\n",
    "print(\"\\nç­”æ¡ˆ:\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f61a9ee-2d7b-4917-bab6-34156f2cca15",
   "metadata": {},
   "source": [
    "### RAG-Fusion vs Multi-Query\n",
    "\n",
    "| ç‰¹æ€§ | Multi-Query | RAG-Fusion | è¯´æ˜ |\n",
    "|:-----|:-----------:|:----------:|:-----|\n",
    "| æŸ¥è¯¢ç”Ÿæˆ | âœ… | âœ… | éƒ½æ”¯æŒç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“ |\n",
    "| å¹¶è¡Œæ£€ç´¢ | âœ… | âœ… | éƒ½æ”¯æŒå¹¶è¡Œæ£€ç´¢å¤šä¸ªæŸ¥è¯¢ |\n",
    "| æ™ºèƒ½æ’åº | âŒ | âœ… RRFç®—æ³• | RAG-Fusionä½¿ç”¨å€’æ•°æ’åºèåˆ |\n",
    "| ç»“æœè´¨é‡ | ä¸­ç­‰ | é«˜ | RRFæå‡æ£€ç´¢è´¨é‡ |\n",
    "| è®¡ç®—æˆæœ¬ | ä½ | ä¸­ç­‰ | RRFå¢åŠ è®¡ç®—å¼€é”€ |\n",
    "| é€‚ç”¨åœºæ™¯ | ä¸€èˆ¬æ£€ç´¢ | é«˜è´¨é‡éœ€æ±‚ | æ ¹æ®éœ€æ±‚é€‰æ‹© |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303aaab-45e1-4461-a714-80061c9fb037",
   "metadata": {},
   "source": [
    "## Part 3: Query Decomposition - æŸ¥è¯¢åˆ†è§£\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šå¯¹äºå¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜ï¼ŒQuery Decompositionå°†å…¶åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œåˆ†åˆ«å›ç­”åå†åˆæˆæœ€ç»ˆç­”æ¡ˆã€‚\n",
    "\n",
    "**ä¸¤ç§åˆ†è§£ç­–ç•¥ï¼š**\n",
    "\n",
    "1. é€’å½’åˆ†è§£ï¼ˆAnswer Recursivelyï¼‰\n",
    "\n",
    "```python\n",
    "å¤æ‚é—®é¢˜: \"æ¯”è¾ƒGPT-3å’ŒGPT-4åœ¨å¤šæ¨¡æ€èƒ½åŠ›ä¸Šçš„å·®å¼‚\"\n",
    "    â†“\n",
    "å­é—®é¢˜1: \"GPT-3æœ‰å“ªäº›èƒ½åŠ›ï¼Ÿ\"\n",
    "    â†“ æ£€ç´¢ + å›ç­”\n",
    "ç­”æ¡ˆ1: \"GPT-3ä¸»è¦æ˜¯æ–‡æœ¬æ¨¡å‹...\"\n",
    "    â†“\n",
    "å­é—®é¢˜2: \"GPT-4æœ‰å“ªäº›æ–°èƒ½åŠ›ï¼Ÿ\" (åŸºäºç­”æ¡ˆ1)\n",
    "    â†“ æ£€ç´¢ + å›ç­”\n",
    "ç­”æ¡ˆ2: \"GPT-4å¢åŠ äº†å›¾åƒç†è§£...\"\n",
    "    â†“\n",
    "ç»¼åˆç­”æ¡ˆ: \"GPT-3ä»…æ”¯æŒæ–‡æœ¬ï¼Œè€ŒGPT-4...\"\n",
    "```\n",
    "\n",
    "2. å¹¶è¡Œåˆ†è§£ï¼ˆAnswer Individuallyï¼‰\n",
    "\n",
    "```python\n",
    "å¤æ‚é—®é¢˜: \"æ¯”è¾ƒPythonå’ŒJavaScriptåœ¨Webå¼€å‘ä¸­çš„ä¼˜åŠ£\"\n",
    "    â†“\n",
    "å­é—®é¢˜1: \"Pythonåœ¨Webå¼€å‘ä¸­çš„ä¼˜åŠ¿\"\n",
    "å­é—®é¢˜2: \"JavaScriptåœ¨Webå¼€å‘ä¸­çš„ä¼˜åŠ¿\"\n",
    "å­é—®é¢˜3: \"Pythonåœ¨Webå¼€å‘ä¸­çš„åŠ£åŠ¿\"\n",
    "å­é—®é¢˜4: \"JavaScriptåœ¨Webå¼€å‘ä¸­çš„åŠ£åŠ¿\"\n",
    "    â†“\n",
    "å¹¶è¡Œæ£€ç´¢ + å›ç­”\n",
    "    â†“\n",
    "ç»¼åˆæ‰€æœ‰ç­”æ¡ˆ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbb959-caa3-4b70-8703-0c07d065f1a5",
   "metadata": {},
   "source": [
    "#### å…ˆå®ç°é€’å½’åˆ†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085ade40-158e-4f93-bae1-52302eaf6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from vllm import SamplingParams\n",
    "\n",
    "class RecursiveDecomposition:\n",
    "    \"\"\"ä½¿ç”¨ChatMLæ ¼å¼çš„é€’å½’æŸ¥è¯¢åˆ†è§£\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.retriever = vectorstore.as_retriever()\n",
    "        self.llm = llm\n",
    "        print(\"âœ… é€’å½’åˆ†è§£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "        \n",
    "    def decompose_query(self, question: str) -> List[str]:\n",
    "        \"\"\"ä½¿ç”¨ChatMLæ ¼å¼åˆ†è§£å¤æ‚æŸ¥è¯¢ä¸ºå­é—®é¢˜\"\"\"\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºç®€å•çš„å­é—®é¢˜ã€‚\n",
    "            \n",
    "            è¯·å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸º2-4ä¸ªç®€å•çš„å­é—®é¢˜ï¼š\n",
    "            \n",
    "            è¦æ±‚ï¼š\n",
    "            1. å­é—®é¢˜åº”è¯¥æŒ‰é€»è¾‘é¡ºåºæ’åˆ—\n",
    "            2. æ¯ä¸ªå­é—®é¢˜éƒ½åº”è¯¥æ˜¯ç‹¬ç«‹å¯å›ç­”çš„\n",
    "            3. æ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼Œä¸è¦ç¼–å·<|im_end|>\n",
    "            <|im_start|>user\n",
    "            å¤æ‚é—®é¢˜ï¼š{question}\n",
    "            \n",
    "            è¯·åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªï¼š<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=512,\n",
    "            temperature=0.5,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        response = outputs[0].outputs[0].text.strip()\n",
    "        \n",
    "        # è§£æå­é—®é¢˜\n",
    "        sub_questions = []\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„å­é—®é¢˜\n",
    "                # ç§»é™¤å¯èƒ½çš„ç¼–å·æ ‡è®°\n",
    "                if line[0].isdigit() and (line[1] == '.' or line[1] == 'ã€'):\n",
    "                    question = line[2:].strip()\n",
    "                else:\n",
    "                    question = line\n",
    "                sub_questions.append(question)\n",
    "        \n",
    "        # ç¡®ä¿æœ‰å­é—®é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ\n",
    "        if not sub_questions:\n",
    "            sub_questions = self._fallback_decomposition(question)\n",
    "        \n",
    "        return sub_questions[:4]  # æœ€å¤šè¿”å›4ä¸ªå­é—®é¢˜\n",
    "    \n",
    "    def _fallback_decomposition(self, question: str) -> List[str]:\n",
    "        \"\"\"å¤‡é€‰åˆ†è§£æ–¹æ¡ˆ\"\"\"\n",
    "        # ç®€å•çš„åŸºäºè§„åˆ™çš„åˆ†è§£\n",
    "        sub_questions = []\n",
    "        \n",
    "        if \"æ¯”è¾ƒ\" in question and \"å’Œ\" in question:\n",
    "            # æ¯”è¾ƒç±»é—®é¢˜\n",
    "            parts = question.split(\"æ¯”è¾ƒ\")[1].split(\"å’Œ\")\n",
    "            if len(parts) >= 2:\n",
    "                sub_questions.append(f\"{parts[0].strip()}çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "                sub_questions.append(f\"{parts[1].strip()}çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "                sub_questions.append(\"å®ƒä»¬çš„ä¸»è¦å·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "        \n",
    "        elif \"æ­¥éª¤\" in question or \"æµç¨‹\" in question:\n",
    "            # æ­¥éª¤ç±»é—®é¢˜\n",
    "            sub_questions.append(\"ç¬¬ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "            sub_questions.append(\"å…³é”®æ­¥éª¤æœ‰å“ªäº›ï¼Ÿ\")\n",
    "            sub_questions.append(\"æœ€ç»ˆç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "        \n",
    "        else:\n",
    "            # é€šç”¨åˆ†è§£\n",
    "            sub_questions.append(\"åŸºæœ¬æ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "            sub_questions.append(\"ä¸»è¦åº”ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ\")\n",
    "            sub_questions.append(\"ä¼˜åŠ¿å’Œå±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "        \n",
    "        return sub_questions\n",
    "    \n",
    "    def answer_sub_question(self, question: str, context: str = \"\") -> str:\n",
    "        \"\"\"ä½¿ç”¨ChatMLæ ¼å¼å›ç­”å•ä¸ªå­é—®é¢˜\"\"\"\n",
    "        # æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "        docs = self.retriever.get_relevant_documents(question)\n",
    "        doc_context = \"\\n\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "        \n",
    "        # æ„å»ºæç¤ºè¯\n",
    "        if context:\n",
    "            full_context = f\"å·²çŸ¥ä¿¡æ¯:\\n{context}\\n\\nç›¸å…³æ–‡æ¡£:\\n{doc_context}\"\n",
    "        else:\n",
    "            full_context = f\"ç›¸å…³æ–‡æ¡£:\\n{doc_context}\"\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç®€æ´åœ°å›ç­”é—®é¢˜ï¼š\n",
    "            \n",
    "            {full_context}\n",
    "            \n",
    "            è¯·éµå¾ªï¼š\n",
    "            1. åªä½¿ç”¨æä¾›çš„ä¿¡æ¯\n",
    "            2. ä¿æŒå›ç­”ç®€æ´å‡†ç¡®\n",
    "            3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜<|im_end|>\n",
    "            <|im_start|>user\n",
    "            é—®é¢˜ï¼š{question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=300,\n",
    "            temperature=0.3,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        return outputs[0].outputs[0].text.strip()\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰§è¡Œé€’å½’åˆ†è§£æŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¯ åŸå§‹é—®é¢˜: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 1. åˆ†è§£é—®é¢˜\n",
    "        sub_questions = self.decompose_query(question)\n",
    "        print(f\"ğŸ” åˆ†è§£ä¸º {len(sub_questions)} ä¸ªå­é—®é¢˜:\")\n",
    "        for i, sq in enumerate(sub_questions, 1):\n",
    "            print(f\"   {i}. {sq}\")\n",
    "        \n",
    "        # 2. é€’å½’å›ç­”\n",
    "        accumulated_context = \"\"\n",
    "        sub_answers = []\n",
    "        \n",
    "        print(\"\\nğŸ’¡ é€æ­¥å›ç­”å­é—®é¢˜:\")\n",
    "        for i, sq in enumerate(sub_questions, 1):\n",
    "            print(f\"\\n   å­é—®é¢˜{i}: {sq}\")\n",
    "            answer = self.answer_sub_question(sq, accumulated_context)\n",
    "            sub_answers.append(answer)\n",
    "            accumulated_context += f\"\\n\\né—®é¢˜{i}: {sq}\\nç­”æ¡ˆ: {answer}\"\n",
    "            print(f\"   ç­”æ¡ˆ: {answer[:100]}...\")\n",
    "        \n",
    "        # 3. ç»¼åˆæœ€ç»ˆç­”æ¡ˆ\n",
    "        final_prompt = f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œéœ€è¦åŸºäºå­é—®é¢˜çš„ç­”æ¡ˆæ¥ç»¼åˆå›ç­”åŸå§‹é—®é¢˜ã€‚\n",
    "            \n",
    "            åŸå§‹é—®é¢˜ï¼š{question}\n",
    "            \n",
    "            å­é—®é¢˜å’Œç­”æ¡ˆï¼š\n",
    "            {self._format_sub_qa(sub_questions, sub_answers)}\n",
    "            \n",
    "            è¯·åŸºäºä»¥ä¸Šå­é—®é¢˜çš„ç­”æ¡ˆï¼Œç»™å‡ºä¸€ä¸ªå®Œæ•´ã€è¿è´¯çš„ç»¼åˆç­”æ¡ˆã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            è¯·ç»¼åˆå›ç­”åŸå§‹é—®é¢˜ï¼š{question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=500,\n",
    "            temperature=0.3,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([final_prompt], sampling_params)\n",
    "        final_answer = outputs[0].outputs[0].text.strip()\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"sub_questions\": sub_questions,\n",
    "            \"sub_answers\": sub_answers,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"num_sub_questions\": len(sub_questions)\n",
    "        }\n",
    "    \n",
    "    def _format_sub_qa(self, questions: List[str], answers: List[str]) -> str:\n",
    "        \"\"\"æ ¼å¼åŒ–å­é—®é¢˜å’Œç­”æ¡ˆ\"\"\"\n",
    "        formatted = \"\"\n",
    "        for i, (q, a) in enumerate(zip(questions, answers), 1):\n",
    "            formatted += f\"å­é—®é¢˜{i}: {q}\\nç­”æ¡ˆ{i}: {a}\\n\\n\"\n",
    "        return formatted.strip()\n",
    "    \n",
    "    def batch_decompose(self, questions: List[str]) -> List[Dict]:\n",
    "        \"\"\"æ‰¹é‡åˆ†è§£æŸ¥è¯¢\"\"\"\n",
    "        results = []\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\nğŸ” å¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜...\")\n",
    "            result = self.query(question)\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37daa3a5-1b8f-4d9a-ab6f-c6caccd5199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ChatMLæ ¼å¼çš„é€’å½’åˆ†è§£ç³»ç»Ÿ\n",
      "============================================================\n",
      "âœ… é€’å½’åˆ†è§£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n",
      "ğŸ¯ åŸå§‹é—®é¢˜: æ¯”è¾ƒæŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹çš„ä¸»è¦å·®å¼‚\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” åˆ†è§£ä¸º 4 ä¸ªå­é—®é¢˜:\n",
      "   1. æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹éƒ½æ˜¯ç”¨äºæ£€æµ‹ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®çš„æ–¹æ³•ï¼Œå®ƒä»¬ä¹‹é—´æœ‰ä»€ä¹ˆä¸»è¦å·®å¼‚ï¼Ÿ\n",
      "   2. æŠ“å–æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„è¿åŠ¨ç‰¹å¾ï¼Œè¿™ä¸¤è€…ä¹‹é—´æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n",
      "   3. æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ\n",
      "   4. æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨åº”ç”¨åœºæ™¯ä¸Šæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n",
      "\n",
      "ğŸ’¡ é€æ­¥å›ç­”å­é—®é¢˜:\n",
      "\n",
      "   å­é—®é¢˜1: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹éƒ½æ˜¯ç”¨äºæ£€æµ‹ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®çš„æ–¹æ³•ï¼Œå®ƒä»¬ä¹‹é—´æœ‰ä»€ä¹ˆä¸»è¦å·®å¼‚ï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹éƒ½æ˜¯ç”¨äºæ£€æµ‹ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨ä¸€äº›ä¸»è¦å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®å˜åŒ–ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„è¿åŠ¨çŠ¶æ€ã€‚æŠ“å–æ£€æµ‹é€šå¸¸ä½¿ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ»‘åŠ¨æ£€...\n",
      "\n",
      "   å­é—®é¢˜2: æŠ“å–æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„è¿åŠ¨ç‰¹å¾ï¼Œè¿™ä¸¤è€…ä¹‹é—´æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„è¿åŠ¨ç‰¹å¾ã€‚æŠ“å–æ£€æµ‹é€šå¸¸ä½¿ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™å¯ä»¥ä½¿ç”¨è§¦è§‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒæŠ“å–æ£€æµ‹é€šå¸¸éœ€è¦ç²¾ç¡®çš„ç‰©ä½“å®šä½ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ã€‚...\n",
      "\n",
      "   å­é—®é¢˜3: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®å˜åŒ–ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„è¿åŠ¨çŠ¶æ€ã€‚æŠ“å–æ£€æµ‹é€šå¸¸ä½¿ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™å¯ä»¥ä½¿ç”¨è§¦è§‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒæŠ“å–æ£€æµ‹...\n",
      "\n",
      "   å­é—®é¢˜4: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨åº”ç”¨åœºæ™¯ä¸Šæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨åº”ç”¨åœºæ™¯ä¸Šå­˜åœ¨ä¸€äº›ä¸»è¦å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦åº”ç”¨äºç‰©ä½“å®šä½å’ŒæŠ“å–ï¼Œä¾‹å¦‚æœºå™¨äººæŠ“å–ã€æ— äººæœºæŠ“å–ç­‰ã€‚è€Œæ»‘åŠ¨æ£€æµ‹åˆ™ä¸»è¦åº”ç”¨äºç‰©ä½“è¿åŠ¨çŠ¶æ€çš„æ£€æµ‹ï¼Œä¾‹å¦‚ç‰©ä½“æ»‘åŠ¨æ£€æµ‹ã€ç‰©ä½“æ»‘åŠ¨è½¨è¿¹æ£€æµ‹ç­‰ã€‚...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ å­é—®é¢˜åˆ†è§£:\n",
      "1. é—®é¢˜: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹éƒ½æ˜¯ç”¨äºæ£€æµ‹ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®çš„æ–¹æ³•ï¼Œå®ƒä»¬ä¹‹é—´æœ‰ä»€ä¹ˆä¸»è¦å·®å¼‚ï¼Ÿ\n",
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹éƒ½æ˜¯ç”¨äºæ£€æµ‹ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨ä¸€äº›ä¸»è¦å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®å˜åŒ–ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„è¿åŠ¨çŠ¶æ€ã€‚æŠ“å–æ£€æµ‹é€šå¸¸ä½¿ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ»‘åŠ¨æ£€...\n",
      "\n",
      "2. é—®é¢˜: æŠ“å–æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„è¿åŠ¨ç‰¹å¾ï¼Œè¿™ä¸¤è€…ä¹‹é—´æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n",
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹ä¸»è¦ä¾èµ–äºç‰©ä½“çš„è¿åŠ¨ç‰¹å¾ã€‚æŠ“å–æ£€æµ‹é€šå¸¸ä½¿ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™å¯ä»¥ä½¿ç”¨è§¦è§‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒæŠ“å–æ£€æµ‹é€šå¸¸éœ€è¦ç²¾ç¡®çš„ç‰©ä½“å®šä½ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ã€‚...\n",
      "\n",
      "3. é—®é¢˜: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ\n",
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®å˜åŒ–ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„è¿åŠ¨çŠ¶æ€ã€‚æŠ“å–æ£€æµ‹é€šå¸¸ä½¿ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™å¯ä»¥ä½¿ç”¨è§¦è§‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒæŠ“å–æ£€æµ‹...\n",
      "\n",
      "4. é—®é¢˜: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨åº”ç”¨åœºæ™¯ä¸Šæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ\n",
      "   ç­”æ¡ˆ: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨åº”ç”¨åœºæ™¯ä¸Šå­˜åœ¨ä¸€äº›ä¸»è¦å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦åº”ç”¨äºç‰©ä½“å®šä½å’ŒæŠ“å–ï¼Œä¾‹å¦‚æœºå™¨äººæŠ“å–ã€æ— äººæœºæŠ“å–ç­‰ã€‚è€Œæ»‘åŠ¨æ£€æµ‹åˆ™ä¸»è¦åº”ç”¨äºç‰©ä½“è¿åŠ¨çŠ¶æ€çš„æ£€æµ‹ï¼Œä¾‹å¦‚ç‰©ä½“æ»‘åŠ¨æ£€æµ‹ã€ç‰©ä½“æ»‘åŠ¨è½¨è¿¹æ£€æµ‹ç­‰ã€‚...\n",
      "\n",
      "ğŸ’¡ æœ€ç»ˆç»¼åˆç­”æ¡ˆ:\n",
      "æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹éƒ½æ˜¯ç”¨äºæ£€æµ‹ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨ä¸€äº›ä¸»è¦å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®å˜åŒ–ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“åœ¨å›¾åƒä¸­çš„è¿åŠ¨çŠ¶æ€ã€‚æŠ“å–æ£€æµ‹é€šå¸¸ä½¿ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™å¯ä»¥ä½¿ç”¨è§¦è§‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒæŠ“å–æ£€æµ‹é€šå¸¸éœ€è¦ç²¾ç¡®çš„ç‰©ä½“å®šä½ï¼Œè€Œæ»‘åŠ¨æ£€æµ‹åˆ™æ›´å…³æ³¨ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹ã€‚æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚æŠ“å–æ£€æµ‹ä¸»è¦åº”ç”¨äºç‰©ä½“å®šä½å’ŒæŠ“å–ï¼Œä¾‹å¦‚æœºå™¨äººæŠ“å–ã€æ— äººæœºæŠ“å–ç­‰ã€‚è€Œæ»‘åŠ¨æ£€æµ‹åˆ™ä¸»è¦åº”ç”¨äºç‰©ä½“è¿åŠ¨çŠ¶æ€çš„æ£€æµ‹ï¼Œä¾‹å¦‚ç‰©ä½“æ»‘åŠ¨æ£€æµ‹ã€ç‰©ä½“æ»‘åŠ¨è½¨è¿¹æ£€æµ‹ç­‰ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ ChatMLæ ¼å¼çš„é€’å½’åˆ†è§£ç³»ç»Ÿ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. åˆ›å»ºå®ä¾‹\n",
    "decomp = RecursiveDecomposition(vectorstore, llm)\n",
    "\n",
    "# 2. æ‰§è¡ŒæŸ¥è¯¢\n",
    "result = decomp.query(\"æ¯”è¾ƒæŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹çš„ä¸»è¦å·®å¼‚\")\n",
    "\n",
    "# 3. æŸ¥çœ‹ç»“æœ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ å­é—®é¢˜åˆ†è§£:\")\n",
    "for i, (q, a) in enumerate(zip(result[\"sub_questions\"], result[\"sub_answers\"]), 1):\n",
    "    print(f\"{i}. é—®é¢˜: {q}\")\n",
    "    print(f\"   ç­”æ¡ˆ: {a[:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ’¡ æœ€ç»ˆç»¼åˆç­”æ¡ˆ:\")\n",
    "print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3933ad4-f934-4c50-85fd-362975a5cb42",
   "metadata": {},
   "source": [
    "### å†å®ç°å¹¶è¡Œåˆ†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dd32e9d-fc95-4403-8d46-46c6325a8d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•\n",
      "============================================================\n",
      "âœ… çœŸæ­£å¼‚æ­¥å¹¶è¡Œåˆ†è§£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n",
      "ğŸš€ å¹¶è¡Œæ‰§è¡Œæµ‹è¯•...\n",
      "ğŸ¯ é—®é¢˜: æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æœºæ¢°è‡‚ä»»åŠ¡ä¸­çš„å…³è”\n",
      "============================================================\n",
      "ğŸ” åˆ†è§£ä¸º 3 ä¸ªå­é—®é¢˜:\n",
      "   1. æŠ“å–æ£€æµ‹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "   2. æ»‘åŠ¨æ£€æµ‹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "   3. æœºæ¢°è‡‚ä»»åŠ¡ä¸­æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹çš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "âš¡ å¹¶è¡Œæ‰§è¡Œ 3 ä¸ªå­é—®é¢˜...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š æ‰§è¡Œç»Ÿè®¡:\n",
      "   âœ… æˆåŠŸ: 0/3\n",
      "   â±ï¸  å¹¶è¡Œæ—¶é—´: 0.20s\n",
      "\n",
      "ğŸ’¡ å­é—®é¢˜æ‰§è¡Œè¯¦æƒ…:\n",
      "   1. âŒ æ£€ç´¢: 0.05s, ç”Ÿæˆ: 0.01s, æ€»è®¡: 0.06s\n",
      "   2. âŒ æ£€ç´¢: 0.05s, ç”Ÿæˆ: 0.13s, æ€»è®¡: 0.17s\n",
      "   3. âŒ æ£€ç´¢: 0.05s, ç”Ÿæˆ: 0.15s, æ€»è®¡: 0.20s\n",
      "\n",
      "ğŸ§  ç»¼åˆæœ€ç»ˆç­”æ¡ˆ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸  æ€»è€—æ—¶: 1.29s\n",
      "ğŸ“Š å¹¶è¡Œæ•ˆç‡: 15.7%\n",
      "\n",
      "ğŸŒ ä¸²è¡Œæ‰§è¡Œæµ‹è¯•...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.82it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:\n",
      "   ğŸš€ å¹¶è¡Œæ—¶é—´: 1.29s\n",
      "   ğŸŒ ä¸²è¡Œæ—¶é—´: 2.76s\n",
      "   âš¡ åŠ é€Ÿæ¯”: 2.14x\n",
      "   ğŸ“Š æ•ˆç‡æå‡: 53.3%\n",
      "âœ… èµ„æºå·²å…³é—­\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from vllm import SamplingParams\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "class TrueAsyncParallelDecomposition:\n",
    "    \"\"\"çœŸæ­£çš„å¼‚æ­¥å¹¶è¡Œåˆ†è§£\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, max_workers: int = 4):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        print(\"âœ… çœŸæ­£å¼‚æ­¥å¹¶è¡Œåˆ†è§£ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    async def _generate(self, prompt: str, max_tokens: int = 512) -> Dict[str, Any]:\n",
    "        \"\"\"å¼‚æ­¥ç”Ÿæˆ\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.3,\n",
    "            top_p=0.8\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            outputs = await loop.run_in_executor(\n",
    "                self.executor,\n",
    "                lambda: self.llm.generate([prompt], sampling_params)\n",
    "            )\n",
    "            \n",
    "            if outputs and outputs[0].outputs:\n",
    "                text = outputs[0].outputs[0].text.strip()\n",
    "                return {\"success\": True, \"text\": text}\n",
    "            return {\"success\": False, \"error\": \"ç”Ÿæˆå¤±è´¥\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    async def answer_sub_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"å¼‚æ­¥å›ç­”å­é—®é¢˜\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. å¼‚æ­¥æ£€ç´¢\n",
    "            retriever = self.vectorstore.as_retriever()\n",
    "            docs = await retriever.aget_relevant_documents(question)\n",
    "            retrieval_time = time.time() - start_time\n",
    "            \n",
    "            if not docs:\n",
    "                return {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": \"æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£\",\n",
    "                    \"success\": False,\n",
    "                    \"retrieval_time\": retrieval_time\n",
    "                }\n",
    "            \n",
    "            # 2. æ„å»ºä¸Šä¸‹æ–‡\n",
    "            context = \"\\n\".join([doc.page_content[:100] for doc in docs[:2]])\n",
    "            \n",
    "            # 3. æ„å»ºæç¤ºè¯\n",
    "            prompt = f\"\"\"<|im_start|>system\n",
    "                å›ç­”é—®é¢˜ï¼š<|im_end|>\n",
    "                <|im_start|>user\n",
    "                ä¿¡æ¯ï¼š{context}\n",
    "                é—®é¢˜ï¼š{question}<|im_end|>\n",
    "                <|im_start|>assistant\n",
    "                \"\"\"\n",
    "            \n",
    "            # 4. å¼‚æ­¥ç”Ÿæˆ\n",
    "            gen_start = time.time()\n",
    "            result = await self._generate(prompt, max_tokens=512)\n",
    "            generation_time = time.time() - gen_start\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                return {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": result[\"text\"],\n",
    "                    \"success\": True,\n",
    "                    \"retrieval_time\": retrieval_time,\n",
    "                    \"generation_time\": generation_time,\n",
    "                    \"total_time\": time.time() - start_time\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": f\"ç”Ÿæˆå¤±è´¥: {result['error']}\",\n",
    "                    \"success\": False,\n",
    "                    \"retrieval_time\": retrieval_time,\n",
    "                    \"generation_time\": generation_time,\n",
    "                    \"total_time\": time.time() - start_time\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"é”™è¯¯: {str(e)}\",\n",
    "                \"success\": False,\n",
    "                \"retrieval_time\": 0,\n",
    "                \"generation_time\": 0,\n",
    "                \"total_time\": time.time() - start_time\n",
    "            }\n",
    "    \n",
    "    async def query_parallel(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"çœŸæ­£çš„å¼‚æ­¥å¹¶è¡ŒæŸ¥è¯¢\"\"\"\n",
    "        print(f\"ğŸ¯ é—®é¢˜: {question}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        overall_start = time.time()\n",
    "        \n",
    "        # 1. åˆ†è§£é—®é¢˜\n",
    "        sub_questions = [\n",
    "            \"æŠ“å–æ£€æµ‹æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "            \"æ»‘åŠ¨æ£€æµ‹æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "            \"æœºæ¢°è‡‚ä»»åŠ¡ä¸­æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹çš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"ğŸ” åˆ†è§£ä¸º {len(sub_questions)} ä¸ªå­é—®é¢˜:\")\n",
    "        for i, q in enumerate(sub_questions, 1):\n",
    "            print(f\"   {i}. {q}\")\n",
    "        \n",
    "        # 2. ğŸ”¥ çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œï¼ˆå…³é”®ä¿®å¤ï¼‰\n",
    "        print(f\"âš¡ å¹¶è¡Œæ‰§è¡Œ {len(sub_questions)} ä¸ªå­é—®é¢˜...\")\n",
    "        tasks = [self.answer_sub_question(q) for q in sub_questions]\n",
    "        sub_results = await asyncio.gather(*tasks)  # ğŸ”¥ å¹¶è¡Œæ‰§è¡Œ\n",
    "        \n",
    "        parallel_time = time.time() - overall_start\n",
    "        \n",
    "        # 3. åˆ†æç»“æœ\n",
    "        successful = [r for r in sub_results if r['success']]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:\")\n",
    "        print(f\"   âœ… æˆåŠŸ: {len(successful)}/{len(sub_questions)}\")\n",
    "        print(f\"   â±ï¸  å¹¶è¡Œæ—¶é—´: {parallel_time:.2f}s\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ¯ä¸ªå­é—®é¢˜çš„æ‰§è¡Œæ—¶é—´\n",
    "        print(\"\\nğŸ’¡ å­é—®é¢˜æ‰§è¡Œè¯¦æƒ…:\")\n",
    "        for i, result in enumerate(sub_results, 1):\n",
    "            status = \"âœ…\" if result['success'] else \"âŒ\"\n",
    "            print(f\"   {i}. {status} æ£€ç´¢: {result['retrieval_time']:.2f}s, \"\n",
    "                  f\"ç”Ÿæˆ: {result['generation_time']:.2f}s, \"\n",
    "                  f\"æ€»è®¡: {result['total_time']:.2f}s\")\n",
    "        \n",
    "        # 4. ç»¼åˆæœ€ç»ˆç­”æ¡ˆ\n",
    "        print(\"\\nğŸ§  ç»¼åˆæœ€ç»ˆç­”æ¡ˆ...\")\n",
    "        \n",
    "        if successful:\n",
    "            qa_context = \"\\n\".join([\n",
    "                f\"Q{i}: {r['question']}\\nA{i}: {r['answer']}\" \n",
    "                for i, r in enumerate(successful, 1)\n",
    "            ])\n",
    "            \n",
    "            synthesis_prompt = f\"\"\"<|im_start|>system\n",
    "åŸºäºä»¥ä¸‹é—®ç­”ä¿¡æ¯ç»¼åˆå›ç­”åŸå§‹é—®é¢˜ï¼š<|im_end|>\n",
    "<|im_start|>user\n",
    "åŸå§‹é—®é¢˜ï¼š{question}\n",
    "\n",
    "å­é—®é¢˜å’Œç­”æ¡ˆï¼š\n",
    "{qa_context}\n",
    "\n",
    "è¯·ç»™å‡ºç»¼åˆç­”æ¡ˆï¼š<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        else:\n",
    "            synthesis_prompt = f\"\"\"<|im_start|>system\n",
    "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š<|im_end|>\n",
    "<|im_start|>user\n",
    "é—®é¢˜ï¼š{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        \n",
    "        final_result = await self._generate(synthesis_prompt, max_tokens=512)\n",
    "        total_time = time.time() - overall_start\n",
    "        \n",
    "        if final_result[\"success\"]:\n",
    "            final_answer = final_result[\"text\"]\n",
    "        else:\n",
    "            final_answer = f\"ç»¼åˆå¤±è´¥: {final_result['error']}\"\n",
    "        \n",
    "        print(f\"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s\")\n",
    "        print(f\"ğŸ“Š å¹¶è¡Œæ•ˆç‡: {parallel_time/total_time*100:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"sub_questions\": sub_questions,\n",
    "            \"sub_results\": sub_results,\n",
    "            \"metrics\": {\n",
    "                \"total_time\": total_time,\n",
    "                \"parallel_time\": parallel_time,\n",
    "                \"success_rate\": f\"{len(successful)}/{len(sub_questions)}\",\n",
    "                \"parallel_efficiency\": f\"{parallel_time/total_time*100:.1f}%\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"å…³é—­èµ„æº\"\"\"\n",
    "        self.executor.shutdown()\n",
    "        print(\"âœ… èµ„æºå·²å…³é—­\")\n",
    "\n",
    "\n",
    "# === æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===\n",
    "async def performance_comparison():\n",
    "    \"\"\"æ€§èƒ½å¯¹æ¯”ï¼šå¹¶è¡Œ vs ä¸²è¡Œ\"\"\"\n",
    "    print(\"ğŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    decomp = TrueAsyncParallelDecomposition(vectorstore, llm)\n",
    "    \n",
    "    try:\n",
    "        question = \"æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹åœ¨æœºæ¢°è‡‚ä»»åŠ¡ä¸­çš„å…³è”\"\n",
    "        \n",
    "        # å¹¶è¡Œæ‰§è¡Œ\n",
    "        print(\"ğŸš€ å¹¶è¡Œæ‰§è¡Œæµ‹è¯•...\")\n",
    "        parallel_start = time.time()\n",
    "        parallel_result = await decomp.query_parallel(question)\n",
    "        parallel_time = time.time() - parallel_start\n",
    "        \n",
    "        # ä¸²è¡Œæ‰§è¡Œï¼ˆæ¨¡æ‹Ÿï¼‰\n",
    "        print(\"\\nğŸŒ ä¸²è¡Œæ‰§è¡Œæµ‹è¯•...\")\n",
    "        serial_start = time.time()\n",
    "        \n",
    "        sub_questions = [\n",
    "            \"æŠ“å–æ£€æµ‹æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "            \"æ»‘åŠ¨æ£€æµ‹æ˜¯ä»€ä¹ˆï¼Ÿ\", \n",
    "            \"æœºæ¢°è‡‚ä»»åŠ¡ä¸­æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹çš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "        ]\n",
    "        \n",
    "        serial_results = []\n",
    "        for q in sub_questions:\n",
    "            result = await decomp.answer_sub_question(q)  # é¡ºåºæ‰§è¡Œ\n",
    "            serial_results.append(result)\n",
    "        \n",
    "        serial_time = time.time() - serial_start\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:\")\n",
    "        print(f\"   ğŸš€ å¹¶è¡Œæ—¶é—´: {parallel_time:.2f}s\")\n",
    "        print(f\"   ğŸŒ ä¸²è¡Œæ—¶é—´: {serial_time:.2f}s\")\n",
    "        print(f\"   âš¡ åŠ é€Ÿæ¯”: {serial_time/parallel_time:.2f}x\")\n",
    "        print(f\"   ğŸ“Š æ•ˆç‡æå‡: {(serial_time-parallel_time)/serial_time*100:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"parallel_time\": parallel_time,\n",
    "            \"serial_time\": serial_time,\n",
    "            \"speedup\": serial_time / parallel_time\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        await decomp.close()\n",
    "\n",
    "\n",
    "# === ä½¿ç”¨ç¤ºä¾‹ ===\n",
    "async def true_async_demo():\n",
    "    \"\"\"çœŸæ­£å¼‚æ­¥å¹¶è¡Œæ¼”ç¤º\"\"\"\n",
    "    print(\"ğŸš€ çœŸæ­£å¼‚æ­¥å¹¶è¡Œåˆ†è§£æ¼”ç¤º\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    decomp = TrueAsyncParallelDecomposition(vectorstore, llm)\n",
    "    \n",
    "    try:\n",
    "        result = await decomp.query_parallel(\"æŠ“å–æ£€æµ‹å’Œæ»‘åŠ¨æ£€æµ‹ä¸æœºæ¢°è‡‚ä»»åŠ¡æœ‰ä»€ä¹ˆå…³è”ï¼Ÿ\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ æœ€ç»ˆç»“æœ:\")\n",
    "        print(f\"é—®é¢˜: {result['question']}\")\n",
    "        print(f\"æˆåŠŸç‡: {result['metrics']['success_rate']}\")\n",
    "        print(f\"æ€»è€—æ—¶: {result['metrics']['total_time']:.2f}s\")\n",
    "        print(f\"å¹¶è¡Œæ—¶é—´: {result['metrics']['parallel_time']:.2f}s\")\n",
    "        print(f\"å¹¶è¡Œæ•ˆç‡: {result['metrics']['parallel_efficiency']}\")\n",
    "        print(f\"ç­”æ¡ˆ: {result['final_answer']}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    finally:\n",
    "        await decomp.close()\n",
    "\n",
    "\n",
    "# è¿è¡ŒçœŸæ­£å¼‚æ­¥å¹¶è¡Œç‰ˆæœ¬ï¼š\n",
    "# result = await true_async_demo()\n",
    "\n",
    "# æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼š\n",
    "comparison = await performance_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0352f-dc80-43e0-adf6-17ac75fd83d4",
   "metadata": {},
   "source": [
    "**åˆ†å—ç­–ç•¥å¯¹æ¯”**\n",
    "| ç‰¹æ€§ | é€’å½’åˆ†è§£ | å¹¶è¡Œåˆ†è§£ |\n",
    "|:-----|:-------:|:-------:|\n",
    "| æ‰§è¡Œæ–¹å¼ | é¡ºåº | å¹¶è¡Œ |\n",
    "| é€Ÿåº¦ | æ…¢ | å¿« âš¡ |\n",
    "| å­é—®é¢˜ä¾èµ– | æ”¯æŒ | ä¸æ”¯æŒ |\n",
    "| é€‚ç”¨åœºæ™¯ | æœ‰é€»è¾‘é¡ºåºçš„é—®é¢˜ | ç‹¬ç«‹å­é—®é¢˜ |\n",
    "| å®ç°å¤æ‚åº¦ | ä¸­ç­‰ | é«˜ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f71f4-74c7-4e2f-9237-9859b2a968f3",
   "metadata": {},
   "source": [
    "## Part 4: Step Back Prompting - æŠ½è±¡åŒ–æé—®\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šStep Back Promptingå…ˆæå‡ºä¸€ä¸ªæ›´æŠ½è±¡ã€æ›´æ¦‚æ‹¬çš„é—®é¢˜ï¼Œè·å–èƒŒæ™¯çŸ¥è¯†åï¼Œå†å›ç­”åŸå§‹å…·ä½“é—®é¢˜ã€‚\n",
    "\n",
    "ä¸ºä»€ä¹ˆéœ€è¦Step Backï¼Ÿ\n",
    "\n",
    "```python\n",
    "# âŒ ç›´æ¥å›ç­”å¯èƒ½ç¼ºä¹èƒŒæ™¯\n",
    "\n",
    "åŸå§‹é—®é¢˜: \"Transformerä¸­çš„Multi-Head Attentionæœ‰å‡ ä¸ªå¤´ï¼Ÿ\"\n",
    "\n",
    "ç›´æ¥æ£€ç´¢ â†’ å¯èƒ½æ‰¾ä¸åˆ°ç¡®åˆ‡ç­”æ¡ˆï¼ˆæ–‡æ¡£åªæè¿°äº†åŸç†ï¼Œæ²¡è¯´å…·ä½“æ•°å­—ï¼‰\n",
    "\n",
    "# âœ… Step Backå\n",
    "\n",
    "Step 1: Step Backé—®é¢˜\n",
    "\"Transformeræ¶æ„çš„åŸºæœ¬ç»„æˆæ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "Step 2: è·å–èƒŒæ™¯çŸ¥è¯†\n",
    "\"Transformerç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œä½¿ç”¨Multi-Head Attention...\"\n",
    "\n",
    "Step 3: ç»“åˆèƒŒæ™¯å›ç­”åŸå§‹é—®é¢˜\n",
    "\"æ ¹æ®åŸå§‹è®ºæ–‡ï¼Œä½¿ç”¨8ä¸ªattentionå¤´...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96d6fdda-60c4-40e0-8621-eceef0236ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "from vllm import SamplingParams\n",
    "\n",
    "class StepBackRAG:\n",
    "    \"\"\"Step Backæç¤ºçš„RAGç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.retriever = vectorstore.as_retriever()\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_step_back_question(self, question: str) -> str:\n",
    "        \"\"\"ç”ŸæˆStep Backé—®é¢˜\"\"\"\n",
    "        # ChatMLæ ¼å¼æç¤ºè¯\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            ç»™å®šä¸€ä¸ªå…·ä½“é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªæ›´æŠ½è±¡ã€æ›´æ¦‚æ‹¬çš„é—®é¢˜ã€‚\n",
    "            \n",
    "            ç¤ºä¾‹ï¼š\n",
    "            å…·ä½“é—®é¢˜: \"GPT-4çš„ä¸Šä¸‹æ–‡é•¿åº¦æ˜¯å¤šå°‘ï¼Ÿ\"\n",
    "            Step Backé—®é¢˜: \"GPT-4çš„ä¸»è¦æŠ€æœ¯ç‰¹æ€§æœ‰å“ªäº›ï¼Ÿ\"\n",
    "            \n",
    "            å…·ä½“é—®é¢˜: \"Pythonä¸­çš„è£…é¥°å™¨å¦‚ä½•å·¥ä½œï¼Ÿ\"\n",
    "            Step Backé—®é¢˜: \"Pythonä¸­çš„å…ƒç¼–ç¨‹æ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ\"<|im_end|>\n",
    "            <|im_start|>user\n",
    "            å…·ä½“é—®é¢˜: {question}\n",
    "            Step Backé—®é¢˜:<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=512,\n",
    "            temperature=0.3,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        if outputs and outputs[0].outputs:\n",
    "            step_back_q = outputs[0].outputs[0].text.strip()\n",
    "        else:\n",
    "            step_back_q = \"ç”Ÿæˆå¤±è´¥\"\n",
    "        \n",
    "        return step_back_q\n",
    "    \n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"æ‰§è¡ŒStep Back RAG\"\"\"\n",
    "        print(f\"â“ åŸå§‹é—®é¢˜: {question}\")\n",
    "        \n",
    "        # 1. ç”ŸæˆStep Backé—®é¢˜\n",
    "        step_back_q = self.generate_step_back_question(question)\n",
    "        print(f\"ğŸ“š Step Backé—®é¢˜: {step_back_q}\")\n",
    "        \n",
    "        # 2. æ£€ç´¢èƒŒæ™¯çŸ¥è¯†ï¼ˆStep Backé—®é¢˜ï¼‰\n",
    "        background_docs = self.retriever.get_relevant_documents(step_back_q)\n",
    "        background = \"\\n\\n\".join([doc.page_content for doc in background_docs[:2]])\n",
    "        \n",
    "        # 3. æ£€ç´¢å…·ä½“ä¿¡æ¯ï¼ˆåŸå§‹é—®é¢˜ï¼‰\n",
    "        specific_docs = self.retriever.get_relevant_documents(question)\n",
    "        specific = \"\\n\\n\".join([doc.page_content for doc in specific_docs[:2]])\n",
    "        \n",
    "        # 4. ç»“åˆä¸¤è€…ç”Ÿæˆç­”æ¡ˆ\n",
    "        # ChatMLæ ¼å¼æç¤ºè¯\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            åŸºäºæä¾›çš„èƒŒæ™¯çŸ¥è¯†å’Œå…·ä½“ä¿¡æ¯å›ç­”é—®é¢˜ã€‚\n",
    "            å…ˆç†è§£èƒŒæ™¯ï¼Œå†å›ç­”å…·ä½“é—®é¢˜ã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            èƒŒæ™¯çŸ¥è¯†:\n",
    "            {background}\n",
    "            \n",
    "            å…·ä½“ä¿¡æ¯:\n",
    "            {specific}\n",
    "            \n",
    "            åŸå§‹é—®é¢˜: {question}\n",
    "            è¯·å›ç­”:<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=512,\n",
    "            temperature=0.3,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        if outputs and outputs[0].outputs:\n",
    "            answer = outputs[0].outputs[0].text.strip()\n",
    "        else:\n",
    "            answer = \"ç”Ÿæˆå¤±è´¥\"\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"step_back_question\": step_back_q,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "015bd901-32ef-49aa-82f4-10bfbd013465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ åŸå§‹é—®é¢˜: æ»‘åŠ¨æ£€æµ‹çš„æ“ä½œæµç¨‹ï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Step Backé—®é¢˜: ä»€ä¹ˆæ˜¯æ»‘åŠ¨æ£€æµ‹ï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¡ ç­”æ¡ˆ:\n",
      "æ»‘åŠ¨æ£€æµ‹çš„æ“ä½œæµç¨‹å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. æ»‘åŠ¨å‘ç”Ÿæ—¶ï¼Œæ»‘åŠ¨æ£€æµ‹ç®—æ³•ä¼šè§¦å‘ä¸€ä¸ªæ»‘åŠ¨ä¿¡å·ã€‚\n",
      "2. å½“ç›®æ ‡å‘ç”Ÿæ»‘åŠ¨æ—¶ï¼Œæ§åˆ¶å™¨ä¼šæŒ‰ç…§åŸå§‹æ“ä½œæµç¨‹è¿›è¡Œæ»‘åŠ¨æ£€æµ‹ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "step_back_rag = StepBackRAG(vectorstore, llm)\n",
    "result = step_back_rag.query(\"æ»‘åŠ¨æ£€æµ‹çš„æ“ä½œæµç¨‹ï¼Ÿ\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ç­”æ¡ˆ:\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d48ce-9c72-4961-bc67-607468a01a3c",
   "metadata": {},
   "source": [
    "Step Backçš„ä¼˜åŠ¿ï¼š\n",
    "\n",
    "ä½•æ—¶ä½¿ç”¨Step Back:\n",
    "- é—®é¢˜éœ€è¦èƒŒæ™¯çŸ¥è¯†\n",
    "- ç›´æ¥æ£€ç´¢æ•ˆæœä¸å¥½\n",
    "- é—®é¢˜è¿‡äºå…·ä½“æˆ–æŠ€æœ¯æ€§å¼º\n",
    "- éœ€è¦ç†è®ºæ”¯æ’‘ç­”æ¡ˆ\n",
    "\n",
    "ä¸é€‚åˆçš„åœºæ™¯ï¼š\n",
    "- ç®€å•äº‹å®æ€§é—®é¢˜\n",
    "- å·²æœ‰å……è¶³ç›´æ¥ä¿¡æ¯\n",
    "- å®æ—¶æ•°æ®æŸ¥è¯¢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2bc5d0-c3b3-4aad-918d-519731275ba2",
   "metadata": {},
   "source": [
    "## Part 5: HyDE - å‡è®¾æ€§æ–‡æ¡£åµŒå…¥\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µï¼šHyDE (Hypothetical Document Embeddings) ä¸ç›´æ¥æ£€ç´¢ç”¨æˆ·æŸ¥è¯¢ï¼Œè€Œæ˜¯å…ˆè®©LLMç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§çš„ç­”æ¡ˆæ–‡æ¡£ï¼Œç„¶åç”¨è¿™ä¸ªæ–‡æ¡£å»æ£€ç´¢ç›¸ä¼¼å†…å®¹ã€‚\n",
    "\n",
    "**HyDEçš„ç›´è§‰**\n",
    "\n",
    "```python\n",
    "# ä¼ ç»Ÿæ£€ç´¢\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\"\n",
    "    â†“ ç›´æ¥åµŒå…¥\n",
    "æŸ¥è¯¢å‘é‡: [0.1, 0.3, -0.2, ...]\n",
    "    â†“ æ£€ç´¢\n",
    "æ‰¾åˆ°çš„æ–‡æ¡£\n",
    "\n",
    "# HyDEæ£€ç´¢\n",
    "ç”¨æˆ·æŸ¥è¯¢: \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\"\n",
    "    â†“ LLMç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆ\n",
    "å‡è®¾æ–‡æ¡£: \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹...\"\n",
    "    â†“ åµŒå…¥å‡è®¾æ–‡æ¡£\n",
    "æ–‡æ¡£å‘é‡: [0.2, 0.4, -0.1, ...]  # ä¸çœŸå®ç­”æ¡ˆæ–‡æ¡£æ›´ç›¸ä¼¼ï¼\n",
    "    â†“ æ£€ç´¢\n",
    "æ‰¾åˆ°æ›´ç›¸å…³çš„æ–‡æ¡£\n",
    "```\n",
    "\n",
    "ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ\n",
    "\n",
    "æŸ¥è¯¢é€šå¸¸å¾ˆçŸ­ï¼Œè€Œæ–‡æ¡£å†…å®¹ä¸°å¯Œã€‚å‡è®¾æ€§æ–‡æ¡£æ¯”æŸ¥è¯¢æ›´æ¥è¿‘çœŸå®æ–‡æ¡£çš„è¡¨è¾¾æ–¹å¼ï¼Œå› æ­¤èƒ½æ‰¾åˆ°æ›´ç›¸å…³çš„å†…å®¹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e31cf40d-c081-4720-ade1-e3829cd0a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyDERAG:\n",
    "    \"\"\"HyDE (Hypothetical Document Embeddings) RAG\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm, embeddings):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "    \n",
    "    def generate_hypothetical_document(\n",
    "        self, \n",
    "        question: str,\n",
    "        style: str = \"academic\"\n",
    "    ) -> str:\n",
    "        \"\"\"ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£\"\"\"\n",
    "        if style == \"academic\":\n",
    "            system_msg = \"ä½ æ˜¯ä¸€ä½ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹é—®é¢˜å†™ä¸€æ®µè¯¦ç»†ã€å‡†ç¡®çš„å­¦æœ¯æ€§å›ç­”ï¼ˆ200-300å­—ï¼‰ã€‚\"\n",
    "        elif style == \"concise\":\n",
    "            system_msg = \"è¯·å¯¹ä»¥ä¸‹é—®é¢˜å†™ä¸€æ®µç®€æ´çš„å›ç­”ï¼ˆ100-150å­—ï¼‰ã€‚\"\n",
    "        else:\n",
    "            system_msg = \"è¯·å¯¹ä»¥ä¸‹é—®é¢˜å†™ä¸€æ®µå›ç­”ã€‚\"\n",
    "        \n",
    "        # ChatMLæ ¼å¼æç¤ºè¯\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            {system_msg}<|im_end|>\n",
    "            <|im_start|>user\n",
    "            {question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        from vllm import SamplingParams\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=400,\n",
    "            temperature=0.3,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        if outputs and outputs[0].outputs:\n",
    "            hypothetical_doc = outputs[0].outputs[0].text.strip()\n",
    "        else:\n",
    "            hypothetical_doc = \"ç”Ÿæˆå¤±è´¥\"\n",
    "        \n",
    "        return hypothetical_doc\n",
    "    \n",
    "    def search_with_hyde(\n",
    "        self, \n",
    "        question: str, \n",
    "        k: int = 5\n",
    "    ) -> List:\n",
    "        \"\"\"ä½¿ç”¨HyDEè¿›è¡Œæ£€ç´¢\"\"\"\n",
    "        # 1. ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£\n",
    "        hyp_doc = self.generate_hypothetical_document(question)\n",
    "        print(f\"ğŸ“ å‡è®¾æ€§æ–‡æ¡£:\\n{hyp_doc[:200]}...\\n\")\n",
    "        \n",
    "        # 2. ä½¿ç”¨å‡è®¾æ€§æ–‡æ¡£æ£€ç´¢\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œç”¨å‡è®¾æ–‡æ¡£è€Œä¸æ˜¯åŸå§‹æŸ¥è¯¢\n",
    "        docs = self.vectorstore.similarity_search(hyp_doc, k=k)\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def query(self, question: str, k: int = 5) -> dict:\n",
    "        \"\"\"æ‰§è¡ŒHyDE RAGæŸ¥è¯¢\"\"\"\n",
    "        print(f\"â“ åŸå§‹é—®é¢˜: {question}\\n\")\n",
    "        \n",
    "        # 1. HyDEæ£€ç´¢\n",
    "        docs = self.search_with_hyde(question, k=k)\n",
    "        print(f\"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£\\n\")\n",
    "        \n",
    "        # 2. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # ChatMLæ ¼å¼æç¤ºè¯\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            åŸºäºä»¥ä¸‹æ–‡æ¡£å‡†ç¡®å›ç­”é—®é¢˜ã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            æ–‡æ¡£:\n",
    "            {context}\n",
    "            \n",
    "            é—®é¢˜: {question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        from vllm import SamplingParams\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=500,\n",
    "            temperature=0.3,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        if outputs and outputs[0].outputs:\n",
    "            answer = outputs[0].outputs[0].text.strip()\n",
    "        else:\n",
    "            answer = \"ç”Ÿæˆå¤±è´¥\"\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"num_docs\": len(docs),\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a4c02c8-dc2a-46be-b4d7-5d0b9f41f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ åŸå§‹é—®é¢˜: è§£é‡Šæ»‘åŠ¨æ£€æµ‹ä¸­çš„è§¦è§‰ä¼ æ„Ÿå™¨\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ å‡è®¾æ€§æ–‡æ¡£:\n",
      "æ»‘åŠ¨æ£€æµ‹ä¸­çš„è§¦è§‰ä¼ æ„Ÿå™¨æ˜¯ä¸€ç§ç”¨äºæ£€æµ‹ç‰©ä½“è¡¨é¢æ»‘åŠ¨çš„ä¼ æ„Ÿå™¨ã€‚å®ƒä»¬é€šå¸¸ç”±ä¸€ä¸ªå¼¹æ€§è†œç‰‡å’Œä¸€ä¸ªæ•æ„Ÿå…ƒä»¶ç»„æˆï¼Œå½“ç‰©ä½“æ»‘è¿‡è†œç‰‡æ—¶ï¼Œæ•æ„Ÿå…ƒä»¶ä¼šæ„Ÿå—åˆ°ç‰©ä½“çš„æ»‘åŠ¨ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç”µä¿¡å·ã€‚è¿™äº›ä¼ æ„Ÿå™¨å¯ä»¥ç”¨äºå„ç§åº”ç”¨ï¼Œå¦‚æœºå™¨äººå¯¼èˆªã€æ±½è½¦å®‰å…¨ç³»ç»Ÿå’Œå·¥ä¸šè‡ªåŠ¨åŒ–ã€‚å®ƒä»¬é€šå¸¸å…·æœ‰é«˜ç²¾åº¦ã€é«˜çµæ•åº¦å’Œå¿«é€Ÿå“åº”æ—¶é—´çš„ç‰¹ç‚¹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹å’Œè¯†åˆ«ç‰©ä½“çš„æ»‘åŠ¨è¡Œä¸ºã€‚...\n",
      "\n",
      "ğŸ“š æ£€ç´¢åˆ° 5 ä¸ªæ–‡æ¡£\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:\n",
      "è§¦è§‰ä¼ æ„Ÿå™¨æ˜¯ä¸€ç§èƒ½å¤Ÿæ£€æµ‹ç‰©ä½“è¡¨é¢å‹åŠ›å˜åŒ–çš„ä¼ æ„Ÿå™¨ï¼Œå½“ç‰©ä½“å‘ç”Ÿæ»‘åŠ¨æ—¶ï¼Œè§¦è§‰ä¼ æ„Ÿå™¨ä¼šé«˜é¢‘æŒ¯åŠ¨ï¼Œè§¦è§‰ä¼ æ„Ÿå™¨æ•°æ®ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä»è€Œå¼•èµ·å‹åŠ›ä¸­å¿ƒæ•°å€¼çš„å˜åŒ–ï¼Œé€šè¿‡åˆ†æå‹åŠ›ä¸­å¿ƒæ•°å€¼çš„å˜åŒ–å¯ä»¥è·å¾—ç‰©ä½“çš„æ»‘åŠ¨æƒ…å†µã€‚æ»‘åŠ¨æ£€æµ‹æ˜¯æœºå™¨äººå¤¹çˆªæŠ“å–æ§åˆ¶çš„æ ¸å¿ƒã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "hyde_rag = HyDERAG(vectorstore, llm, embeddings)\n",
    "result = hyde_rag.query(\"è§£é‡Šæ»‘åŠ¨æ£€æµ‹ä¸­çš„è§¦è§‰ä¼ æ„Ÿå™¨\")\n",
    "\n",
    "print(\"ğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce3fa0-fdc8-4965-979b-1612bed41026",
   "metadata": {},
   "source": [
    "HyDE vs ä¼ ç»Ÿæ£€ç´¢\n",
    "\n",
    "| ç»´åº¦ | ä¼ ç»Ÿæ£€ç´¢ | HyDE |\n",
    "|:-----|:-------:|:-----:|\n",
    "| æ£€ç´¢å¯¹è±¡ | ç”¨æˆ·æŸ¥è¯¢ | å‡è®¾æ€§ç­”æ¡ˆ |\n",
    "| è¯­ä¹‰åŒ¹é… | æŸ¥è¯¢â†”æ–‡æ¡£ | ç­”æ¡ˆâ†”ç­”æ¡ˆ |\n",
    "| æŸ¥è¯¢é•¿åº¦æ•æ„Ÿæ€§ | æ˜¯ | å¦ |\n",
    "| é¢å¤–LLMè°ƒç”¨ | 0 | 1 |\n",
    "| é€‚ç”¨åœºæ™¯ | æ¸…æ™°æŸ¥è¯¢ | å¤æ‚/æŠ€æœ¯æ€§æŸ¥è¯¢ |\n",
    "| æ£€ç´¢è´¨é‡ | ä¸­ç­‰ | é«˜ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de224a0d-4dec-40dd-a2c2-818eec587d46",
   "metadata": {},
   "source": [
    "# æŠ€æœ¯å¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—\n",
    "\n",
    "**ç»¼åˆå¯¹æ¯”**\n",
    "\n",
    "| æŠ€æœ¯ | å¬å›ç‡ | å‡†ç¡®ç‡ | é€Ÿåº¦ | æˆæœ¬ | å¤æ‚åº¦ |\n",
    "|:-----|:------:|:------:|:----:|:----:|:------:|\n",
    "| Multi-Query | â­â­â­ | â­â­ | â­â­ | ğŸ’°ğŸ’° | â­ |\n",
    "| RAG-Fusion | â­â­â­â­ | â­â­â­â­ | â­â­ | ğŸ’°ğŸ’°ğŸ’° | â­â­ |\n",
    "| Decomposition | â­â­â­â­ | â­â­â­â­ | â­ | ğŸ’°ğŸ’°ğŸ’°ğŸ’° | â­â­â­ |\n",
    "| Step Back | â­â­â­ | â­â­â­â­ | â­â­ | ğŸ’°ğŸ’° | â­â­ |\n",
    "| HyDE | â­â­â­â­ | â­â­â­â­â­ | â­â­ | ğŸ’°ğŸ’° | â­â­ |\n",
    "\n",
    "**é€‰æ‹©å†³ç­–æ ‘**\n",
    "\n",
    "```python\n",
    "é—®é¢˜ç±»å‹ï¼Ÿ\n",
    "â”œâ”€ ç®€å•æŸ¥è¯¢ï¼ˆå•ä¸€äº‹å®ï¼‰\n",
    "â”‚  â†’ ä¸éœ€è¦ä¼˜åŒ–ï¼Œä½¿ç”¨åŸºç¡€RAG\n",
    "â”‚\n",
    "â”œâ”€ è¡¨è¾¾æ¨¡ç³Š/å¤šè§’åº¦\n",
    "â”‚  â†’ Multi-Query æˆ– RAG-Fusion\n",
    "â”‚\n",
    "â”œâ”€ å¤æ‚å¤šæ­¥éª¤é—®é¢˜\n",
    "â”‚  â†’ Query Decomposition\n",
    "â”‚\n",
    "â”œâ”€ éœ€è¦èƒŒæ™¯çŸ¥è¯†\n",
    "â”‚  â†’ Step Back\n",
    "â”‚\n",
    "â””â”€ æŠ€æœ¯æ€§å¼º/ä¸“ä¸šé¢†åŸŸ\n",
    "   â†’ HyDE\n",
    "```\n",
    "\n",
    "**ç»„åˆä½¿ç”¨**\n",
    "\n",
    "è¿™äº›æŠ€æœ¯å¯ä»¥ç»„åˆä½¿ç”¨ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a874833d-4873-480e-a5eb-81f91065507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    \"\"\"ç»„åˆå¤šç§æŠ€æœ¯çš„é«˜çº§RAGç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        \n",
    "    def intelligent_query(self, question: str) -> dict:\n",
    "        \"\"\"æ™ºèƒ½é€‰æ‹©å’Œç»„åˆæŠ€æœ¯\"\"\"\n",
    "        \n",
    "        # 1. åˆ†ææŸ¥è¯¢å¤æ‚åº¦\n",
    "        complexity = self.analyze_complexity(question)\n",
    "        \n",
    "        if complexity == \"simple\":\n",
    "            # ç®€å•æŸ¥è¯¢ï¼šç›´æ¥æ£€ç´¢\n",
    "            return self.simple_rag(question)\n",
    "        \n",
    "        elif complexity == \"ambiguous\":\n",
    "            # æ¨¡ç³ŠæŸ¥è¯¢ï¼šMulti-Query + RRF\n",
    "            return self.multi_query_with_fusion(question)\n",
    "        \n",
    "        elif complexity == \"complex\":\n",
    "            # å¤æ‚æŸ¥è¯¢ï¼šåˆ†è§£ + HyDE\n",
    "            return self.decomposition_with_hyde(question)\n",
    "        \n",
    "        elif complexity == \"technical\":\n",
    "            # æŠ€æœ¯æŸ¥è¯¢ï¼šStep Back + HyDE\n",
    "            return self.step_back_with_hyde(question)\n",
    "    \n",
    "    def analyze_complexity(self, question: str) -> str:\n",
    "        \"\"\"åˆ†ææŸ¥è¯¢å¤æ‚åº¦\"\"\"\n",
    "        # ChatMLæ ¼å¼æç¤ºè¯\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "            åˆ†ææŸ¥è¯¢çš„å¤æ‚åº¦ï¼Œè¿”å›ä»¥ä¸‹ä¹‹ä¸€:\n",
    "            - simple: ç®€å•äº‹å®æ€§é—®é¢˜\n",
    "            - ambiguous: è¡¨è¾¾æ¨¡ç³Šçš„é—®é¢˜\n",
    "            - complex: éœ€è¦å¤šæ­¥æ¨ç†çš„å¤æ‚é—®é¢˜\n",
    "            - technical: æŠ€æœ¯æ€§å¼ºçš„ä¸“ä¸šé—®é¢˜\n",
    "            \n",
    "            åªè¿”å›ç±»åˆ«ï¼Œä¸è¦è§£é‡Šã€‚<|im_end|>\n",
    "            <|im_start|>user\n",
    "            {question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "        \n",
    "        from vllm import SamplingParams\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=50,\n",
    "            temperature=0.1,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        if outputs and outputs[0].outputs:\n",
    "            complexity = outputs[0].outputs[0].text.strip().lower()\n",
    "        else:\n",
    "            complexity = \"simple\"  # é»˜è®¤ç®€å•\n",
    "        \n",
    "        return complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc36365-907c-4094-896e-271e9e2d0298",
   "metadata": {},
   "source": [
    "# æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea63715-287e-4563-8576-edeeca00ac58",
   "metadata": {},
   "source": [
    "## ç¼“å­˜ç­–ç•¥\n",
    "\n",
    "æŠ€æœ¯æµç¨‹ï¼š\n",
    "\n",
    "```python\n",
    "ç”¨æˆ·è¾“å…¥é—®é¢˜ question\n",
    "        â†“\n",
    "è°ƒç”¨ cached_multi_query(question)\n",
    "        â†“\n",
    "1. ç”Ÿæˆç¼“å­˜é”®ï¼šmethod + question â†’ MD5 å“ˆå¸Œå€¼\n",
    "        â†“\n",
    "2. æ£€æŸ¥ç¼“å­˜ï¼ˆquery_cacheï¼‰ä¸­æ˜¯å¦å­˜åœ¨è¯¥é”®ï¼Ÿ\n",
    "        â”œâ”€â”€ æ˜¯ â†’ ç›´æ¥è¿”å›ç¼“å­˜ä¸­çš„æŸ¥è¯¢åˆ—è¡¨ï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰\n",
    "        â””â”€â”€ å¦ â†’ \n",
    "                â†“\n",
    "            è°ƒç”¨ generate_queries(question) ç”Ÿæˆæ–°æŸ¥è¯¢\n",
    "                â†“\n",
    "            å°†ç»“æœå­˜å…¥ç¼“å­˜ï¼šquery_cache[cache_key] = queries\n",
    "                â†“\n",
    "            è¿”å›æ–°ç”Ÿæˆçš„æŸ¥è¯¢åˆ—è¡¨\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c27644d9-7536-4037-8a4d-601ae262052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class CachedQueryOptimizer:\n",
    "    \"\"\"å¸¦ç¼“å­˜çš„æŸ¥è¯¢ä¼˜åŒ–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.query_cache = {}\n",
    "        self.doc_cache = {}\n",
    "    \n",
    "    def get_cache_key(self, query: str, method: str) -> str:\n",
    "        \"\"\"ç”Ÿæˆç¼“å­˜é”®\"\"\"\n",
    "        content = f\"{method}:{query}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def cached_multi_query(self, question: str) -> List[str]:\n",
    "        \"\"\"ç¼“å­˜Multi-Queryç»“æœ\"\"\"\n",
    "        cache_key = self.get_cache_key(question, \"multi_query\")\n",
    "        \n",
    "        if cache_key in self.query_cache:\n",
    "            print(\"ğŸ’¾ ä½¿ç”¨ç¼“å­˜çš„æŸ¥è¯¢å˜ä½“\")\n",
    "            return self.query_cache[cache_key]\n",
    "        \n",
    "        # ç”ŸæˆæŸ¥è¯¢\n",
    "        queries = generate_queries(question)\n",
    "        self.query_cache[cache_key] = queries\n",
    "        \n",
    "        return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc5592-9ebf-4e7b-8736-d58f08814d0a",
   "metadata": {},
   "source": [
    "## å¹¶è¡Œå¤„ç†\n",
    "\n",
    "```python\n",
    "å¼€å§‹å¹¶è¡Œå¤šæŸ¥è¯¢å¤„ç†\n",
    "        â†“\n",
    "è¾“å…¥å‚æ•°ï¼šqueriesåˆ—è¡¨ï¼Œretrieveræ£€ç´¢å™¨ï¼Œmax_workers=5\n",
    "        â†“\n",
    "1. è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼šasyncio.get_event_loop()\n",
    "        â†“\n",
    "2. åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼šThreadPoolExecutor(max_workers=max_workers)\n",
    "        â†“\n",
    "3. éå†æŸ¥è¯¢åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªæŸ¥è¯¢åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ï¼š\n",
    "        â”œâ”€â”€ æŸ¥è¯¢1 â†’ loop.run_in_executor(executor, retriever.get_relevant_documents, query1)\n",
    "        â”œâ”€â”€ æŸ¥è¯¢2 â†’ loop.run_in_executor(executor, retriever.get_relevant_documents, query2)\n",
    "        â”œâ”€â”€ æŸ¥è¯¢3 â†’ loop.run_in_executor(executor, retriever.get_relevant_documents, query3)\n",
    "        â”œâ”€â”€ æŸ¥è¯¢4 â†’ loop.run_in_executor(executor, retriever.get_relevant_documents, query4)\n",
    "        â””â”€â”€ æŸ¥è¯¢5 â†’ loop.run_in_executor(executor, retriever.get_relevant_documents, query5)\n",
    "        â†“\n",
    "4. ä½¿ç”¨asyncio.gatherå¹¶è¡Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
    "        â†“\n",
    "5. æ”¶é›†æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æœ\n",
    "        â†“\n",
    "è¿”å›æ ¼å¼ï¼šList[List[Document]]\n",
    "        â”œâ”€â”€ ç»“æœ1: query1çš„æ–‡æ¡£åˆ—è¡¨\n",
    "        â”œâ”€â”€ ç»“æœ2: query2çš„æ–‡æ¡£åˆ—è¡¨\n",
    "        â”œâ”€â”€ ç»“æœ3: query3çš„æ–‡æ¡£åˆ—è¡¨\n",
    "        â”œâ”€â”€ ç»“æœ4: query4çš„æ–‡æ¡£åˆ—è¡¨\n",
    "        â””â”€â”€ ç»“æœ5: query5çš„æ–‡æ¡£åˆ—è¡¨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab42a6de-b64b-449f-8c45-0e5670ed8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "async def parallel_multi_query(\n",
    "    queries: List[str],\n",
    "    retriever,\n",
    "    max_workers: int = 5\n",
    ") -> List[List]:\n",
    "    \"\"\"å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢\"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        tasks = [\n",
    "            loop.run_in_executor(\n",
    "                executor,\n",
    "                retriever.get_relevant_documents,\n",
    "                query\n",
    "            )\n",
    "            for query in queries\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a70585-2887-47a6-aa0c-7afa83fb2701",
   "metadata": {},
   "source": [
    "## è‡ªé€‚åº”å‚æ•°è°ƒæ•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "13912246-24c6-4a50-9b89-2f115b78e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_k_selection(question: str, base_k: int = 5) -> int:\n",
    "    \"\"\"æ ¹æ®æŸ¥è¯¢åŠ¨æ€è°ƒæ•´kå€¼\"\"\"\n",
    "    # è®¡ç®—æŸ¥è¯¢é•¿åº¦\n",
    "    word_count = len(question.split())\n",
    "    \n",
    "    if word_count < 5:\n",
    "        # çŸ­æŸ¥è¯¢éœ€è¦æ›´å¤šæ–‡æ¡£\n",
    "        return base_k + 2\n",
    "    elif word_count > 20:\n",
    "        # é•¿æŸ¥è¯¢å¯ä»¥å‡å°‘æ–‡æ¡£æ•°\n",
    "        return max(base_k - 1, 3)\n",
    "    \n",
    "    return base_k\n",
    "\n",
    "# ä½¿ç”¨\n",
    "k = adaptive_k_selection(\"ä»€ä¹ˆæ˜¯AIï¼Ÿ\")  # è¿”å›7\n",
    "k = adaptive_k_selection(\"è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ ä¸­çš„...\")  # è¿”å›4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21b954-e376-482a-82e3-acb92c2eb1bb",
   "metadata": {},
   "source": [
    "# æ¡ˆä¾‹1ï¼šæ„å»ºæ™ºèƒ½å®¢æœåŠ©æ‰‹\n",
    "\n",
    "è¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¢æœRAGç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·é—®é¢˜çš„ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ã€‚\n",
    "\n",
    "ç³»ç»Ÿé¦–å…ˆé€šè¿‡å…³é”®è¯åŒ¹é…å¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œåˆ†ç±»ï¼Œè¯†åˆ«å‡ºå››ç§ä¸»è¦åœºæ™¯ï¼š\n",
    "\n",
    "â€¢ äº§å“ä¿¡æ¯ç±»ï¼ˆåŒ…å«\"äº§å“\"ã€\"åŠŸèƒ½\"ç­‰å…³é”®è¯ï¼‰ï¼šä½¿ç”¨HyDEæŠ€æœ¯ç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆè¿›è¡Œæ£€ç´¢ï¼Œé€‚åˆæŠ€æœ¯æ€§å¼ºçš„ä¸“ä¸šé—®é¢˜\n",
    "\n",
    "â€¢ æ•…éšœæ’æŸ¥ç±»ï¼ˆåŒ…å«\"é—®é¢˜\"ã€\"é”™è¯¯\"ç­‰å…³é”®è¯ï¼‰ï¼šé‡‡ç”¨é€’å½’åˆ†è§£æŠ€æœ¯ï¼Œå°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºæ­¥éª¤åŒ–è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "â€¢ æ“ä½œæ•™ç¨‹ç±»ï¼ˆåŒ…å«\"å¦‚ä½•\"ã€\"æ€ä¹ˆ\"ç­‰å…³é”®è¯ï¼‰ï¼šè¿ç”¨Step BackæŠ€æœ¯ï¼Œå…ˆç†è§£èƒŒæ™¯çŸ¥è¯†å†æä¾›å…·ä½“æ­¥éª¤\n",
    "\n",
    "â€¢ ä¸€èˆ¬æ€§é—®é¢˜ï¼šä½¿ç”¨RAG-Fusionèåˆå¤šç§æŸ¥è¯¢ç»“æœï¼Œç¡®ä¿å›ç­”çš„å…¨é¢æ€§\n",
    "\n",
    "è¿™ç§æ™ºèƒ½è·¯ç”±æœºåˆ¶ç¡®ä¿äº†ä¸åŒç±»å‹çš„é—®é¢˜éƒ½èƒ½è·å¾—æœ€åˆé€‚çš„å¤„ç†æ–¹å¼ï¼Œå¤§å¤§æå‡äº†å®¢æœç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd6934ad-af3a-4a6d-8941-8a76356dbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerServiceRAG:\n",
    "    \"\"\"æ™ºèƒ½å®¢æœRAGç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        \n",
    "        # é¢„å®šä¹‰åœºæ™¯\n",
    "        self.scenarios = {\n",
    "            \"product_info\": [\"äº§å“\", \"åŠŸèƒ½\", \"ç‰¹æ€§\", \"ä»·æ ¼\"],\n",
    "            \"troubleshooting\": [\"é—®é¢˜\", \"é”™è¯¯\", \"ä¸å·¥ä½œ\", \"æ•…éšœ\"],\n",
    "            \"how_to\": [\"å¦‚ä½•\", \"æ€ä¹ˆ\", \"æ­¥éª¤\", \"æ•™ç¨‹\"]\n",
    "        }\n",
    "    \n",
    "    def classify_query(self, question: str) -> str:\n",
    "        \"\"\"åˆ†ç±»ç”¨æˆ·æŸ¥è¯¢\"\"\"\n",
    "        for scenario, keywords in self.scenarios.items():\n",
    "            if any(kw in question for kw in keywords):\n",
    "                return scenario\n",
    "        return \"general\"\n",
    "    \n",
    "    def handle_query(self, question: str) -> str:\n",
    "        \"\"\"å¤„ç†å®¢æœæŸ¥è¯¢\"\"\"\n",
    "        scenario = self.classify_query(question)\n",
    "        \n",
    "        if scenario == \"product_info\":\n",
    "            # äº§å“ä¿¡æ¯ï¼šä½¿ç”¨HyDEè·å¾—æ›´ä¸“ä¸šçš„ç­”æ¡ˆ\n",
    "            return HyDERAG(self.vectorstore, self.llm).query(question)\n",
    "        \n",
    "        elif scenario == \"troubleshooting\":\n",
    "            # æ•…éšœæ’æŸ¥ï¼šä½¿ç”¨åˆ†è§£è·å¾—æ­¥éª¤åŒ–ç­”æ¡ˆ\n",
    "            return RecursiveDecomposition(self.vectorstore, self.llm).query(question)\n",
    "        \n",
    "        elif scenario == \"how_to\":\n",
    "            # æ•™ç¨‹ç±»ï¼šä½¿ç”¨Step Backè·å¾—èƒŒæ™¯+æ­¥éª¤\n",
    "            return StepBackRAG(self.vectorstore, self.llm).query(question)\n",
    "        \n",
    "        else:\n",
    "            # ä¸€èˆ¬æŸ¥è¯¢ï¼šä½¿ç”¨RAG-Fusion\n",
    "            return RAGFusion(self.vectorstore, self.llm).query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc88c9c-5a70-419b-8a28-2686072909b4",
   "metadata": {},
   "source": [
    "# æ¡ˆä¾‹2ï¼šå­¦æœ¯è®ºæ–‡åŠ©æ‰‹\n",
    "\n",
    "è¿™æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡åŠ©æ‰‹RAGç³»ç»Ÿï¼Œä¸“é—¨ä¸ºå­¦æœ¯ç ”ç©¶åœºæ™¯è®¾è®¡ï¼Œé›†æˆäº†å¤šç§å…ˆè¿›çš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æ¥æ”¯æŒå­¦æœ¯å·¥ä½œã€‚\n",
    "\n",
    "ç³»ç»Ÿæä¾›äº†ä¸¤å¤§æ ¸å¿ƒåŠŸèƒ½ï¼š\n",
    "\n",
    "â€¢ æ–‡çŒ®ç»¼è¿°ç”Ÿæˆï¼šé€šè¿‡Multi-QueryæŠ€æœ¯å¹¿æ³›æ£€ç´¢ç›¸å…³ç ”ç©¶ç°çŠ¶ï¼Œç»“åˆHyDEæŠ€æœ¯æ·±å…¥æŸ¥æ‰¾ç†è®ºåŸºç¡€å’Œæ ¸å¿ƒæ¦‚å¿µï¼Œè‡ªåŠ¨ç”Ÿæˆå…¨é¢çš„æ–‡çŒ®ç»¼è¿°\n",
    "\n",
    "â€¢ æ–¹æ³•å¯¹æ¯”åˆ†æï¼šåˆ©ç”¨é€’å½’åˆ†è§£æŠ€æœ¯å°†å¤æ‚çš„æ¯”è¾ƒé—®é¢˜æ‹†è§£ä¸ºç»“æ„åŒ–å­é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯\n",
    "\n",
    "è¿™ä¸ªç³»ç»Ÿç‰¹åˆ«é€‚åˆç ”ç©¶äººå‘˜å¿«é€Ÿäº†è§£é¢†åŸŸç°çŠ¶ã€æ¢³ç†ç†è®ºæ¡†æ¶ï¼Œä»¥åŠè¿›è¡Œæ·±å…¥çš„å­¦æœ¯æ–¹æ³•æ¯”è¾ƒï¼Œå¤§å¤§æå‡äº†å­¦æœ¯ç ”ç©¶çš„æ•ˆç‡å’Œè´¨é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "218ec348-4251-4fe1-ad6a-6a2dc6d015bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcademicRAG:\n",
    "    \"\"\"å­¦æœ¯è®ºæ–‡åŠ©æ‰‹\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.multi_query = MultiQueryRAG(vectorstore, llm)\n",
    "        self.hyde = HyDERAG(vectorstore, llm, embeddings)\n",
    "        self.decomp = RecursiveDecomposition(vectorstore, llm)\n",
    "    \n",
    "    def literature_review(self, topic: str) -> str:\n",
    "        \"\"\"æ–‡çŒ®ç»¼è¿°\"\"\"\n",
    "        # 1. Multi-Queryæ‰¾å…¨é¢çš„ç›¸å…³è®ºæ–‡\n",
    "        broad_results = self.multi_query.query(\n",
    "            f\"å…³äº{topic}çš„ç ”ç©¶ç°çŠ¶å’Œä¸»è¦å‘ç°\"\n",
    "        )\n",
    "        \n",
    "        # 2. HyDEæ‰¾é«˜è´¨é‡çš„ç†è®ºè®ºæ–‡\n",
    "        theory_results = self.hyde.query(\n",
    "            f\"{topic}çš„ç†è®ºåŸºç¡€å’Œæ ¸å¿ƒæ¦‚å¿µ\"\n",
    "        )\n",
    "        \n",
    "        # 3. ç»¼åˆç”Ÿæˆæ–‡çŒ®ç»¼è¿°\n",
    "        # ...\n",
    "        \n",
    "        return review\n",
    "    \n",
    "    def compare_methods(self, method1: str, method2: str) -> str:\n",
    "        \"\"\"æ–¹æ³•å¯¹æ¯”\"\"\"\n",
    "        # ä½¿ç”¨æŸ¥è¯¢åˆ†è§£\n",
    "        comparison_query = f\"æ¯”è¾ƒ{method1}å’Œ{method2}çš„ä¼˜ç¼ºç‚¹\"\n",
    "        return self.decomp.query(comparison_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6c3d8-6249-4d25-86c3-264fce8233c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
