{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c125b828-4515-4103-bc37-84a8f4000d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥åŒ…\n",
    "import os\n",
    "# å¯¼å…¥LangChainæ ¸å¿ƒç»„ä»¶\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7a2f9-d4f8-4dc7-b5bc-72f43fd2c407",
   "metadata": {},
   "source": [
    "# æ­¥éª¤1ï¼šåŠ è½½æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87b73a-fb5a-4d5b-acd7-af4f744b1349",
   "metadata": {},
   "source": [
    "## 1.1åŠ è½½webæ–‡æ¡£\n",
    "\n",
    "ä» `load` æºç æ¥çœ‹ï¼Œè¿”å›çš„ `docs` æ˜¯ä¸€ä¸ª `Document` åˆ—è¡¨ï¼Œ  \n",
    "`Document` æœ‰ä¸¤ä¸ªå±æ€§ï¼Œåˆ†åˆ«æ˜¯ `page_content` å’Œ `metadata`ã€‚\n",
    "\n",
    "```python\n",
    "def aload(self) -> List[Document]:  # type: ignore[override]\n",
    "    \"\"\"Load text from the urls in web_path async into Documents.\"\"\"\n",
    "    results = self.scrape_all(self.web_paths)\n",
    "    docs = []\n",
    "    for path, soup in zip(self.web_paths, results):\n",
    "        text = soup.get_text(**self.bs_get_text_kwargs)\n",
    "        metadata = _build_metadata(soup, path)\n",
    "        docs.append(Document(page_content=text, metadata=metadata))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c2a591-e43e-4bda-ac15-40e9c1789092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½äº† 1 ä¸ªæ–‡æ¡£\n",
      "ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„æ ‡é¢˜ï¼šæ¨¡å‹è®­ç»ƒï¼ˆå››ï¼‰æ¢¯åº¦ç´¯è®¡Gradient Accumulation-CSDNåšå®¢\n",
      "ç¬¬ä¸€ä¸ªæ–‡æ¡£é•¿åº¦: 11642 å­—ç¬¦\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    WebBaseLoader,      # ç½‘é¡µ\n",
    "    PyPDFLoader,        # PDF\n",
    "    TextLoader,         # æ–‡æœ¬æ–‡ä»¶\n",
    "    DirectoryLoader,    # ç›®å½•\n",
    "    CSVLoader,          # CSV\n",
    ")\n",
    "\n",
    "loader = WebBaseLoader(\"https://blog.csdn.net/weixin_44919384/article/details/154616759?spm=1001.2014.3001.5501\")\n",
    "docs = loader.load()\n",
    "print(f\"åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£\")\n",
    "title = docs[0].metadata.get('title', 'N/A')\n",
    "print(f\"ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„æ ‡é¢˜ï¼š{title}\")\n",
    "print(f\"ç¬¬ä¸€ä¸ªæ–‡æ¡£é•¿åº¦: {len(docs[0].page_content)} å­—ç¬¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e9c90-fe3e-4cbd-a4b1-5844a291cb4f",
   "metadata": {},
   "source": [
    "## 1.2åŠ è½½ PDF æ–‡æ¡£\n",
    "\n",
    "ä»æºç æ¥çœ‹ï¼Œ`lazy_load` æ–¹æ³•ä¼šè¿”å›ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼ˆ`Iterator[Document]`ï¼‰ï¼Œæ¯æ¬¡è¿­ä»£ä¼šäº§å‡ºï¼ˆ`yield`ï¼‰ä¸€ä¸ª `Document` å¯¹è±¡ã€‚  \n",
    "æ¯å½“ä½ ä»è¯¥è¿­ä»£å™¨ä¸­å–ä¸€ä¸ªå…ƒç´ ï¼Œå®ƒå°±æŒ‰éœ€å¤„ç†ä¸€éƒ¨åˆ†å†…å®¹ï¼Œè¿”å›ä¸€ä¸ª `Document`ï¼Œä»è€Œå®ç°å†…å­˜é«˜æ•ˆçš„**æƒ°æ€§åŠ è½½**ï¼ˆlazy loadingï¼‰ã€‚\n",
    "\n",
    "```python\n",
    "def lazy_load(self) -> Iterator[Document]:\n",
    "    \"\"\"\n",
    "    Lazy load given path as pages.\n",
    "    Insert image, if possible, between two paragraphs.\n",
    "    In this way, a paragraph can be continued on the next page.\n",
    "    \"\"\"\n",
    "    if self.web_path:\n",
    "        blob = Blob.from_data(open(self.file_path, \"rb\").read(), path=self.web_path)\n",
    "    else:\n",
    "        blob = Blob.from_path(self.file_path)\n",
    "    yield from self.parser.lazy_parse(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79d3337-21c7-4fe5-bd79-dc6323e9e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_data: {'producer': 'TTKN', 'creator': 'ReaderEx_DIS 2.5.0 Build 4088', 'creationdate': '2025-11-03T14:24:27-08:00', 'author': 'CNKI', 'source': './Dataset/PDF/åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äººæŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶_é—«è…¾.pdf', 'total_pages': 79, 'page': 0, 'page_label': '1'}\n",
      "content: ç¡•å£«å­¦ä½è®º æ–‡\n",
      "å­¦ä½ç”³è¯·äººå§“å é—«è…¾\n",
      "å­¦ä½ç”³è¯·äººå­¦å· 2200411007\n",
      "ä¸“ ä¸š å ç§° æœºæ¢°å·¥ç¨‹\n",
      "å­¦ ç§‘ é—¨ ç±» å·¥å­¦\n",
      "å­¦é™¢ï¼ˆéƒ¨ã€ç ”ç©¶é™¢ï¼‰ åº”ç”¨æŠ€æœ¯å­¦é™¢\n",
      "å¯¼ å¸ˆ å§“ å ææ–‡è´¤\n",
      "äºŒã€‡äºŒäº”å¹´å…­æœˆ\n",
      "åˆ†ç±»å· å­¦æ ¡ä»£ç  10590\n",
      "UDC å¯† çº§ å…¬å¼€\n",
      "åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äºº\n",
      "æŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶\n",
      "à´§áªÕ¶à¿\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./Dataset/PDF/åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äººæŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶_é—«è…¾.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# æ‰“å°æ–¹å¼1ï¼š\n",
    "# for doc in docs:\n",
    "#     print(doc.page_content)\n",
    "\n",
    "# æ‰“å°æ–¹å¼2ï¼š\n",
    "first_doc = docs[0]\n",
    "print(\"meta_data:\",first_doc.metadata)\n",
    "print(\"content:\",first_doc.page_content)\n",
    "# second_doc = docs[1]\n",
    "# print(\"meta_data:\",first_doc.metadata)\n",
    "# print(\"content:\",first_doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f98179-4a90-4b61-8f33-55420a8e4690",
   "metadata": {},
   "source": [
    "# æ­¥éª¤2ï¼šæ–‡æœ¬åˆ†å—\n",
    "\n",
    "ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ\n",
    "\n",
    "- LLMä¸Šä¸‹æ–‡é™åˆ¶ï¼šå¤§å¤šæ•°LLMæœ‰æœ€å¤§tokené™åˆ¶ï¼ˆå¦‚GPT-4çš„8K/32Kï¼‰\n",
    "\n",
    "- æé«˜æ£€ç´¢ç²¾åº¦ï¼šå°å—æ–‡æœ¬æ›´å®¹æ˜“åŒ¹é…ç‰¹å®šæŸ¥è¯¢\n",
    "\n",
    "- é™ä½æˆæœ¬ï¼šåªæ£€ç´¢å’Œå¤„ç†ç›¸å…³éƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02027626-cede-43ad-8e42-c2d473c8060a",
   "metadata": {},
   "source": [
    "## 2.1åˆ›å»ºæ–‡æœ¬åˆ†å—å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00752ce8-6135-41bf-a67d-c343039bcd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ–‡æ¡£: 79 ä¸ª\n",
      "åˆ†å—å: 99 ä¸ª\n",
      "\n",
      "ç¬¬ä¸€ä¸ªåˆ†å—ç¤ºä¾‹:\n",
      "ç¡•å£«å­¦ä½è®º æ–‡\n",
      "å­¦ä½ç”³è¯·äººå§“å é—«è…¾\n",
      "å­¦ä½ç”³è¯·äººå­¦å· 2200411007\n",
      "ä¸“ ä¸š å ç§° æœºæ¢°å·¥ç¨‹\n",
      "å­¦ ç§‘ é—¨ ç±» å·¥å­¦\n",
      "å­¦é™¢ï¼ˆéƒ¨ã€ç ”ç©¶é™¢ï¼‰ åº”ç”¨æŠ€æœ¯å­¦é™¢\n",
      "å¯¼ å¸ˆ å§“ å ææ–‡è´¤\n",
      "äºŒã€‡äºŒäº”å¹´å…­æœˆ\n",
      "åˆ†ç±»å· å­¦æ ¡ä»£ç  10590\n",
      "UDC å¯† çº§ å…¬å¼€\n",
      "åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äºº\n",
      "æŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶\n",
      "à´§áªÕ¶à¿...\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºæ–‡æœ¬åˆ†å—å™¨\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # æ¯å—æœ€å¤§å­—ç¬¦æ•°\n",
    "    chunk_overlap=200,      # å—ä¹‹é—´çš„é‡å \n",
    "    length_function=len,    # é•¿åº¦è®¡ç®—å‡½æ•°\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# åˆ†å—æ–‡æ¡£\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"åŸå§‹æ–‡æ¡£: {len(docs)} ä¸ª\")\n",
    "print(f\"åˆ†å—å: {len(splits)} ä¸ª\")\n",
    "print(f\"\\nç¬¬ä¸€ä¸ªåˆ†å—ç¤ºä¾‹:\\n{splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8cdc8-14b7-4532-a20e-9fc41dc3884e",
   "metadata": {},
   "source": [
    "## 2.2åˆ†å—ç­–ç•¥å¯¹æ¯”\n",
    "\n",
    "å­—ç¬¦åˆ†å—ï¼šç®€å•å¿«ï¼Œä½†æ˜“åˆ‡æ–­å¥å­ï¼Œè¯­ä¹‰å·®ã€‚\n",
    "\n",
    "é€’å½’åˆ†å—ï¼šä¼˜å…ˆæŒ‰æ®µè½/æ¢è¡Œåˆ‡ï¼Œå…¼é¡¾è¯­ä¹‰ä¸æ•ˆç‡ï¼Œæ¨èé»˜è®¤ä½¿ç”¨ã€‚\n",
    "\n",
    "Tokenåˆ†å—ï¼šç²¾å‡†æ§åˆ¶LLMè¾“å…¥é•¿åº¦ï¼Œä½†ä¾èµ–tokenizerï¼Œé€Ÿåº¦è¾ƒæ…¢ã€‚\n",
    "\n",
    "è¯­ä¹‰åˆ†å—ï¼šæŒ‰è¯­ä¹‰è¾¹ç•Œåˆ‡ï¼Œè´¨é‡é«˜ä½†è®¡ç®—å¼€é”€å¤§ï¼Œé€‚åˆé«˜è¦æ±‚åœºæ™¯ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16dbb48b-f951-4004-abc8-016043f8ec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å­—ç¬¦åˆ†å—æ•°é‡: 79\n",
      "ç¬¬1å—é¢„è§ˆ:\n",
      " ç¡•å£«å­¦ä½è®º æ–‡\n",
      "å­¦ä½ç”³è¯·äººå§“å é—«è…¾\n",
      "å­¦ä½ç”³è¯·äººå­¦å· 2200411007\n",
      "ä¸“ ä¸š å ç§° æœºæ¢°å·¥ç¨‹\n",
      "å­¦ ç§‘ é—¨ ç±» å·¥å­¦\n",
      "å­¦é™¢ï¼ˆéƒ¨ã€ç ”ç©¶é™¢ï¼‰ åº”ç”¨æŠ€æœ¯å­¦é™¢\n",
      "å¯¼ å¸ˆ å§“ å ææ–‡è´¤\n",
      "äºŒã€‡äºŒäº”å¹´å…­æœˆ\n",
      "åˆ†ç±»å· å­¦æ ¡ä»£ç  10590\n",
      "UDC å¯† çº§ å…¬å¼€\n",
      "åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äºº\n",
      "æŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶\n",
      "à´§áªÕ¶à¿ \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 1. å­—ç¬¦åˆ†å—ï¼ˆæœ€åŸºç¡€ï¼‰\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"å­—ç¬¦åˆ†å—æ•°é‡: {len(chunks)}\")\n",
    "print(\"ç¬¬1å—é¢„è§ˆ:\\n\", chunks[0].page_content[:300], \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31383952-2c9c-4d7d-9af0-73c11b96dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é€’å½’åˆ†å—æ•°é‡: 99\n",
      "ç¬¬1å—é¢„è§ˆ:\n",
      " ç¡•å£«å­¦ä½è®º æ–‡\n",
      "å­¦ä½ç”³è¯·äººå§“å é—«è…¾\n",
      "å­¦ä½ç”³è¯·äººå­¦å· 2200411007\n",
      "ä¸“ ä¸š å ç§° æœºæ¢°å·¥ç¨‹\n",
      "å­¦ ç§‘ é—¨ ç±» å·¥å­¦\n",
      "å­¦é™¢ï¼ˆéƒ¨ã€ç ”ç©¶é™¢ï¼‰ åº”ç”¨æŠ€æœ¯å­¦é™¢\n",
      "å¯¼ å¸ˆ å§“ å ææ–‡è´¤\n",
      "äºŒã€‡äºŒäº”å¹´å…­æœˆ\n",
      "åˆ†ç±»å· å­¦æ ¡ä»£ç  10590\n",
      "UDC å¯† çº§ å…¬å¼€\n",
      "åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äºº\n",
      "æŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶\n",
      "à´§áªÕ¶à¿ \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 2. é€’å½’åˆ†å—ï¼ˆæ¨èï¼Œæ™ºèƒ½è¯†åˆ«æ®µè½\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"é€’å½’åˆ†å—æ•°é‡: {len(chunks)}\")\n",
    "print(\"ç¬¬1å—é¢„è§ˆ:\\n\", chunks[0].page_content[:300], \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bfbf958-beb2-4f4a-94fa-cea2dc0c05a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenåˆ†å—æ•°é‡: 542\n",
      "ç¬¬1å—é¢„è§ˆï¼ˆå‰100å­—ç¬¦ï¼‰:\n",
      " ç¡•å£«å­¦ä½è®º æ–‡\n",
      "å­¦ä½ç”³è¯·äººå§“å é—«è…¾\n",
      "å­¦ä½ç”³è¯·äººå­¦å· 2200411007\n",
      "ä¸“ ä¸š å ç§° æœºæ¢°å·¥ç¨‹\n",
      "å­¦ ç§‘ é—¨ ç±» å·¥å­¦\n",
      "å­¦é™¢ï¼ˆéƒ¨ã€ç ”ç©¶é™¢ï¼‰ åº”ç”¨æŠ€æœ¯å­¦é™¢\n",
      "å¯¼ å¸ˆ å§“ å ææ–‡è´¤\n",
      "äºŒã€‡äºŒäº”å¹´å…­æœˆ\n",
      "åˆ† \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenåˆ†å—ï¼ˆç²¾ç¡®æ§åˆ¶tokenæ•°é‡ï¼‰\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Tokenåˆ†å—æ•°é‡: {len(chunks)}\")\n",
    "print(\"ç¬¬1å—é¢„è§ˆï¼ˆå‰100å­—ç¬¦ï¼‰:\\n\", chunks[0].page_content[:100], \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f827129-a686-417e-9e3a-3b579a470f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. è¯­ä¹‰åˆ†å—ï¼ˆåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# éœ€è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"è¯­ä¹‰åˆ†å—æ•°é‡: {len(chunks)}\")\n",
    "print(\"ç¬¬1å—é¢„è§ˆ:\\n\", chunks[0].page_content[:300], \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993161f-6745-4282-b933-dd37af9f8d6a",
   "metadata": {},
   "source": [
    "# æ­¥éª¤3ï¼šå‘é‡åŒ–\n",
    "\n",
    "å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­¦å‘é‡ï¼Œè¿™æ˜¯è¯­ä¹‰æœç´¢çš„æ ¸å¿ƒã€‚\n",
    "\n",
    "å¸¸ç”¨çš„åµŒå…¥æ¨¡å‹å¯¹æ¯”ï¼š\n",
    "\n",
    "| æ¨¡å‹ | æä¾›å•† | ç»´åº¦ | æˆæœ¬ | æ€§èƒ½ | ç‰¹ç‚¹ |\n",
    "|------|--------|------|------|------|------|\n",
    "| text-embedding-3-small | OpenAI | 1536 | \\$ | é«˜æ€§ä»·æ¯” â­ | é€šç”¨åœºæ™¯ï¼Œå¹³è¡¡æ€§èƒ½ |\n",
    "| text-embedding-3-large | OpenAI | 3072 | \\$\\$ | æœ€é«˜è´¨é‡ | è¦æ±‚æœ€é«˜çš„åº”ç”¨åœºæ™¯ |\n",
    "| text-embedding-ada-002 | OpenAI | 1536 | \\$ | ä¸Šä¸€ä»£ | å…¼å®¹æ—§ç³»ç»Ÿ |\n",
    "| all-MiniLM-L6-v2 | HuggingFace | 384 | å…è´¹ | æœ¬åœ°éƒ¨ç½² | è½»é‡çº§ï¼Œå¿«é€Ÿæ¨ç† |\n",
    "| bce-embedding-base_v1 | ç™¾åº¦ | 768 | å…è´¹ | ä¸­æ–‡ä¼˜åŒ– â­ | ä¸­æ–‡ä»»åŠ¡è¡¨ç°ä¼˜å¼‚ |\n",
    "| bce-reranker-base_v1 | ç™¾åº¦ | - | å…è´¹ | é‡æ’åºä¸“ç”¨ | æå‡æ£€ç´¢ç²¾åº¦ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7cc3f8b-83e3-4ad9-993f-3dfdd95098c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 20:03:03,135 - modelscope - INFO - PyTorch version 2.3.0+cu118 Found.\n",
      "2025-11-11 20:03:03,136 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-11-11 20:03:03,293 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 298ceecce207285dd10b135af16e71cc and a total number of 964 components indexed\n",
      "2025-11-11 20:03:04,192 - modelscope - WARNING - Model revision not specified, use revision: v0.0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æœ¬: RAGæ˜¯ä¸€ç§å¼ºå¤§çš„AIæŠ€æœ¯\n",
      "å‘é‡ç»´åº¦: 768\n",
      "å‘é‡å‰5ä¸ªå€¼: [0.0048, 0.0216, -0.005, 0.02, -0.0113]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from modelscope import snapshot_download\n",
    "import os\n",
    "\n",
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹å­˜å‚¨ç›®å½•\n",
    "local_models_dir = \"./Models\" \n",
    "os.makedirs(local_models_dir, exist_ok=True)  \n",
    "\n",
    "# 1. ä» ModelScope ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šç›®å½•\n",
    "model_id = \"maidalun/bce-embedding-base_v1\"\n",
    "local_model_path = snapshot_download(\n",
    "    model_id,\n",
    "    cache_dir=local_models_dir \n",
    ")\n",
    "\n",
    "# 2. åˆ›å»ºåµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨è¯¥æœ¬åœ°è·¯å¾„ï¼‰\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=local_model_path,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 3. æµ‹è¯•å‘é‡åŒ–\n",
    "text = \"RAGæ˜¯ä¸€ç§å¼ºå¤§çš„AIæŠ€æœ¯\"\n",
    "vector = embeddings.embed_query(text)\n",
    "\n",
    "print(f\"æ–‡æœ¬: {text}\")\n",
    "print(f\"å‘é‡ç»´åº¦: {len(vector)}\")\n",
    "print(f\"å‘é‡å‰5ä¸ªå€¼: {[round(x, 4) for x in vector[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45fa5b-fad4-45a2-a513-64bdfa3bdbb8",
   "metadata": {},
   "source": [
    "### å‘é‡ç›¸ä¼¼åº¦è®¡ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "014436cd-3007-44dd-a9e9-f9fdaf0b8d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã€Œè‹¹æœæ˜¯ä¸€ç§æ°´æœã€ vs ã€Œé¦™è•‰æ˜¯ä¸€ç§æ°´æœã€ ç›¸ä¼¼åº¦: 0.7481\n",
      "ã€Œè‹¹æœæ˜¯ä¸€ç§æ°´æœã€ vs ã€Œè‹¹æœæ˜¯ä¸€ç§å¥½åƒçš„æ°´æœã€ ç›¸ä¼¼åº¦: 0.9040\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "text1 = \"è‹¹æœæ˜¯ä¸€ç§æ°´æœ\"\n",
    "text2 = \"é¦™è•‰æ˜¯ä¸€ç§æ°´æœ\"\n",
    "text3 = \"è‹¹æœæ˜¯ä¸€ç§å¥½åƒçš„æ°´æœ\"\n",
    "\n",
    "vector1 = embeddings.embed_query(text1)\n",
    "vector2 = embeddings.embed_query(text2)  \n",
    "vector3 = embeddings.embed_query(text3) \n",
    "\n",
    "# è½¬ä¸º NumPy æ•°ç»„å¹¶ reshape ä¸º (1, dim)\n",
    "v1 = np.array(vector1).reshape(1, -1)\n",
    "v2 = np.array(vector2).reshape(1, -1)\n",
    "v3 = np.array(vector3).reshape(1, -1)\n",
    "\n",
    "# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "sim_1_2 = cosine_similarity(v1, v2)[0][0]\n",
    "sim_1_3 = cosine_similarity(v1, v3)[0][0]\n",
    "\n",
    "print(f\"ã€Œ{text1}ã€ vs ã€Œ{text2}ã€ ç›¸ä¼¼åº¦: {sim_1_2:.4f}\")\n",
    "print(f\"ã€Œ{text1}ã€ vs ã€Œ{text3}ã€ ç›¸ä¼¼åº¦: {sim_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b404206-362c-4536-9c00-e5dc10dd8da6",
   "metadata": {},
   "source": [
    "# æ­¥éª¤4ï¼šå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“\n",
    "\n",
    "| æ•°æ®åº“ | ç±»å‹ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |\n",
    "|:-------|:----:|:-----|:---------|\n",
    "| Chroma | åµŒå…¥å¼ | ç®€å•æ˜“ç”¨ï¼Œæ— éœ€é¢å¤–æœåŠ¡ | å¼€å‘ã€å°è§„æ¨¡åº”ç”¨ â­ |\n",
    "| Pinecone | äº‘æœåŠ¡ | é«˜æ€§èƒ½ï¼Œæ‰˜ç®¡æœåŠ¡ | ç”Ÿäº§ç¯å¢ƒ |\n",
    "| Weaviate | è‡ªå»º | åŠŸèƒ½ä¸°å¯Œï¼Œå¼€æº | å¤§è§„æ¨¡éƒ¨ç½² |\n",
    "| FAISS | åº“ | é€Ÿåº¦å¿«ï¼ŒMetaå¼€æº | ç ”ç©¶å’ŒåŸå‹ |\n",
    "\n",
    "å°†./Dataset/PDFç›®å½•ä¸‹çš„æ‰€æœ‰PDFæ–‡ä»¶éƒ½å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0fe613-50f8-4dfb-9004-f7241d6226a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æ‰¾åˆ° 10 ä¸ªPDFæ–‡ä»¶:\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç….pdf\n",
      "âœ… åŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç….pdf: 81é¡µ -> 1234ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™.pdf\n",
      "âœ… åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™.pdf: 182é¡µ -> 3905ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x35029d for key /MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±.pdf: 99é¡µ -> 1335ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§¦è§†è§‰èåˆçš„æœºå™¨äººç›®æ ‡è¯†åˆ«å’Œæ»‘åŠ¨æ£€æµ‹ç ”ç©¶_æ—ä¸ºæ¢.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x74db3a for key /MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åŸºäºè§¦è§†è§‰èåˆçš„æœºå™¨äººç›®æ ‡è¯†åˆ«å’Œæ»‘åŠ¨æ£€æµ‹ç ”ç©¶_æ—ä¸ºæ¢.pdf: 71é¡µ -> 1074ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n",
      "âœ… åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf: 72é¡µ -> 920ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§†è§¦è§‰èåˆçš„æœºæ¢°æ‰‹åˆ†ç±»æŠ“å–æ–¹æ³•ç ”ç©¶_ä½™èˆª.pdf\n",
      "âœ… åŸºäºè§†è§¦è§‰èåˆçš„æœºæ¢°æ‰‹åˆ†ç±»æŠ“å–æ–¹æ³•ç ”ç©¶_ä½™èˆª.pdf: 67é¡µ -> 780ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§†è§¦è§‰èåˆçš„æœºå™¨äººç‰©ä½“è¯†åˆ«å’ŒæŠ“å–ç¨³å®šæ€§æ£€æµ‹çš„ç ”ç©¶ä¸åº”ç”¨_ä¸Šå®˜æ˜é›¨.pdf\n",
      "âœ… åŸºäºè§†è§¦è§‰èåˆçš„æœºå™¨äººç‰©ä½“è¯†åˆ«å’ŒæŠ“å–ç¨³å®šæ€§æ£€æµ‹çš„ç ”ç©¶ä¸åº”ç”¨_ä¸Šå®˜æ˜é›¨.pdf: 76é¡µ -> 926ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: è§†è§¦è§‰èåˆä¸‹çš„æœºå™¨äººæŠ“å–æ»‘åŠ¨æ£€æµ‹ç ”ç©¶_é»„å…†åŸº.pdf\n",
      "âœ… è§†è§¦è§‰èåˆä¸‹çš„æœºå™¨äººæŠ“å–æ»‘åŠ¨æ£€æµ‹ç ”ç©¶_é»„å…†åŸº.pdf: 75é¡µ -> 562ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äººæŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶_é—«è…¾.pdf\n",
      "âœ… åŸºäºè§†-è§¦è§‰èåˆæ„ŸçŸ¥çš„æœºå™¨äººæŠ“å–æ»‘åŠ¨æ£€æµ‹ä¸åŠ›æ§ç ”ç©¶_é—«è…¾.pdf: 79é¡µ -> 921ä¸ªåˆ†å—\n",
      "ğŸ“– æ­£åœ¨åŠ è½½: é’ˆå¯¹æœºå™¨æ‰‹ç¨³å®šæŠ“å–çš„è§†è§¦èåˆç®—æ³•ç ”ç©¶_é—«å‡¯æ³¢.pdf\n",
      "âœ… é’ˆå¯¹æœºå™¨æ‰‹ç¨³å®šæŠ“å–çš„è§†è§¦èåˆç®—æ³•ç ”ç©¶_é—«å‡¯æ³¢.pdf: 81é¡µ -> 1072ä¸ªåˆ†å—\n",
      "ğŸ‰ æ€»å…±åŠ è½½ 12729 ä¸ªæ–‡æœ¬åˆ†å—ï¼ˆå‡å·²æ·»åŠ  idï¼‰\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_all_pdfs_from_directory(directory_path=\"./Dataset/PDF/\"):\n",
    "    \"\"\"åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰PDFæ–‡ä»¶ï¼Œå¹¶ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ å”¯ä¸€ID\"\"\"\n",
    "    all_splits = []\n",
    "    pdf_files = []\n",
    "    \n",
    "    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"âŒ ç›®å½•ä¸å­˜åœ¨: {directory_path}\")\n",
    "        return all_splits, pdf_files\n",
    "    \n",
    "    # è·å–æ‰€æœ‰PDFæ–‡ä»¶\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_files.append(filename)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"âš ï¸  ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶: {directory_path}\")\n",
    "        return all_splits, pdf_files\n",
    "    \n",
    "    print(f\"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶:\")\n",
    "    \n",
    "    # åˆ›å»ºæ–‡æœ¬åˆ†å—å™¨\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    # å…¨å±€åˆ†å—è®¡æ•°å™¨ï¼ˆç”¨äºç”Ÿæˆå…¨å±€å”¯ä¸€IDï¼‰\n",
    "    global_chunk_index = 0\n",
    "    \n",
    "    # é€ä¸ªåŠ è½½PDFæ–‡ä»¶\n",
    "    for filename in pdf_files:\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        base_name = os.path.splitext(filename)[0]  # å»æ‰ .pdf åç¼€\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“– æ­£åœ¨åŠ è½½: {filename}\")\n",
    "            \n",
    "            # åŠ è½½PDF\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs = loader.load()  # æ¯ä¸ª doc å¯¹åº”ä¸€é¡µï¼Œmetadata åŒ…å« page\n",
    "            \n",
    "            # åˆ†å—å¤„ç†\n",
    "            splits = text_splitter.split_documents(docs)\n",
    "            \n",
    "            # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ å”¯ä¸€ ID å’Œå¢å¼ºå…ƒæ•°æ®\n",
    "            for i, split in enumerate(splits):\n",
    "                # ä½¿ç”¨ æ–‡ä»¶å + é¡µç  + åˆ†å—åºå·ï¼ˆæ¨èï¼‰\n",
    "                page_num = split.metadata.get('page', 'unknown')\n",
    "                split_id = f\"{base_name}_p{page_num}_c{i}\"\n",
    "                 \n",
    "                # æ·»åŠ  ID åˆ° metadata\n",
    "                split.metadata['id'] = split_id\n",
    "                split.metadata['source_file'] = filename  # å¯é€‰ï¼šä¿ç•™åŸå§‹æ–‡ä»¶å\n",
    "                \n",
    "                all_splits.append(split)\n",
    "            \n",
    "            print(f\"âœ… {filename}: {len(docs)}é¡µ -> {len(splits)}ä¸ªåˆ†å—\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åŠ è½½å¤±è´¥ {filename}: {str(e)}\")\n",
    "    \n",
    "    print(f\"ğŸ‰ æ€»å…±åŠ è½½ {len(all_splits)} ä¸ªæ–‡æœ¬åˆ†å—ï¼ˆå‡å·²æ·»åŠ  idï¼‰\")\n",
    "    return all_splits, pdf_files\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "splits, loaded_files = load_all_pdfs_from_directory(\"./Dataset/PDF/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e1ba5-eb74-48ba-9a31-2f527382be56",
   "metadata": {},
   "source": [
    "åœ¨ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ä¸­æ—¶å‡ºç°æŠ¥é”™UnicodeEncodeError: 'utf-8' codec can't encode characters in position 795-798: surrogates not allowedï¼Œè¯´æ˜æ–‡æ¡£å†…å®¹ï¼ˆsplits ä¸­çš„æŸæ®µæ–‡æœ¬ï¼‰åŒ…å«éæ³• Unicode å­—ç¬¦ï¼ˆsurrogate charactersï¼‰ï¼Œè¿™äº›å­—ç¬¦æ— æ³•è¢« UTF-8 ç¼–ç ï¼Œè€Œ ChromaDBï¼ˆåº•å±‚ä½¿ç”¨ Rustï¼‰è¦æ±‚æ‰€æœ‰å­—ç¬¦ä¸²å¿…é¡»æ˜¯åˆæ³•çš„ UTF-8ã€‚å› æ­¤åœ¨ä¿å­˜ä¹‹å‰éœ€è¦è¿›è¡Œæ¸…æ´—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1cfef9-7d8d-498a-903c-c86c8676f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    return text.encode('utf-8', errors='ignore').decode('utf-8').strip('\\n').strip()\n",
    "\n",
    "# æ¸…æ´—æ‰€æœ‰æ–‡æ¡£å—çš„ page_content\n",
    "for doc in splits:\n",
    "    doc.page_content = clean_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5d78dc-b73a-49cf-a93b-cdad036a71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_vectorstore_creation(documents, embeddings, persist_directory=\"./chroma_db\", collection_name=\"langchain\"):\n",
    "    \"\"\"æ™ºèƒ½å‘é‡æ•°æ®åº“åˆ›å»ºï¼ˆæ¨èä½¿ç”¨ï¼‰\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”§ å¤„ç†é›†åˆ: {collection_name}\")\n",
    "    \n",
    "    # ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # å°è¯•è¿æ¥åˆ°ç°æœ‰é›†åˆ\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # æ£€æŸ¥é›†åˆæ˜¯å¦ä¸ºç©º\n",
    "        collection_info = vectorstore.get()\n",
    "        if collection_info['ids']:\n",
    "            print(f\"ğŸ—‘ï¸ æ¸…ç†é›†åˆä¸­çš„ {len(collection_info['ids'])} ä¸ªæ–‡æ¡£\")\n",
    "            vectorstore.delete(ids=collection_info['ids'])\n",
    "        else:\n",
    "            print(\"âœ… é›†åˆä¸ºç©ºï¼Œæ— éœ€æ¸…ç†\")\n",
    "            \n",
    "    except Exception:\n",
    "        # é›†åˆä¸å­˜åœ¨ï¼Œå°†åœ¨åç»­åˆ›å»º\n",
    "        print(f\"ğŸ†• é›†åˆä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°é›†åˆ\")\n",
    "        vectorstore = None\n",
    "    \n",
    "    # åˆ†æ‰¹æ·»åŠ æ–‡æ¡£\n",
    "    total_docs = len(documents)\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for i in range(0, total_docs, batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        total_batches = (total_docs - 1) // batch_size + 1\n",
    "        \n",
    "        print(f\"ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch)} ä¸ªæ–‡æ¡£\")\n",
    "        \n",
    "        if vectorstore is None and i == 0:\n",
    "            # ç¬¬ä¸€æ‰¹ä¸”é›†åˆä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°é›†åˆ\n",
    "            vectorstore = Chroma.from_documents(\n",
    "                documents=batch,\n",
    "                embedding=embeddings,\n",
    "                persist_directory=persist_directory,\n",
    "                collection_name=collection_name\n",
    "            )\n",
    "        else:\n",
    "            # åç»­æ‰¹æ¬¡æˆ–é›†åˆå·²å­˜åœ¨ï¼Œæ·»åŠ åˆ°ç°æœ‰é›†åˆ\n",
    "            vectorstore.add_documents(batch)\n",
    "    \n",
    "    print(f\"âœ… å®Œæˆï¼é›†åˆ '{collection_name}' ç°æœ‰ {total_docs} ä¸ªæ–‡æ¡£\")\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8849189-46cb-45c0-9747-cf80ecd708c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ å¤„ç†é›†åˆ: langchain\n",
      "âœ… é›†åˆä¸ºç©ºï¼Œæ— éœ€æ¸…ç†\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 1/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 2/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 3/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 4/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 5/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 6/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 7/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 8/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 9/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 10/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 11/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 12/13: 1000 ä¸ªæ–‡æ¡£\n",
      "ğŸ“¦ æ·»åŠ æ‰¹æ¬¡ 13/13: 729 ä¸ªæ–‡æ¡£\n",
      "âœ… å®Œæˆï¼é›†åˆ 'langchain' ç°æœ‰ 12729 ä¸ªæ–‡æ¡£\n"
     ]
    }
   ],
   "source": [
    "vectorstore = smart_vectorstore_creation(splits, embeddings, collection_name=\"langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b05a9-c8ee-4678-a63b-e074bf70a92c",
   "metadata": {},
   "source": [
    "# æ­¥éª¤5ï¼šæ£€ç´¢\n",
    "\n",
    "æ£€ç´¢ç±»å‹ï¼š\n",
    "\n",
    "1. ç›¸ä¼¼åº¦æœç´¢ (Similarity Search)ï¼šæœ€åŸºç¡€çš„æœç´¢æ–¹å¼ï¼Œè¿”å›ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "search_type=\"similarity\",\n",
    "search_kwargs={\"k\": 5} # è¿”å› top-5\n",
    ")\n",
    "```\n",
    "\n",
    "2. æœ€å¤§è¾¹é™…ç›¸å…³æ€§ (MMR)ï¼šå¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼Œé¿å…è¿”å›è¿‡äºç›¸ä¼¼çš„ç»“æœã€‚\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "search_type=\"mmr\",\n",
    "search_kwargs={\n",
    "\"k\": 5,\n",
    "\"fetch_k\": 20, # å…ˆè·å– 20 ä¸ªå€™é€‰\n",
    "\"lambda_mult\": 0.5 # å¤šæ ·æ€§å‚æ•° (0=æœ€å¤šæ ·, 1=æœ€ç›¸å…³)\n",
    "}\n",
    ")\n",
    "```\n",
    "\n",
    "    MMR å·¥ä½œåŸç†ï¼š\n",
    "    \n",
    "    æŸ¥è¯¢: \"æœºå™¨å­¦ä¹ ç®—æ³•\"\n",
    "    \n",
    "    æ­¥éª¤1: è·å– top-20 ç›¸ä¼¼æ–‡æ¡£\n",
    "        1. \"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯...\" (ç›¸ä¼¼åº¦: 0.95)\n",
    "        2. \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ...\" (ç›¸ä¼¼åº¦: 0.94) â† ä¸ç¬¬1ä¸ªå¾ˆç›¸ä¼¼\n",
    "        3. \"å†³ç­–æ ‘æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ç®—æ³•...\" (ç›¸ä¼¼åº¦: 0.90)\n",
    "        4. \"æ”¯æŒå‘é‡æœº(SVM)ç”¨äºåˆ†ç±»...\" (ç›¸ä¼¼åº¦: 0.88)\n",
    "    ...\n",
    "    \n",
    "    æ­¥éª¤2: MMR é€‰æ‹© (k=3, lambda=0.5)\n",
    "    é€‰ä¸­: #1 (æœ€ç›¸å…³)\n",
    "    é€‰ä¸­: #3 (ç›¸å…³ä¸”ä¸ #1 ä¸åŒ) â† è·³è¿‡ #2 å› ä¸ºå¤ªç›¸ä¼¼\n",
    "    é€‰ä¸­: #4 (å¢åŠ å¤šæ ·æ€§)\n",
    "\n",
    "3. ç›¸ä¼¼åº¦é˜ˆå€¼ (Similarity Score Threshold)ï¼šåªè¿”å›ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "search_type=\"similarity_score_threshold\",\n",
    "search_kwargs={\n",
    "\"score_threshold\": 0.5, # åªè¿”å›ç›¸ä¼¼åº¦ > 0.5 çš„ç»“æœ\n",
    "\"k\": 5\n",
    "}\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "| ç­–ç•¥ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| Similarity | ç®€å•å¿«é€Ÿ | å¯èƒ½é‡å¤ | é»˜è®¤é€‰æ‹© |\n",
    "| MMR | ç»“æœå¤šæ · | ç¨æ…¢ | éœ€è¦ä¸åŒè§’åº¦ä¿¡æ¯ |\n",
    "| Threshold | è´¨é‡ä¿è¯ | å¯èƒ½æ— ç»“æœ | ä¸¥æ ¼åŒ¹é…éœ€æ±‚ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd2ff2-67b0-4068-a9cd-70cb3ceede24",
   "metadata": {},
   "source": [
    "æ£€ç´¢çš„ç»“æœä¹Ÿä¼šå‡ºç°æ–‡æœ¬åŒ…å«å¤§é‡ä¹±ç /æ’ç‰ˆç¬¦å·ï¼ˆå¦‚ î—, î—’î—, æ¢è¡Œæ··ä¹±ç­‰ï¼‰ï¼Œè¿™æ˜¯ PDF è§£ææ—¶å¸¸è§çš„é—®é¢˜ï¼Œæ‰€ä»¥ä¹Ÿéœ€è¦è¿›è¡Œæ–‡æœ¬æ¸…ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5c87fe-9213-4069-8104-93e403f0af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼\n",
      "   å·²å­˜åœ¨ 12721 ä¸ªæ–‡æ¡£å—\n",
      "   åŠ è½½è·¯å¾„: ./chroma_db\n",
      "   Collection åç§°: langchain\n",
      "   ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼: æœªæŒ‡å®šï¼ˆé»˜è®¤é€šå¸¸ä¸º cosineï¼‰\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ‰€æœ‰ä¸­æ–‡å­—ç¬¦ä¹‹é—´çš„ä»»æ„ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬æ™®é€šç©ºæ ¼ã€å…¨è§’ç©ºæ ¼ã€ä¸é—´æ–­ç©ºæ ¼ç­‰ï¼‰\n",
    "    åŒæ—¶ä¿ç•™è‹±æ–‡/æ•°å­—é—´çš„æ­£å¸¸ç©ºæ ¼ã€‚\n",
    "    \"\"\"\n",
    "    # åŒ¹é…ï¼šä¸­æ–‡ + (ä»»æ„ç©ºç™½åºåˆ—) + ä¸­æ–‡ â†’ æ›¿æ¢ä¸ºä¸¤ä¸ªä¸­æ–‡ç›´æ¥ç›¸è¿\n",
    "    # \\u4e00-\\u9fff: åŸºæœ¬æ±‰å­—\n",
    "    # \\u3400-\\u4dbf: æ‰©å±•A\n",
    "    # \\u20000-\\u2a6df: æ‰©å±•Bï¼ˆä½†Python reä¸æ”¯æŒ4å­—èŠ‚ï¼Œå¯å¿½ç•¥æˆ–ç”¨ä»£ç†ï¼‰\n",
    "    chinese_char = r'[\\u4e00-\\u9fff]'\n",
    "    # åŒ¹é…ä»»æ„ç©ºç™½ï¼ˆåŒ…æ‹¬å…¨è§’ã€ä¸é—´æ–­ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰\n",
    "    any_whitespace = r'[\\s\\u00A0\\u2000-\\u200F\\u2028-\\u202F\\u3000]+'\n",
    "    \n",
    "    pattern = f'({chinese_char}){any_whitespace}(?={chinese_char})'\n",
    "    # ä½¿ç”¨æ­£å‘å…ˆè¡Œæ–­è¨€ï¼Œé¿å…åƒæ‰åé¢çš„ä¸­æ–‡\n",
    "    result = re.sub(pattern, r'\\1', text)\n",
    "    \n",
    "    # é¢å¤–ï¼šæ¸…ç†è¿ç»­å¤šä½™ç©ºæ ¼ï¼ˆéä¸­æ–‡åŒºåŸŸï¼‰\n",
    "    result = re.sub(r'\\s+', ' ', result)\n",
    "    return result.strip()\n",
    "\n",
    "# æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(\"./chroma_db\"):\n",
    "    # åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    collection = vectorstore._collection\n",
    "    doc_count = collection.count()\n",
    "    \n",
    "    print(f\"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼\")\n",
    "    print(f\"   å·²å­˜åœ¨ {doc_count} ä¸ªæ–‡æ¡£å—\")\n",
    "    print(f\"   åŠ è½½è·¯å¾„: ./chroma_db\")\n",
    "    \n",
    "    collection_name = collection.name\n",
    "    metadata = collection.metadata or {}\n",
    "    distance_metric = metadata.get(\"hnsw:space\", \"æœªæŒ‡å®šï¼ˆé»˜è®¤é€šå¸¸ä¸º cosineï¼‰\")\n",
    "    \n",
    "    print(f\"   Collection åç§°: {collection_name}\")\n",
    "    print(f\"   ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼: {distance_metric}\")\n",
    "    \n",
    "retriever = vectorstore.as_retriever(\n",
    "search_type=\"similarity\",\n",
    "search_kwargs={\"k\": 5} # è¿”å› top-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c58efe-457b-4225-b727-4b1e90dfd4cb",
   "metadata": {},
   "source": [
    "**å‘é‡æ•°æ®åº“ä¸Collectionçš„å…³ç³»ï¼š**\n",
    "\n",
    "| å‘é‡æ•°æ®åº“æ¦‚å¿µ | ä¼ ç»Ÿå…³ç³»å‹æ•°æ®åº“ï¼ˆå¦‚ MySQLï¼‰ç±»æ¯” |\n",
    "|:--------------|:--------------------------------|\n",
    "| å‘é‡æ•°æ®åº“ï¼ˆVector Databaseï¼‰ | æ•´ä¸ª æ•°æ®åº“å®ä¾‹ï¼ˆDatabase Instanceï¼‰ï¼Œæ¯”å¦‚ my_company_db |\n",
    "| Collectionï¼ˆé›†åˆï¼‰ | ä¸€ä¸ª æ•°æ®è¡¨ï¼ˆTableï¼‰ï¼Œæ¯”å¦‚ users è¡¨ã€products è¡¨ |\n",
    "| ä¸€æ¡æ–‡æ¡£ + åµŒå…¥å‘é‡ | è¡¨ä¸­çš„ä¸€è¡Œè®°å½•ï¼ˆRow / Recordï¼‰ |\n",
    "| åµŒå…¥ç»´åº¦ã€ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼ | è¡¨çš„ç»“æ„å®šä¹‰ï¼ˆSchemaï¼‰ï¼Œæ¯”å¦‚å­—æ®µç±»å‹ã€ç´¢å¼•æ–¹å¼ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336746e5-9b43-4c8f-8d7f-f2d24638afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ ç»“æœ 1:\n",
      "å†…å®¹: perception çš®è‚¤ã€‚ä½¿ç”¨ä¸€ç§è¢«ç§°ä¸ºGelsightçš„è§¦è§‰ä¼ æ„Ÿå™¨ä½œä¸ºæœºå™¨äººçš„æŒ‡å°–è§¦è§‰æ„Ÿå—å™¨ï¼Œè¯¥ä¼ æ„Ÿå™¨å¯ä»¥æµ‹é‡é«˜åˆ†è¾¨ç‡çš„å‡ ä½•å½¢çŠ¶å¹¶ä»¥ 3Dæ–¹å¼é‡å»ºæ¥è§¦é¢ï¼Œæ­¤å¤–å¯ç”¨äºé—´æ¥æµ‹é‡æ³•å‘\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±.pdf\n",
      "idï¼šåŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±_p23_c383\n",
      "\n",
      "ğŸ“„ ç»“æœ 2:\n",
      "å†…å®¹: 1.2.2 è§¦è§‰æ„ŸçŸ¥ ..................................................................................... 3\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦è§‰èåˆçš„æœºæ¢°æ‰‹åˆ†ç±»æŠ“å–æ–¹æ³•ç ”ç©¶_ä½™èˆª.pdf\n",
      "idï¼šåŸºäºè§†è§¦è§‰èåˆçš„æœºæ¢°æ‰‹åˆ†ç±»æŠ“å–æ–¹æ³•ç ”ç©¶_ä½™èˆª_p5_c61\n",
      "\n",
      "ğŸ“„ ç»“æœ 3:\n",
      "å†…å®¹: è§‰ä¼ æ„Ÿå™¨ ï¼Œ æœ€ä¸»è¦çš„æµ‹é‡å¯¹è±¡å³æ˜¯ä¸ç›®æ ‡ç‰©çš„ç›¸äº’ä½œç”¨åŠ›ã€‚î—’î— ç”±äº ï¼Œ ç”µå­çš®è‚¤çš„å®è´¨æ˜¯é«˜å¯†åº¦çš„é˜µåˆ—å¼ä¼ æ„Ÿå™¨ ï¼Œ å…¶é‡‡é›†åˆ°çš„è§¦è§‰ä¿¡å·æ˜¯ä¸ä¼ \n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç….pdf\n",
      "idï¼šåŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç…_p39_c535\n",
      "\n",
      "ğŸ“„ ç»“æœ 4:\n",
      "å†…å®¹: 1.2.3 è§†è§¦è§‰èåˆæ„ŸçŸ¥äººä»¬å¯¹ä¸€ä¸ªç‰©ä½“çš„æè¿°å¾€å¾€æ˜¯ä»å¤šä¸ªè§’åº¦å»æè¿°çš„ï¼Œä¾é çš„æ˜¯å¤šä¸ªå™¨å®˜çš„å…±åŒæ„ŸçŸ¥ï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯è§†è§‰æ„ŸçŸ¥å’Œè§¦è§‰æ„ŸçŸ¥ï¼Œè§†è§‰æ„ŸçŸ¥æä¾›ç‰©ä½“çš„é¢œè‰²ã€å½¢çŠ¶\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦è§‰èåˆçš„æœºæ¢°æ‰‹åˆ†ç±»æŠ“å–æ–¹æ³•ç ”ç©¶_ä½™èˆª.pdf\n",
      "idï¼šåŸºäºè§†è§¦è§‰èåˆçš„æœºæ¢°æ‰‹åˆ†ç±»æŠ“å–æ–¹æ³•ç ”ç©¶_ä½™èˆª_p11_c178\n",
      "\n",
      "ğŸ“„ ç»“æœ 5:\n",
      "å†…å®¹: åæ˜ î—’î— ç»¼ä¸Š ï¼Œ äººæ‰‹å…·æœ‰æé«˜çš„çµæ´»æ€§å’Œä¸°å¯Œçš„è§¦è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œ èƒ½å¤Ÿçµæ´» ã€ å¯é åœ°æŠ“î—’î— æ¡å’Œæ“ä½œå„ç§ç‰©ä½“ ã€‚ é€šè¿‡èåˆè§¦è§‰ ã€ è§†\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™_p33_c685\n"
     ]
    }
   ],
   "source": [
    "# æ‰§è¡Œæ£€ç´¢\n",
    "query = \"ä»€ä¹ˆæ˜¯è§†è§¦è§‰ï¼Ÿ\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# æŸ¥çœ‹ç»“æœ\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    cleaned_content = clean_text(doc.page_content)  # æ¸…ç†å‰300å­—ç¬¦\n",
    "    source = doc.metadata.get('source', 'N/A')\n",
    "    id = doc.metadata.get('id', 'N/A')\n",
    "    print(f\"\\nğŸ“„ ç»“æœ {i}:\")\n",
    "    print(f\"å†…å®¹: {cleaned_content}\")\n",
    "    print(f\"æ¥æº: {source}\")\n",
    "    print(f\"idï¼š{id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c165de58-1a16-4ddd-9d74-38d23f9456c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ ç»“æœ 1:\n",
      "å†…å®¹: äººçµå·§æŠ“å–è¿‡ç¨‹ ï¼Œî—’î— ä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®æŠ“å–è¿‡ç¨‹ä¸­ç‰©ç†äº¤äº’æƒ…å†µ ï¼Œ åŠæ—¶è°ƒæ•´æŠ“å–å§¿æ€å’Œå¤¹æŒåŠ›åº¦ ï¼Œ æ˜¯î—’î— ä¿è¯æœºå™¨äººé‡‡æ‘˜è¿‡ç¨‹\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™_p164_c3266\n",
      "\n",
      "ğŸ“„ ç»“æœ 2:\n",
      "å†…å®¹: è¥¿å—ç§‘æŠ€å¤§å­¦ç¡•å£«å­¦ä½è®ºæ–‡ 32 å›¾ 4-2 æŠ“å–è¿‡ç¨‹ Fig.4-2 Grabbing Process ï¼ˆ1ï¼‰æŠ“æ‰‹æ‰“å¼€é˜¶æ®µï¼šæŠ“æ‰‹åœ¨åˆå§‹ä½ç½®å°†æŠ“å–æ‰“å¼€åˆ°æœ€å¤§å¼€åˆçš„çŠ¶æ€ï¼Œ ç­‰å¾…æŠ“\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™_p39_c500\n",
      "\n",
      "ğŸ“„ ç»“æœ 3:\n",
      "å†…å®¹: å…ç›®æ ‡ç‰©å‘ç”Ÿæ»‘åŠ¨ã€‚ æ ¹æ®æ»‘åŠ¨æ£€æµ‹ç®—î—’î— æ³•çš„ç»“æœï¼Œ å¯¹å¤¹å–åŠ›ä½œå‡ºç›¸å¯¹åº”çš„è°ƒæ•´æ˜¯æŠ“å–çš„åŸºæœ¬æ§åˆ¶ç­–ç•¥ã€‚ ç ”å®„äººå‘˜è¿‘å¹´æ¥î—’î— åœ¨æ»‘åŠ¨æ£€æµ‹åŠå®ç°æŠ“å–\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç….pdf\n",
      "idï¼šåŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç…_p17_c212\n",
      "\n",
      "ğŸ“„ ç»“æœ 4:\n",
      "å†…å®¹: æŠ“å–åŠŸèƒ½ï¼Œ ä½¿èƒ½æœºå™¨äººè§†è§¦ååŒæŠ“å–ä¼˜åŒ–ç®—æ³• ï¼Œ è§‚å¯Ÿæœºå™¨äººåœ¨î—’î— æ¥è§¦ç‰©ä½“åï¼Œ å°è¯•å¤¹æŒçš„è¿‡ç¨‹ ã€‚ è®°å½•æœºå™¨äººåœ¨å°è¯•å¤¹æŒè‡³å¤¹\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™_p154_c3054\n",
      "\n",
      "ğŸ“„ ç»“æœ 5:\n",
      "å†…å®¹: ä½“è¿›è¡ŒæŠ“å–ã€‚ è§¦è§‰æ„ŸçŸ¥ï¼šæŠ“å–è¿‡ç¨‹ä¸­ä½¿ç”¨è§¦è§‰ä¼ æ„Ÿå™¨å¯¹ç‰©ä½“è¿›è¡Œè§¦è§‰æ„ŸçŸ¥ï¼Œæ„ŸçŸ¥ç‰©ä½“ç¡¬åº¦\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™_p18_c277\n"
     ]
    }
   ],
   "source": [
    "# æ‰§è¡Œæ£€ç´¢\n",
    "query = \"ä»€ä¹ˆæ˜¯æŠ“å–æ£€æµ‹ï¼Ÿ\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# æŸ¥çœ‹ç»“æœ\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    cleaned_content = clean_text(doc.page_content)  # æ¸…ç†å‰300å­—ç¬¦\n",
    "    source = doc.metadata.get('source', 'N/A')\n",
    "    id = doc.metadata.get('id', 'N/A')\n",
    "    print(f\"\\nğŸ“„ ç»“æœ {i}:\")\n",
    "    print(f\"å†…å®¹: {cleaned_content}\")\n",
    "    print(f\"æ¥æº: {source}\")\n",
    "    print(f\"idï¼š{id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5c1be2-ce24-43f1-b26e-1dcfac9aec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ ç»“æœ 1:\n",
      "å†…å®¹: å°½ç®¡ï¼Œ æœºæ¢°î—’î— è‡‚çš„è¿åŠ¨é€Ÿåº¦å¯ä»¥è®¾å®šä¸ºåŒ€é€Ÿè¿åŠ¨ï¼Œ é¿å…åŠ é€Ÿåº¦å˜åŒ–è€Œé€ æˆå¯¹ç›®æ ‡å’Œå¤¹çˆªä¹‹é—´åŠ¨î—’î— åŠ›å­¦å¹³è¡¡å…³ç³»çš„å½±å“ ã€‚ ä½†æ˜¯ ï¼Œ æœºæ¢°è‡‚çš„\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç….pdf\n",
      "idï¼šåŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç…_p66_c947\n",
      "\n",
      "ğŸ“„ ç»“æœ 2:\n",
      "å†…å®¹: Â±180Â°/sï¼Œæœºæ¢°è‡‚æœ«ç«¯é€Ÿåº¦è§†å„å…³èŠ‚è½½è·ä¸å®é™…é€Ÿåº¦è€Œå®šã€‚æœºæ¢°è‡‚æœ¬ä½“çš„ä½å§¿å¯é‡å¤æ€§ç²¾åº¦ä¸º Â±0.03mmã€‚ä¸å…¶å®ƒå‹å·æœºæ¢°è‡‚ä¸åŒçš„æ˜¯ï¼ŒUR5eæœºæ¢°è‡‚çš„å·¥å…·æ³•å…°å¸¦æœ‰åŠ›æ„Ÿåº”\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±.pdf\n",
      "idï¼šåŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±_p25_c407\n",
      "\n",
      "ğŸ“„ ç»“æœ 3:\n",
      "å†…å®¹: æœºæ¢°è‡‚å¯é‡å¤æ€§ +-0.03mm æœ‰æ•ˆè´Ÿè½½ 5 åƒå…‹/11 ç£…å·¥ä½œåŠå¾„ 850mm/33.5 è‹±å¯¸è‡ªç”±åº¦ 6 ä¸ªæ—‹è½¬å…³èŠ‚\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™_p19_c282\n",
      "\n",
      "ğŸ“„ ç»“æœ 4:\n",
      "å†…å®¹: î— ï¼ ï¼ ï¼ï¼˜ï¼’ï¼“ î— ï¼ î—î—’î— æœ¬æ–‡å°†å†—ä½™åº¦æœºæ¢°è‡‚è¿åŠ¨æ§åˆ¶æŒ‡æ ‡å®šä¹‰ä¸º ï¼† ä»£è¡¨ç€æœºæ¢°è‡‚çš„é‡å¤è¿åŠ¨æ€§èƒ½ ï¼Œî—’î— å…¶è¡¨è¾¾å¼å¯ä»¥è®°ä¸º\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™_p138_c2748\n",
      "\n",
      "ğŸ“„ ç»“æœ 5:\n",
      "å†…å®¹: î— ç¬¬ï¼• ç« æœºæ¢°æ‰‹ç¨³å®šæŠ“å–ä¸­çš„åŠ›ä½åˆ†å±‚æ§åˆ¶ î—î—’î— ï½’ î—¥ ï¼¶ î—¥ ï½† î— ï½† î—¥ î— ï¼¾ î—¥ ï½’î—’î— ï¼ˆ ï½‚ï¼‰ æœºæ¢°è‡‚æ²¿åœ†å‘¨è¿åŠ¨î—’î— ï¼Œ ï¼š ï¼š äººî—¥ ï¼­ ï½‰ï¼š ä¸€å¿\n",
      "æ¥æº: ./Dataset/PDF/åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™.pdf\n",
      "idï¼šåŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™_p147_c2919\n"
     ]
    }
   ],
   "source": [
    "# æ‰§è¡Œæ£€ç´¢\n",
    "query = \"ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿ\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# æŸ¥çœ‹ç»“æœ\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    cleaned_content = clean_text(doc.page_content)  # æ¸…ç†å‰300å­—ç¬¦\n",
    "    source = doc.metadata.get('source', 'N/A')\n",
    "    id = doc.metadata.get('id', 'N/A')\n",
    "    print(f\"\\nğŸ“„ ç»“æœ {i}:\")\n",
    "    print(f\"å†…å®¹: {cleaned_content}\")\n",
    "    print(f\"æ¥æº: {source}\")\n",
    "    print(f\"idï¼š{id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189da459-ffb4-43a6-a9f8-ff4c78a04051",
   "metadata": {},
   "source": [
    "## 5.1é«˜çº§æ£€ç´¢ï¼šè‡ªå®šä¹‰æ£€ç´¢å™¨\n",
    "\n",
    "| æ­¥éª¤ | æ“ä½œ | ç›®çš„ |\n",
    "|------|------|------|\n",
    "| 1ï¸âƒ£ | æ‰©å¤§æ£€ç´¢èŒƒå›´ï¼ˆk=10ï¼‰ | è·å–æ›´å¤šå€™é€‰ï¼Œé¿å…é—æ¼ |\n",
    "| 2ï¸âƒ£ | æŒ‰å…ƒæ•°æ®è¿‡æ»¤ï¼ˆå¦‚ type='article'ï¼‰ | æ’é™¤ç›®å½•ã€å›¾æ³¨ç­‰æ— å…³å†…å®¹ |\n",
    "| 3ï¸âƒ£ | é‡æ’åºï¼ˆç”¨æ›´ç²¾ç»†çš„æ‰“åˆ†å‡½æ•°ï¼‰ | æå‡ top ç»“æœçš„ç›¸å…³æ€§ |\n",
    "| 4ï¸âƒ£ | è¿”å› top-5 | ç»™ä¸‹æ¸¸ï¼ˆå¦‚ LLMï¼‰æä¾›é«˜è´¨é‡ä¸Šä¸‹æ–‡ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d4dd86d-7897-4725-b5c0-e0114bd3c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def custom_retriever(question: str) -> list:\n",
    "    \"\"\"è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘\"\"\"\n",
    "    \n",
    "    # 1. åŸºç¡€æ£€ç´¢\n",
    "    base_docs = vectorstore.similarity_search(question, k=10)\n",
    "    \n",
    "    # 2. æ ¹æ®å…ƒæ•°æ®è¿‡æ»¤\n",
    "    filtered_docs = [\n",
    "        doc for doc in base_docs\n",
    "        if \"å¼ é™\" in doc.metadata.get('source')\n",
    "    ]\n",
    "    \n",
    "    # 3. é‡æ’åºï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹ï¼‰\n",
    "    scored_docs = [\n",
    "        (doc, calculate_relevance_score(doc, question))\n",
    "        for doc in filtered_docs\n",
    "    ]\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 4. è¿”å›top-k\n",
    "    return [doc for doc, score in scored_docs[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccdbaf-37f5-46a4-aab4-7459341290af",
   "metadata": {},
   "source": [
    "## 5.2è¯„ä¼°æ£€ç´¢è´¨é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66e86713-8681-4443-b2c8-7c018decd6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³å‡ç²¾ç¡®ç‡: 40.00%\n",
      "å¹³å‡å¬å›ç‡: 87.50%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_retrieval(retriever, test_cases):\n",
    "    \"\"\"è¯„ä¼°æ£€ç´¢å™¨æ€§èƒ½\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        \"precision\": [],  # ç²¾ç¡®ç‡\n",
    "        \"recall\": [],     # å¬å›ç‡\n",
    "    }\n",
    "    \n",
    "    for query, expected_doc_ids in test_cases:\n",
    "        # æ‰§è¡Œæ£€ç´¢\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        retrieved_ids = [doc.metadata['id'] for doc in retrieved_docs]\n",
    "        \n",
    "        # è®¡ç®—æŒ‡æ ‡\n",
    "        relevant_retrieved = set(retrieved_ids) & set(expected_doc_ids)\n",
    "        \n",
    "        precision = len(relevant_retrieved) / len(retrieved_ids) if retrieved_ids else 0\n",
    "        recall = len(relevant_retrieved) / len(expected_doc_ids) if expected_doc_ids else 0\n",
    "        \n",
    "        metrics[\"precision\"].append(precision)\n",
    "        metrics[\"recall\"].append(recall)\n",
    "    \n",
    "    return {\n",
    "        \"avg_precision\": sum(metrics[\"precision\"]) / len(metrics[\"precision\"]),\n",
    "        \"avg_recall\": sum(metrics[\"recall\"]) / len(metrics[\"recall\"])\n",
    "    }\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "test_cases = [\n",
    "    (\"ä»€ä¹ˆæ˜¯æŠ“å–æ£€æµ‹ï¼Ÿ\", [\"åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„å¤šæŒ‡çµå·§æ‰‹æŠ“å–æ–¹æ³•ç ”ç©¶_å¼ é™_p164_c3266\", \n",
    "                  \"åŸºäºè§†è§¦æ„ŸçŸ¥ååŒçš„æœºå™¨äººæŠ“å–æŠ€æœ¯ç ”ç©¶_ç¥ä¼šé¾™_p39_c500\", \n",
    "                  \"åŸºäºè§¦è§‰æ„ŸçŸ¥çš„æœºå™¨äººæ»‘åŠ¨æ£€æµ‹ä¸æŠ“å–æ§åˆ¶_ä»£ç¨·ç…_p17_c212\",\n",
    "                  \"åŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±_p23_c383\"]),\n",
    "    (\"ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿ\", [\"åŸºäºè§¦è§‰å›¾åƒåºåˆ—çš„æœºå™¨äººæŠ“å–ç›®æ ‡çŠ¶æ€æ„ŸçŸ¥_éŸ©ç­±_p25_c407\"]),\n",
    "]\n",
    "\n",
    "results = evaluate_retrieval(retriever, test_cases)\n",
    "print(f\"å¹³å‡ç²¾ç¡®ç‡: {results['avg_precision']:.2%}\")\n",
    "print(f\"å¹³å‡å¬å›ç‡: {results['avg_recall']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500246e-64af-40a4-bd7a-14f1a57fa5ae",
   "metadata": {},
   "source": [
    "# æ­¥éª¤6ï¼šç”Ÿæˆ\n",
    "\n",
    "åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œè®©LLMç”Ÿæˆç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c9ad08-96d5-4e2a-b6f4-2571d2bc70a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/shared-nvme/conda-envs/py310/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2025-11-10 23:27:46,884 - modelscope - INFO - PyTorch version 2.3.0+cu118 Found.\n",
      "2025-11-10 23:27:46,886 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2025-11-10 23:27:46,921 - modelscope - INFO - Loading done! Current index file version is 1.12.0, with md5 298ceecce207285dd10b135af16e71cc and a total number of 964 components indexed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from vllm import LLM, SamplingParams\n",
    "from modelscope import snapshot_download\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class LocalQASystem:\n",
    "    \"\"\"æœ¬åœ°é—®ç­”ç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir, chroma_db_path=\"./chroma_db\", embeddings_model_path=None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ\n",
    "        \n",
    "        Args:\n",
    "            model_dir: å¯¹è¯æ¨¡å‹è·¯å¾„\n",
    "            chroma_db_path: Chromaæ•°æ®åº“è·¯å¾„\n",
    "            embeddings_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„ï¼ˆä¸åˆ›å»ºæ—¶ç›¸åŒï¼‰\n",
    "        \"\"\"\n",
    "        self.model_dir = self._setup_model_dir(model_dir)\n",
    "        self.chroma_db_path = chroma_db_path\n",
    "        # è®¾ç½®åµŒå…¥æ¨¡å‹è·¯å¾„\n",
    "        if embeddings_model_path:\n",
    "            self.embeddings_model_path = embeddings_model_path\n",
    "        else:\n",
    "            self.embeddings_model_path = \"./Models/maidalun/bce-embedding-base_v1\"\n",
    "        self._setup_model()\n",
    "        self._setup_exact_embeddings() \n",
    "        self._setup_vectorstore()\n",
    "        print(\"âœ… é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def _setup_model_dir(self, model_dir):\n",
    "        if os.path.exists(model_dir):\n",
    "            print(f\"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_dir}\")\n",
    "            return model_dir\n",
    "        else:\n",
    "            print(f\"â¬‡ï¸  ä»ModelScopeä¸‹è½½æ¨¡å‹: {model_dir}\")\n",
    "            return snapshot_download(model_dir)\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "        self.llm = LLM(\n",
    "            model=self.model_dir,\n",
    "            tokenizer=self.model_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"ğŸ¤– vLLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def _setup_exact_embeddings(self):\n",
    "        \"\"\"ä½¿ç”¨ä¸åˆ›å»ºæ—¶å®Œå…¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹\"\"\"\n",
    "        if self.embeddings_model_path:\n",
    "            # ä½¿ç”¨æ‚¨æŒ‡å®šçš„è·¯å¾„\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=self.embeddings_model_path,\n",
    "                model_kwargs={\"device\": \"cuda\"},\n",
    "                encode_kwargs={\"normalize_embeddings\": True}\n",
    "            )\n",
    "            print(f\"âœ… ä½¿ç”¨åµŒå…¥æ¨¡å‹: {self.embeddings_model_path}\")\n",
    "        else:\n",
    "            # ä½¿ç”¨ä¸åˆ›å»ºæ—¶ç›¸åŒçš„æ¨¡å‹ID\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"maidalun/bce-embedding-base_v1\",\n",
    "                model_kwargs={\"device\": \"cuda\"},\n",
    "                encode_kwargs={\"normalize_embeddings\": True}\n",
    "            )\n",
    "            print(\"âœ… ä½¿ç”¨é»˜è®¤åµŒå…¥æ¨¡å‹: maidalun/bce-embedding-base_v1\")\n",
    "        \n",
    "        # æµ‹è¯•åµŒå…¥ç»´åº¦\n",
    "        test_vector = self.embeddings.embed_query(\"test\")\n",
    "        print(f\"ğŸ”¢ åµŒå…¥æ¨¡å‹ç»´åº¦: {len(test_vector)}\")\n",
    "    \n",
    "    def _setup_vectorstore(self):\n",
    "        \"\"\"åˆå§‹åŒ–å‘é‡æ•°æ®åº“\"\"\"\n",
    "        try:\n",
    "            self.client = chromadb.PersistentClient(path=self.chroma_db_path)\n",
    "            self.collection = self.client.get_collection(\"langchain\")\n",
    "            print(f\"ğŸ—‚ï¸  å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ: {self.chroma_db_path}\")\n",
    "            \n",
    "            # æ£€æŸ¥é›†åˆçŠ¶æ€\n",
    "            count = self.collection.count()\n",
    "            print(f\"ğŸ“„ æ–‡æ¡£æ•°é‡: {count}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {e}\")\n",
    "            self.collection = None\n",
    "    \n",
    "    def retrieve_with_exact_embedding(self, query, n_results=5):\n",
    "        \"\"\"ä½¿ç”¨ç²¾ç¡®åŒ¹é…çš„åµŒå…¥æ¨¡å‹è¿›è¡Œæ£€ç´¢\"\"\"\n",
    "        if not self.collection:\n",
    "            return {\"documents\": [[]]}\n",
    "        \n",
    "        try:\n",
    "            # å…³é”®ï¼šä½¿ç”¨ä¸åˆ›å»ºæ—¶ç›¸åŒçš„åµŒå…¥æ¨¡å‹ç”ŸæˆæŸ¥è¯¢å‘é‡\n",
    "            query_embedding = self.embeddings.embed_query(query)\n",
    "            print(f\"ğŸ” æŸ¥è¯¢åµŒå…¥ç»´åº¦: {len(query_embedding)}\")\n",
    "            \n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],  # ä½¿ç”¨åµŒå…¥å‘é‡\n",
    "                n_results=n_results\n",
    "            )\n",
    "            print(f\"âœ… æ£€ç´¢åˆ° {len(results['documents'][0])} ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ£€ç´¢å¤±è´¥: {e}\")\n",
    "            return {\"documents\": [[]]}\n",
    "    \n",
    "    def ask(self, question, max_tokens=512, temperature=0.3, n_sources=5):\n",
    "        \"\"\"é—®ç­”å‡½æ•°\"\"\"\n",
    "        if not self.collection:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": \"âŒ å‘é‡æ•°æ®åº“ä¸å¯ç”¨\",\n",
    "                \"sources\": 0,\n",
    "                \"error\": \"Vector database not available\"\n",
    "            }\n",
    "        \n",
    "        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨ç²¾ç¡®åŒ¹é…çš„åµŒå…¥æ¨¡å‹ï¼‰\n",
    "        results = self.retrieve_with_exact_embedding(question, n_sources)\n",
    "        \n",
    "        if not results or not results.get('documents') or not results['documents'][0]:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": \"âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£\",\n",
    "                \"sources\": 0,\n",
    "                \"error\": \"No relevant documents found\"\n",
    "            }\n",
    "        \n",
    "        # 2. æ„å»ºä¸Šä¸‹æ–‡\n",
    "        documents = results['documents'][0]\n",
    "        context = \"\\n\\n\".join(documents)\n",
    "        \n",
    "        # 3. æ„å»ºæç¤ºè¯\n",
    "        prompt = self._build_prompt(question, context)\n",
    "        # print(\"=\"*50)\n",
    "        # print(\"ç”Ÿæˆçš„promptï¼š\\n\")\n",
    "        # print(prompt)\n",
    "        # print(\"=\"*50)\n",
    "        \n",
    "        # 4. ç”Ÿæˆç­”æ¡ˆ\n",
    "        answer = self._generate_answer(prompt, max_tokens, temperature)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer.strip(),\n",
    "            \"sources\": len(documents),\n",
    "            \"context\": context,\n",
    "            \"documents\": documents\n",
    "        }\n",
    "        \n",
    "    def _clean_context(self, text):\n",
    "        \"\"\"æ¸…æ´—å•ä¸ªæ–‡æ¡£å†…å®¹\"\"\"\n",
    "        import re\n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "        \n",
    "        # 1. åˆå¹¶è¢«æ¢è¡Œç¬¦é”™è¯¯åˆ†å‰²çš„æ–‡å­—ï¼ˆå¦‚ï¼š\\nè¿\\nåŠ¨\\n â†’ è¿åŠ¨ï¼‰\n",
    "        # åŒ¹é…ï¼šæ¢è¡Œç¬¦å‰åéƒ½æ˜¯éç©ºç™½å­—ç¬¦ï¼ˆå°¤å…¶æ˜¯ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—ï¼‰ï¼Œåˆ™å»æ‰æ¢è¡Œ\n",
    "        cleaned = re.sub(r'(?<=[^\\s])\\n(?=[^\\s])', '', text)\n",
    "        # 2. å¤„ç†æ‚¨æåˆ°çš„ç‰¹å®šç©ºæ ¼æ¨¡å¼ï¼š\\n            \\n\n",
    "        cleaned = re.sub(r'\\n\\s+\\n', '\\n\\n', cleaned)  # å¤šè¡Œç©ºç™½å‹ç¼©ä¸ºå•ä¸ªç©ºè¡Œ\n",
    "        cleaned = re.sub(r'\\n\\s+', '\\n', cleaned)     # è¡Œå†…å¤šä½™ç©ºæ ¼\n",
    "        # 3. å¤„ç†æ‚¨æåˆ°çš„ç‰¹å®šæ¨¡å¼ï¼šæŒ‡\\n            \\n\n",
    "        cleaned = re.sub(r'([\\u4e00-\\u9fa5a-zA-Z0-9])\\n\\s*\\n', r'\\n\\n', cleaned)\n",
    "        # 4. ç§»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™æœ€å¤šä¸€ä¸ªç©ºè¡Œï¼‰\n",
    "        cleaned = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned)\n",
    "        # 5. æ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼\n",
    "        cleaned = re.sub(r'^\\s+|\\s+$', '', cleaned, flags=re.MULTILINE)\n",
    "        # 6. æ¸…ç†è¿ç»­ç©ºæ ¼\n",
    "        cleaned = re.sub(r'[ \\t]{2,}', ' ', cleaned)\n",
    "        \n",
    "        return cleaned.strip()\n",
    "        \n",
    "    def _build_prompt(self, question, context):\n",
    "        \"\"\"ChatMLæç¤ºè¯\"\"\"\n",
    "        cleaned_context = self._clean_context(context)\n",
    "        \n",
    "        return f\"\"\"<|im_start|>system\n",
    "            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n",
    "            \n",
    "            {cleaned_context}\n",
    "            \n",
    "            è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n",
    "            1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å›ç­”\n",
    "            2. å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”\"æˆ‘ä¸çŸ¥é“\"\n",
    "            3. ä¿æŒå›ç­”å‡†ç¡®ã€ç®€æ´\n",
    "            4. ä¸è¦ç¼–é€ ä¿¡æ¯<|im_end|>\n",
    "            <|im_start|>user\n",
    "            {question}<|im_end|>\n",
    "            <|im_start|>assistant\n",
    "            \"\"\"\n",
    "    \n",
    "    def _generate_answer(self, prompt, max_tokens, temperature):\n",
    "        \"\"\"ç”Ÿæˆç­”æ¡ˆ\"\"\"\n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.8,\n",
    "            stop=[\"<|im_end|>\", \"<|endoftext|>\"]\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        # print(\"outputs:\",outputs)\n",
    "        return outputs[0].outputs[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb59db3-1921-47a0-9e0b-8b84fa4c35e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: ../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8\n",
      "INFO 11-10 23:27:47 config.py:177] The model is convertible to Marlin format. Using Marlin kernel.\n",
      "INFO 11-10 23:27:47 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', speculative_config=None, tokenizer='../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8)\n",
      "WARNING 11-10 23:27:47 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-10 23:27:48 utils.py:660] Found nccl from library /usr/lib/x86_64-linux-gnu/libnccl.so.2\n",
      "INFO 11-10 23:27:48 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 11-10 23:27:48 selector.py:32] Using XFormers backend.\n",
      "INFO 11-10 23:27:54 model_runner.py:175] Loading model weights took 8.4877 GB\n",
      "INFO 11-10 23:27:56 gpu_executor.py:114] # GPU blocks: 1395, # CPU blocks: 512\n",
      "INFO 11-10 23:27:57 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-10 23:27:57 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-10 23:28:07 model_runner.py:1017] Graph capturing finished in 10 secs.\n",
      "ğŸ¤– vLLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ\n",
      "âœ… ä½¿ç”¨åµŒå…¥æ¨¡å‹: ./Models/maidalun/bce-embedding-base_v1\n",
      "ğŸ”¢ åµŒå…¥æ¨¡å‹ç»´åº¦: 768\n",
      "ğŸ—‚ï¸  å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ: ./chroma_db\n",
      "ğŸ“„ æ–‡æ¡£æ•°é‡: 12721\n",
      "âœ… é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ\n",
    "qa_system = LocalQASystem(\n",
    "    model_dir=\"../Qwen-vllm/Models/Qwen/Qwen-7B-Chat-Int8\",\n",
    "    chroma_db_path=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2bd856-23cf-4bab-80fc-4af55db2ca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹æµ‹è¯•é—®ç­”ç³»ç»Ÿ\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ é—®é¢˜ 1: ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ\n",
      "--------------------------------------------------\n",
      "ğŸ” æŸ¥è¯¢åµŒå…¥ç»´åº¦: 768\n",
      "âœ… æ£€ç´¢åˆ° 5 ä¸ªç›¸å…³æ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ ç­”æ¡ˆ: æœºæ¢°è‡‚æ˜¯ä¸€ç§å¯ä»¥è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡çš„æœºå™¨äººæ‰‹è‡‚ï¼Œå®ƒå…·æœ‰ç²¾ç¡®åº¦é«˜ã€å¯é‡å¤æ€§å¥½ã€è´Ÿè½½èƒ½åŠ›å¼ºã€å·¥ä½œåŠå¾„å¤§ã€è‡ªç”±åº¦å¤šç­‰èƒ½åŠ›ã€‚\n",
      "ğŸ“š å‚è€ƒæ–‡æ¡£: 5 ä¸ª\n",
      "\n",
      "ğŸ¯ é—®é¢˜ 2: è§£é‡Šä¸€ä¸‹æ»‘åŠ¨æ£€æµ‹çš„åŸºæœ¬æ¦‚å¿µ\n",
      "--------------------------------------------------\n",
      "ğŸ” æŸ¥è¯¢åµŒå…¥ç»´åº¦: 768\n",
      "âœ… æ£€ç´¢åˆ° 5 ä¸ªç›¸å…³æ–‡æ¡£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ ç­”æ¡ˆ: æ»‘åŠ¨æ£€æµ‹æ˜¯ä¸€ç§ç”¨äºæ£€æµ‹ç‰©ä½“æ˜¯å¦å‘ç”Ÿæ»‘åŠ¨çš„ç®—æ³•ã€‚å®ƒé€šè¿‡æ¯”è¾ƒå¸§é—´çš„æ ‡å‡†å·®æ¥åˆ¤æ–­ç‰©ä½“æ˜¯å¦å‘ç”Ÿæ»‘åŠ¨ã€‚å¦‚æœå¸§é—´çš„æ ‡å‡†å·®è¶…è¿‡è®¾å®šçš„é˜ˆå€¼ï¼Œé‚£ä¹ˆå°±è®¤ä¸ºç‰©ä½“å‘ç”Ÿäº†æ»‘åŠ¨ã€‚æ»‘åŠ¨æ£€æµ‹ç®—æ³•é€šå¸¸ç”¨äºæœºæ¢°è‡‚çš„æŠ“å–ä»»åŠ¡ä¸­ï¼Œä»¥é˜²æ­¢ç‰©ä½“åœ¨æŠ“å–è¿‡ç¨‹ä¸­æ»‘è½ã€‚\n",
      "ğŸ“š å‚è€ƒæ–‡æ¡£: 5 ä¸ª\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•é—®ç­”\n",
    "questions = [\n",
    "    \"ä»€ä¹ˆæ˜¯æœºæ¢°è‡‚ï¼Ÿå®ƒæœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ\",\n",
    "    \"è§£é‡Šä¸€ä¸‹æ»‘åŠ¨æ£€æµ‹çš„åŸºæœ¬æ¦‚å¿µ\",\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹æµ‹è¯•é—®ç­”ç³»ç»Ÿ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\nğŸ¯ é—®é¢˜ {i}: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = qa_system.ask(question, max_tokens=400, temperature=0.3)\n",
    "    # print(\"result:\",result)\n",
    "    print(f\"ğŸ’¡ ç­”æ¡ˆ: {result['answer']}\")\n",
    "    print(f\"ğŸ“š å‚è€ƒæ–‡æ¡£: {result['sources']} ä¸ª\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ–‡æ¡£é¢„è§ˆ\n",
    "    # for j, doc in enumerate(result.get('documents', [])[:2], 1):\n",
    "    #     preview = doc[:150] + \"...\" if len(doc) > 150 else doc\n",
    "    #     print(f\"   ğŸ“„ æ–‡æ¡£ {j}: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb42a0-0a36-48f3-b266-86ac2cc32e5c",
   "metadata": {},
   "source": [
    "### Chain Typesè¯¦è§£\n",
    "\n",
    "LangChainæä¾›äº†4ç§ä¸åŒçš„é“¾ç±»å‹æ¥å¤„ç†æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼š\n",
    "\n",
    "1. Stuffï¼ˆå¡«å……ï¼‰ - æœ€ç®€å•ï¼Œæ¨èä¼˜å…ˆä½¿ç”¨\n",
    "\n",
    "```python\n",
    "chain_type=\"stuff\"\n",
    "\n",
    "# å·¥ä½œåŸç†ï¼š\n",
    "æç¤ºè¯: \"\"\"\n",
    "åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n",
    "\n",
    "æ–‡æ¡£1: ...\n",
    "æ–‡æ¡£2: ...  \n",
    "æ–‡æ¡£3: ...\n",
    "\n",
    "é—®é¢˜: {{question}}\n",
    "ç­”æ¡ˆ:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "ä¼˜ç‚¹ï¼šç®€å•ç›´æ¥ï¼Œåªéœ€è¦ä¸€æ¬¡LLMè°ƒç”¨ï¼Œæˆæœ¬ä½ã€‚ ç¼ºç‚¹ï¼šæ–‡æ¡£è¿‡å¤šä¼šè¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶\n",
    "\n",
    "2. Map-Reduceï¼ˆæ˜ å°„-å½’çº¦ï¼‰ - å¤„ç†å¤§é‡æ–‡æ¡£\n",
    "\n",
    "```python\n",
    "chain_type=\"map_reduce\"\n",
    "\n",
    "# å·¥ä½œåŸç†ï¼š\n",
    "# Mapé˜¶æ®µï¼šå¯¹æ¯ä¸ªæ–‡æ¡£åˆ†åˆ«æé—®\n",
    "æ–‡æ¡£1 â†’ LLM â†’ éƒ¨åˆ†ç­”æ¡ˆ1\n",
    "æ–‡æ¡£2 â†’ LLM â†’ éƒ¨åˆ†ç­”æ¡ˆ2\n",
    "æ–‡æ¡£3 â†’ LLM â†’ éƒ¨åˆ†ç­”æ¡ˆ3\n",
    "\n",
    "# Reduceé˜¶æ®µï¼šåˆå¹¶æ‰€æœ‰éƒ¨åˆ†ç­”æ¡ˆ\n",
    "[éƒ¨åˆ†ç­”æ¡ˆ1, éƒ¨åˆ†ç­”æ¡ˆ2, éƒ¨åˆ†ç­”æ¡ˆ3] â†’ LLM â†’ æœ€ç»ˆç­”æ¡ˆ\n",
    "```\n",
    "\n",
    "ä¼˜ç‚¹ï¼šå¯ä»¥å¤„ç†ä»»æ„æ•°é‡æ–‡æ¡£ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†ã€‚ ç¼ºç‚¹ï¼šéœ€è¦å¤šæ¬¡LLMè°ƒç”¨ï¼ˆæˆæœ¬é«˜ï¼‰ï¼Œå¯èƒ½ä¸¢å¤±æ–‡æ¡£é—´çš„å…³è”ã€‚\n",
    "\n",
    "3. Refineï¼ˆç²¾ç‚¼ï¼‰ - è¿­ä»£æ”¹è¿›ç­”æ¡ˆ\n",
    "\n",
    "```python\n",
    "chain_type=\"refine\"\n",
    "\n",
    "# å·¥ä½œåŸç†ï¼š\n",
    "æ–‡æ¡£1 â†’ LLM â†’ åˆå§‹ç­”æ¡ˆ\n",
    "[åˆå§‹ç­”æ¡ˆ + æ–‡æ¡£2] â†’ LLM â†’ æ”¹è¿›ç­”æ¡ˆ1\n",
    "[æ”¹è¿›ç­”æ¡ˆ1 + æ–‡æ¡£3] â†’ LLM â†’ æœ€ç»ˆç­”æ¡ˆ\n",
    "```\n",
    "\n",
    "ä¼˜ç‚¹ï¼šç­”æ¡ˆè´¨é‡é«˜ï¼Œä¿æŒæ–‡æ¡£é—´çš„å…³è”ã€‚ ç¼ºç‚¹ï¼šé¡ºåºæ•æ„Ÿï¼ˆæ–‡æ¡£é¡ºåºå½±å“ç»“æœï¼‰ï¼Œä¸èƒ½å¹¶è¡Œã€‚\n",
    "\n",
    "4.  Map-Rerankï¼ˆæ˜ å°„-é‡æ’åºï¼‰ - æ‰¾æœ€ä½³ç­”æ¡ˆ\n",
    "\n",
    "```python\n",
    "chain_type=\"map_rerank\"\n",
    "\n",
    "# å·¥ä½œåŸç†ï¼š\n",
    "æ–‡æ¡£1 â†’ LLM â†’ ç­”æ¡ˆ1 (ç½®ä¿¡åº¦: 0.8)\n",
    "æ–‡æ¡£2 â†’ LLM â†’ ç­”æ¡ˆ2 (ç½®ä¿¡åº¦: 0.6)\n",
    "æ–‡æ¡£3 â†’ LLM â†’ ç­”æ¡ˆ3 (ç½®ä¿¡åº¦: 0.9) â† é€‰æ‹©è¿™ä¸ª\n",
    "```\n",
    "\n",
    "5.  Chain Typeé€‰æ‹©æŒ‡å—\n",
    "\n",
    "| åœºæ™¯ | æ¨èç±»å‹ | åŸå›  |\n",
    "|:-----|:--------:|:-----|\n",
    "| æ–‡æ¡£å°‘(<10) | Stuff | æœ€ç®€å•é«˜æ•ˆ |\n",
    "| æ–‡æ¡£å¤š(>10) | Map-Reduce | å¯æ‰©å±• |\n",
    "| éœ€è¦é«˜è´¨é‡ç­”æ¡ˆ | Refine | è¿­ä»£æ”¹è¿› |\n",
    "| éœ€è¦æœ€ç›¸å…³ç­”æ¡ˆ | Map-Rerank | è‡ªåŠ¨é€‰æ‹©æœ€ä½³ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf1b5d-1390-4cb2-ba31-fb811717dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
